{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About these docs","text":"<p>Welcome to Marsh's totally unofficial and totally unsupported documentation for the OpenAI Python library.</p> <ul> <li> OpenAI Python library reference</li> <li> Get started with the library</li> </ul>"},{"location":"#unofficial","title":"Unofficial","text":"<p>Neither this site nor its contents are officially endorsed by OpenAI. Other than being a customer, I have no affiliation with OpenAI. I've published this documentation because I wanted full API reference for the Python library. (1) You might find it useful, too.</p> <ol> <li>I'll likely decommission this site when OpenAI publishes their own Python API reference.</li> </ol> <p>You might instead prefer to use OpenAI's official docs on OpenAI's official documentation site and in the upstream openai/openai-python repository on GitHub, however.</p>"},{"location":"#unsupported","title":"Unsupported","text":"<p>The documentation on this site is provided AS-IS and with NO WARRANTY. The API reference on this site is generated from the OpenAI Python library's code, but I make no promises nor guarantee these docs reflect the current, past, or future state of the library.</p> <p>That said, I use these docs myself and thus intend to keep them (mostly) current, but there's no automation pulling content from their repo to this fork. (1)</p> <ol> <li>This means you might encounter inaccuracies or you might not find what you think should be here. In either case, you should refer to openai/openai-python as the source of truth.</li> </ol> <p>Quote</p> <p>If these docs help you, yay! If they don't, don't use 'em. Enjoy! \u2014Marsh</p>"},{"location":"__init__/","title":"openai","text":""},{"location":"__init__/#src.openai","title":"openai","text":""},{"location":"__init__/#src.openai.AsyncClient","title":"AsyncClient  <code>module-attribute</code>","text":"<pre><code>AsyncClient = AsyncOpenAI\n</code></pre>"},{"location":"__init__/#src.openai.Client","title":"Client  <code>module-attribute</code>","text":"<pre><code>Client = OpenAI\n</code></pre>"},{"location":"__init__/#src.openai.NoneType","title":"NoneType  <code>module-attribute</code>","text":"<pre><code>NoneType: Type[None]\n</code></pre>"},{"location":"__init__/#src.openai.ProxiesTypes","title":"ProxiesTypes  <code>module-attribute</code>","text":"<pre><code>ProxiesTypes = Union[str, Proxy, ProxiesDict]\n</code></pre>"},{"location":"__init__/#src.openai.Transport","title":"Transport  <code>module-attribute</code>","text":"<pre><code>Transport = BaseTransport\n</code></pre>"},{"location":"__init__/#src.openai.api_key","title":"api_key  <code>module-attribute</code>","text":"<pre><code>api_key: str | None = None\n</code></pre>"},{"location":"__init__/#src.openai.api_type","title":"api_type  <code>module-attribute</code>","text":"<pre><code>api_type: _ApiType | None = cast(\n    _ApiType, get(\"OPENAI_API_TYPE\")\n)\n</code></pre>"},{"location":"__init__/#src.openai.api_version","title":"api_version  <code>module-attribute</code>","text":"<pre><code>api_version: str | None = get('OPENAI_API_VERSION')\n</code></pre>"},{"location":"__init__/#src.openai.azure_ad_token","title":"azure_ad_token  <code>module-attribute</code>","text":"<pre><code>azure_ad_token: str | None = get('AZURE_OPENAI_AD_TOKEN')\n</code></pre>"},{"location":"__init__/#src.openai.azure_ad_token_provider","title":"azure_ad_token_provider  <code>module-attribute</code>","text":"<pre><code>azure_ad_token_provider: AzureADTokenProvider | None = None\n</code></pre>"},{"location":"__init__/#src.openai.azure_endpoint","title":"azure_endpoint  <code>module-attribute</code>","text":"<pre><code>azure_endpoint: str | None = get('AZURE_OPENAI_ENDPOINT')\n</code></pre>"},{"location":"__init__/#src.openai.base_url","title":"base_url  <code>module-attribute</code>","text":"<pre><code>base_url: str | URL | None = None\n</code></pre>"},{"location":"__init__/#src.openai.default_headers","title":"default_headers  <code>module-attribute</code>","text":"<pre><code>default_headers: Mapping[str, str] | None = None\n</code></pre>"},{"location":"__init__/#src.openai.default_query","title":"default_query  <code>module-attribute</code>","text":"<pre><code>default_query: Mapping[str, object] | None = None\n</code></pre>"},{"location":"__init__/#src.openai.http_client","title":"http_client  <code>module-attribute</code>","text":"<pre><code>http_client: Client | None = None\n</code></pre>"},{"location":"__init__/#src.openai.max_retries","title":"max_retries  <code>module-attribute</code>","text":"<pre><code>max_retries: int = DEFAULT_MAX_RETRIES\n</code></pre>"},{"location":"__init__/#src.openai.organization","title":"organization  <code>module-attribute</code>","text":"<pre><code>organization: str | None = None\n</code></pre>"},{"location":"__init__/#src.openai.timeout","title":"timeout  <code>module-attribute</code>","text":"<pre><code>timeout: float | Timeout | None = DEFAULT_TIMEOUT\n</code></pre>"},{"location":"__init__/#src.openai.APIConnectionError","title":"APIConnectionError","text":"<pre><code>APIConnectionError(\n    *, message: str = \"Connection error.\", request: Request\n)\n</code></pre>"},{"location":"__init__/#src.openai.APIError","title":"APIError","text":"<pre><code>APIError(\n    message: str, request: Request, *, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.APIError.body","title":"body  <code>instance-attribute</code>","text":"<pre><code>body: object | None = body\n</code></pre> <p>The API response body.</p> <p>If the API responded with a valid JSON structure then this property will be the decoded result.</p> <p>If it isn't a valid JSON structure then this will be the raw response.</p> <p>If there was no response associated with this error then it will be <code>None</code>.</p>"},{"location":"__init__/#src.openai.APIError.code","title":"code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>code: Optional[str] = None\n</code></pre>"},{"location":"__init__/#src.openai.APIError.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str = message\n</code></pre>"},{"location":"__init__/#src.openai.APIError.param","title":"param  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>param: Optional[str] = None\n</code></pre>"},{"location":"__init__/#src.openai.APIError.request","title":"request  <code>instance-attribute</code>","text":"<pre><code>request: Request = request\n</code></pre>"},{"location":"__init__/#src.openai.APIError.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Optional[str]\n</code></pre>"},{"location":"__init__/#src.openai.APIResponseValidationError","title":"APIResponseValidationError","text":"<pre><code>APIResponseValidationError(\n    response: Response,\n    body: object | None,\n    *,\n    message: str | None = None\n)\n</code></pre>"},{"location":"__init__/#src.openai.APIResponseValidationError.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"__init__/#src.openai.APIResponseValidationError.status_code","title":"status_code  <code>instance-attribute</code>","text":"<pre><code>status_code: int = status_code\n</code></pre>"},{"location":"__init__/#src.openai.APIStatusError","title":"APIStatusError","text":"<pre><code>APIStatusError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre> <p>Raised when an API response has a status code of 4xx or 5xx.</p>"},{"location":"__init__/#src.openai.APIStatusError.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"__init__/#src.openai.APIStatusError.status_code","title":"status_code  <code>instance-attribute</code>","text":"<pre><code>status_code: int = status_code\n</code></pre>"},{"location":"__init__/#src.openai.APITimeoutError","title":"APITimeoutError","text":"<pre><code>APITimeoutError(request: Request)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI","title":"AsyncOpenAI","text":"<pre><code>AsyncOpenAI(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: Union[\n        float, Timeout, None, NotGiven\n    ] = NOT_GIVEN,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    http_client: AsyncClient | None = None,\n    _strict_response_validation: bool = False\n)\n</code></pre> <p>This automatically infers the following arguments from their corresponding environment variables if they are not provided: - <code>api_key</code> from <code>OPENAI_API_KEY</code> - <code>organization</code> from <code>OPENAI_ORG_ID</code></p>"},{"location":"__init__/#src.openai.AsyncOpenAI.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: str = api_key\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: AsyncAudio = AsyncAudio(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.auth_headers","title":"auth_headers  <code>property</code>","text":"<pre><code>auth_headers: dict[str, str]\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta: AsyncBeta = AsyncBeta(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat: AsyncChat = AsyncChat(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions: AsyncCompletions = AsyncCompletions(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.default_headers","title":"default_headers  <code>property</code>","text":"<pre><code>default_headers: dict[str, str | Omit]\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings: AsyncEmbeddings = AsyncEmbeddings(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: AsyncFiles = AsyncFiles(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning: AsyncFineTuning = AsyncFineTuning(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images: AsyncImages = AsyncImages(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models: AsyncModels = AsyncModels(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations: AsyncModerations = AsyncModerations(self)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.organization","title":"organization  <code>instance-attribute</code>","text":"<pre><code>organization: str | None = organization\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.qs","title":"qs  <code>property</code>","text":"<pre><code>qs: Querystring\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.with_options","title":"with_options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>with_options = copy\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.with_raw_response","title":"with_raw_response  <code>instance-attribute</code>","text":"<pre><code>with_raw_response: AsyncOpenAIWithRawResponse = (\n    AsyncOpenAIWithRawResponse(self)\n)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.with_streaming_response","title":"with_streaming_response  <code>instance-attribute</code>","text":"<pre><code>with_streaming_response: AsyncOpenAIWithStreamedResponse = (\n    AsyncOpenAIWithStreamedResponse(self)\n)\n</code></pre>"},{"location":"__init__/#src.openai.AsyncOpenAI.copy","title":"copy","text":"<pre><code>copy(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    http_client: AsyncClient | None = None,\n    max_retries: int | NotGiven = NOT_GIVEN,\n    default_headers: Mapping[str, str] | None = None,\n    set_default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    set_default_query: Mapping[str, object] | None = None,\n    _extra_kwargs: Mapping[str, Any] = {}\n) -&gt; Self\n</code></pre> <p>Create a new client instance re-using the same options given to the current client with optional overriding.</p>"},{"location":"__init__/#src.openai.AsyncStream","title":"AsyncStream","text":"<pre><code>AsyncStream(\n    *,\n    cast_to: type[_T],\n    response: Response,\n    client: AsyncOpenAI\n)\n</code></pre> <p>Provides the core interface to iterate over an asynchronous stream response.</p>"},{"location":"__init__/#src.openai.AsyncStream.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"__init__/#src.openai.AsyncStream.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the response and release the connection.</p> <p>Automatically called if the response body is read to completion.</p>"},{"location":"__init__/#src.openai.AuthenticationError","title":"AuthenticationError","text":"<pre><code>AuthenticationError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.AuthenticationError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[401] = 401\n</code></pre>"},{"location":"__init__/#src.openai.BadRequestError","title":"BadRequestError","text":"<pre><code>BadRequestError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.BadRequestError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[400] = 400\n</code></pre>"},{"location":"__init__/#src.openai.BaseModel","title":"BaseModel","text":""},{"location":"__init__/#src.openai.BaseModel.model_config","title":"model_config  <code>class-attribute</code>","text":"<pre><code>model_config: ConfigDict = ConfigDict(extra='allow')\n</code></pre>"},{"location":"__init__/#src.openai.BaseModel.model_construct","title":"model_construct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_construct = construct\n</code></pre>"},{"location":"__init__/#src.openai.BaseModel.model_fields_set","title":"model_fields_set  <code>property</code>","text":"<pre><code>model_fields_set: set[str]\n</code></pre>"},{"location":"__init__/#src.openai.BaseModel.Config","title":"Config","text":""},{"location":"__init__/#src.openai.BaseModel.Config.extra","title":"extra  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra: Any = allow\n</code></pre>"},{"location":"__init__/#src.openai.BaseModel.construct","title":"construct  <code>classmethod</code>","text":"<pre><code>construct(\n    _fields_set: set[str] | None = None, **values: object\n) -&gt; ModelT\n</code></pre>"},{"location":"__init__/#src.openai.BaseModel.model_dump","title":"model_dump","text":"<pre><code>model_dump(\n    *,\n    mode: Literal[\"json\", \"python\"] | str = \"python\",\n    include: IncEx = None,\n    exclude: IncEx = None,\n    by_alias: bool = False,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    round_trip: bool = False,\n    warnings: bool = True\n) -&gt; dict[str, Any]\n</code></pre> <p>Usage docs: https://docs.pydantic.dev/2.4/concepts/serialization/#modelmodel_dump</p> <p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['json', 'python'] | str</code> <p>The mode in which <code>to_python</code> should run. If mode is 'json', the dictionary will only contain JSON serializable types. If mode is 'python', the dictionary may contain any Python objects.</p> <code>'python'</code> <code>include</code> <code>IncEx</code> <p>A list of fields to include in the output.</p> <code>None</code> <code>exclude</code> <code>IncEx</code> <p>A list of fields to exclude from the output.</p> <code>None</code> <code>by_alias</code> <code>bool</code> <p>Whether to use the field's alias in the dictionary key if defined.</p> <code>False</code> <code>exclude_unset</code> <code>bool</code> <p>Whether to exclude fields that are unset or None from the output.</p> <code>False</code> <code>exclude_defaults</code> <code>bool</code> <p>Whether to exclude fields that are set to their default value from the output.</p> <code>False</code> <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields that have a value of <code>None</code> from the output.</p> <code>False</code> <code>round_trip</code> <code>bool</code> <p>Whether to enable serialization and deserialization round-trip support.</p> <code>False</code> <code>warnings</code> <code>bool</code> <p>Whether to log warnings when invalid fields are encountered.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representation of the model.</p>"},{"location":"__init__/#src.openai.BaseModel.model_dump_json","title":"model_dump_json","text":"<pre><code>model_dump_json(\n    *,\n    indent: int | None = None,\n    include: IncEx = None,\n    exclude: IncEx = None,\n    by_alias: bool = False,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    round_trip: bool = False,\n    warnings: bool = True\n) -&gt; str\n</code></pre> <p>Usage docs: https://docs.pydantic.dev/2.4/concepts/serialization/#modelmodel_dump_json</p> <p>Generates a JSON representation of the model using Pydantic's <code>to_json</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int | None</code> <p>Indentation to use in the JSON output. If None is passed, the output will be compact.</p> <code>None</code> <code>include</code> <code>IncEx</code> <p>Field(s) to include in the JSON output. Can take either a string or set of strings.</p> <code>None</code> <code>exclude</code> <code>IncEx</code> <p>Field(s) to exclude from the JSON output. Can take either a string or set of strings.</p> <code>None</code> <code>by_alias</code> <code>bool</code> <p>Whether to serialize using field aliases.</p> <code>False</code> <code>exclude_unset</code> <code>bool</code> <p>Whether to exclude fields that have not been explicitly set.</p> <code>False</code> <code>exclude_defaults</code> <code>bool</code> <p>Whether to exclude fields that have the default value.</p> <code>False</code> <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields that have a value of <code>None</code>.</p> <code>False</code> <code>round_trip</code> <code>bool</code> <p>Whether to use serialization/deserialization between JSON and class instance.</p> <code>False</code> <code>warnings</code> <code>bool</code> <p>Whether to show any warnings that occurred during serialization.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string representation of the model.</p>"},{"location":"__init__/#src.openai.ConflictError","title":"ConflictError","text":"<pre><code>ConflictError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.ConflictError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[409] = 409\n</code></pre>"},{"location":"__init__/#src.openai.InternalServerError","title":"InternalServerError","text":"<pre><code>InternalServerError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.NotFoundError","title":"NotFoundError","text":"<pre><code>NotFoundError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.NotFoundError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[404] = 404\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI","title":"OpenAI","text":"<pre><code>OpenAI(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: Union[\n        float, Timeout, None, NotGiven\n    ] = NOT_GIVEN,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    http_client: Client | None = None,\n    _strict_response_validation: bool = False\n)\n</code></pre> <p>This automatically infers the following arguments from their corresponding environment variables if they are not provided: - <code>api_key</code> from <code>OPENAI_API_KEY</code> - <code>organization</code> from <code>OPENAI_ORG_ID</code></p>"},{"location":"__init__/#src.openai.OpenAI.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: str = api_key\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: Audio = Audio(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.auth_headers","title":"auth_headers  <code>property</code>","text":"<pre><code>auth_headers: dict[str, str]\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta: Beta = Beta(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat: Chat = Chat(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions: Completions = Completions(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.default_headers","title":"default_headers  <code>property</code>","text":"<pre><code>default_headers: dict[str, str | Omit]\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings: Embeddings = Embeddings(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: Files = Files(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning: FineTuning = FineTuning(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images: Images = Images(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models: Models = Models(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations: Moderations = Moderations(self)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.organization","title":"organization  <code>instance-attribute</code>","text":"<pre><code>organization: str | None = organization\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.qs","title":"qs  <code>property</code>","text":"<pre><code>qs: Querystring\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.with_options","title":"with_options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>with_options = copy\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.with_raw_response","title":"with_raw_response  <code>instance-attribute</code>","text":"<pre><code>with_raw_response: OpenAIWithRawResponse = (\n    OpenAIWithRawResponse(self)\n)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.with_streaming_response","title":"with_streaming_response  <code>instance-attribute</code>","text":"<pre><code>with_streaming_response: OpenAIWithStreamedResponse = (\n    OpenAIWithStreamedResponse(self)\n)\n</code></pre>"},{"location":"__init__/#src.openai.OpenAI.copy","title":"copy","text":"<pre><code>copy(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    http_client: Client | None = None,\n    max_retries: int | NotGiven = NOT_GIVEN,\n    default_headers: Mapping[str, str] | None = None,\n    set_default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    set_default_query: Mapping[str, object] | None = None,\n    _extra_kwargs: Mapping[str, Any] = {}\n) -&gt; Self\n</code></pre> <p>Create a new client instance re-using the same options given to the current client with optional overriding.</p>"},{"location":"__init__/#src.openai.OpenAIError","title":"OpenAIError","text":""},{"location":"__init__/#src.openai.PermissionDeniedError","title":"PermissionDeniedError","text":"<pre><code>PermissionDeniedError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.PermissionDeniedError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[403] = 403\n</code></pre>"},{"location":"__init__/#src.openai.RateLimitError","title":"RateLimitError","text":"<pre><code>RateLimitError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.RateLimitError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[429] = 429\n</code></pre>"},{"location":"__init__/#src.openai.RequestOptions","title":"RequestOptions","text":""},{"location":"__init__/#src.openai.RequestOptions.extra_json","title":"extra_json  <code>instance-attribute</code>","text":"<pre><code>extra_json: AnyMapping\n</code></pre>"},{"location":"__init__/#src.openai.RequestOptions.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: Headers\n</code></pre>"},{"location":"__init__/#src.openai.RequestOptions.idempotency_key","title":"idempotency_key  <code>instance-attribute</code>","text":"<pre><code>idempotency_key: str\n</code></pre>"},{"location":"__init__/#src.openai.RequestOptions.max_retries","title":"max_retries  <code>instance-attribute</code>","text":"<pre><code>max_retries: int\n</code></pre>"},{"location":"__init__/#src.openai.RequestOptions.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Query\n</code></pre>"},{"location":"__init__/#src.openai.RequestOptions.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: float | Timeout | None\n</code></pre>"},{"location":"__init__/#src.openai.Stream","title":"Stream","text":"<pre><code>Stream(\n    *, cast_to: type[_T], response: Response, client: OpenAI\n)\n</code></pre> <p>Provides the core interface to iterate over a synchronous stream response.</p>"},{"location":"__init__/#src.openai.Stream.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"__init__/#src.openai.Stream.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the response and release the connection.</p> <p>Automatically called if the response body is read to completion.</p>"},{"location":"__init__/#src.openai.UnprocessableEntityError","title":"UnprocessableEntityError","text":"<pre><code>UnprocessableEntityError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"__init__/#src.openai.UnprocessableEntityError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[422] = 422\n</code></pre>"},{"location":"__init__/#src.openai.file_from_path","title":"file_from_path","text":"<pre><code>file_from_path(path: str) -&gt; FileTypes\n</code></pre>"},{"location":"_base_client/","title":"base client","text":""},{"location":"_base_client/#src.openai._base_client","title":"_base_client","text":""},{"location":"_base_client/#src.openai._base_client.Arch","title":"Arch  <code>module-attribute</code>","text":"<pre><code>Arch = Union[\n    OtherArch,\n    Literal[\"x32\", \"x64\", \"arm\", \"arm64\", \"unknown\"],\n]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncPageT","title":"AsyncPageT  <code>module-attribute</code>","text":"<pre><code>AsyncPageT = TypeVar(\n    \"AsyncPageT\", bound=\"BaseAsyncPage[Any]\"\n)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.Platform","title":"Platform  <code>module-attribute</code>","text":"<pre><code>Platform = Union[\n    OtherPlatform,\n    Literal[\n        \"MacOS\",\n        \"Linux\",\n        \"Windows\",\n        \"FreeBSD\",\n        \"OpenBSD\",\n        \"iOS\",\n        \"Android\",\n        \"Unknown\",\n    ],\n]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncPageT","title":"SyncPageT  <code>module-attribute</code>","text":"<pre><code>SyncPageT = TypeVar('SyncPageT', bound='BaseSyncPage[Any]')\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.log","title":"log  <code>module-attribute</code>","text":"<pre><code>log: Logger = getLogger(__name__)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient","title":"AsyncAPIClient","text":"<pre><code>AsyncAPIClient(\n    *,\n    version: str,\n    base_url: str | URL,\n    _strict_response_validation: bool,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    transport: AsyncTransport | None = None,\n    proxies: ProxiesTypes | None = None,\n    limits: Limits | None = None,\n    http_client: AsyncClient | None = None,\n    custom_headers: Mapping[str, str] | None = None,\n    custom_query: Mapping[str, object] | None = None\n)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the underlying HTTPX client.</p> <p>The client will not be usable after this.</p>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    options: RequestOptions = {}\n) -&gt; ResponseT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.get","title":"get  <code>async</code>","text":"<pre><code>get(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    options: RequestOptions = {},\n    stream: bool = False,\n    stream_cls: type[_AsyncStreamT] | None = None\n) -&gt; ResponseT | _AsyncStreamT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.get_api_list","title":"get_api_list","text":"<pre><code>get_api_list(\n    path: str,\n    *,\n    model: Type[_T],\n    page: Type[AsyncPageT],\n    body: Body | None = None,\n    options: RequestOptions = {},\n    method: str = \"get\"\n) -&gt; AsyncPaginator[_T, AsyncPageT]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.is_closed","title":"is_closed","text":"<pre><code>is_closed() -&gt; bool\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.patch","title":"patch  <code>async</code>","text":"<pre><code>patch(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    options: RequestOptions = {}\n) -&gt; ResponseT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.post","title":"post  <code>async</code>","text":"<pre><code>post(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    files: RequestFiles | None = None,\n    options: RequestOptions = {},\n    stream: bool = False,\n    stream_cls: type[_AsyncStreamT] | None = None\n) -&gt; ResponseT | _AsyncStreamT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.put","title":"put  <code>async</code>","text":"<pre><code>put(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    files: RequestFiles | None = None,\n    options: RequestOptions = {}\n) -&gt; ResponseT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncAPIClient.request","title":"request  <code>async</code>","text":"<pre><code>request(\n    cast_to: Type[ResponseT],\n    options: FinalRequestOptions,\n    *,\n    stream: bool = False,\n    stream_cls: type[_AsyncStreamT] | None = None,\n    remaining_retries: Optional[int] = None\n) -&gt; ResponseT | _AsyncStreamT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.AsyncHttpxClientWrapper","title":"AsyncHttpxClientWrapper","text":""},{"location":"_base_client/#src.openai._base_client.AsyncPaginator","title":"AsyncPaginator","text":"<pre><code>AsyncPaginator(\n    client: AsyncAPIClient,\n    options: FinalRequestOptions,\n    page_cls: Type[AsyncPageT],\n    model: Type[_T],\n)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseAsyncPage","title":"BaseAsyncPage","text":""},{"location":"_base_client/#src.openai._base_client.BaseAsyncPage.get_next_page","title":"get_next_page  <code>async</code>","text":"<pre><code>get_next_page() -&gt; AsyncPageT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseAsyncPage.iter_pages","title":"iter_pages  <code>async</code>","text":"<pre><code>iter_pages() -&gt; AsyncIterator[AsyncPageT]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient","title":"BaseClient","text":"<pre><code>BaseClient(\n    *,\n    version: str,\n    base_url: str | URL,\n    _strict_response_validation: bool,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    timeout: float | Timeout | None = DEFAULT_TIMEOUT,\n    limits: Limits,\n    transport: Transport | AsyncTransport | None,\n    proxies: ProxiesTypes | None,\n    custom_headers: Mapping[str, str] | None = None,\n    custom_query: Mapping[str, object] | None = None\n)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.auth_headers","title":"auth_headers  <code>property</code>","text":"<pre><code>auth_headers: dict[str, str]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.base_url","title":"base_url  <code>property</code> <code>writable</code>","text":"<pre><code>base_url: URL\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.custom_auth","title":"custom_auth  <code>property</code>","text":"<pre><code>custom_auth: Auth | None\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.default_headers","title":"default_headers  <code>property</code>","text":"<pre><code>default_headers: dict[str, str | Omit]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.max_retries","title":"max_retries  <code>instance-attribute</code>","text":"<pre><code>max_retries: int = max_retries\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.qs","title":"qs  <code>property</code>","text":"<pre><code>qs: Querystring\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: Union[float, Timeout, None] = timeout\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.user_agent","title":"user_agent  <code>property</code>","text":"<pre><code>user_agent: str\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseClient.platform_headers","title":"platform_headers","text":"<pre><code>platform_headers() -&gt; Dict[str, str]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BasePage","title":"BasePage","text":"<p>Defines the core interface for pagination.</p> Type Args <p>ModelT: The pydantic model that represents an item in the response.</p> <p>Methods:</p> Name Description <code>has_next_page</code> <p>Check if there is another page available</p> <code>next_page_info</code> <p>Get the necessary information to make a request for the next page</p>"},{"location":"_base_client/#src.openai._base_client.BasePage.has_next_page","title":"has_next_page","text":"<pre><code>has_next_page() -&gt; bool\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BasePage.next_page_info","title":"next_page_info","text":"<pre><code>next_page_info() -&gt; Optional[PageInfo]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseSyncPage","title":"BaseSyncPage","text":""},{"location":"_base_client/#src.openai._base_client.BaseSyncPage.get_next_page","title":"get_next_page","text":"<pre><code>get_next_page() -&gt; SyncPageT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.BaseSyncPage.iter_pages","title":"iter_pages","text":"<pre><code>iter_pages() -&gt; Iterator[SyncPageT]\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.OtherArch","title":"OtherArch","text":"<pre><code>OtherArch(name: str)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.OtherArch.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.OtherPlatform","title":"OtherPlatform","text":"<pre><code>OtherPlatform(name: str)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.OtherPlatform.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.PageInfo","title":"PageInfo","text":"<pre><code>PageInfo(\n    *,\n    url: URL | NotGiven = NOT_GIVEN,\n    params: Query | NotGiven = NOT_GIVEN\n)\n</code></pre> <p>Stores the necessary information to build the request to retrieve the next page.</p> <p>Either <code>url</code> or <code>params</code> must be set.</p>"},{"location":"_base_client/#src.openai._base_client.PageInfo.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Query | NotGiven = params\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.PageInfo.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: URL | NotGiven = url\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient","title":"SyncAPIClient","text":"<pre><code>SyncAPIClient(\n    *,\n    version: str,\n    base_url: str | URL,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    transport: Transport | None = None,\n    proxies: ProxiesTypes | None = None,\n    limits: Limits | None = None,\n    http_client: Client | None = None,\n    custom_headers: Mapping[str, str] | None = None,\n    custom_query: Mapping[str, object] | None = None,\n    _strict_response_validation: bool\n)\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the underlying HTTPX client.</p> <p>The client will not be usable after this.</p>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.delete","title":"delete","text":"<pre><code>delete(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    options: RequestOptions = {}\n) -&gt; ResponseT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.get","title":"get","text":"<pre><code>get(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    options: RequestOptions = {},\n    stream: bool = False,\n    stream_cls: type[_StreamT] | None = None\n) -&gt; ResponseT | _StreamT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.get_api_list","title":"get_api_list","text":"<pre><code>get_api_list(\n    path: str,\n    *,\n    model: Type[object],\n    page: Type[SyncPageT],\n    body: Body | None = None,\n    options: RequestOptions = {},\n    method: str = \"get\"\n) -&gt; SyncPageT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.is_closed","title":"is_closed","text":"<pre><code>is_closed() -&gt; bool\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.patch","title":"patch","text":"<pre><code>patch(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    options: RequestOptions = {}\n) -&gt; ResponseT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.post","title":"post","text":"<pre><code>post(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    options: RequestOptions = {},\n    files: RequestFiles | None = None,\n    stream: bool = False,\n    stream_cls: type[_StreamT] | None = None\n) -&gt; ResponseT | _StreamT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.put","title":"put","text":"<pre><code>put(\n    path: str,\n    *,\n    cast_to: Type[ResponseT],\n    body: Body | None = None,\n    files: RequestFiles | None = None,\n    options: RequestOptions = {}\n) -&gt; ResponseT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncAPIClient.request","title":"request","text":"<pre><code>request(\n    cast_to: Type[ResponseT],\n    options: FinalRequestOptions,\n    remaining_retries: Optional[int] = None,\n    *,\n    stream: bool = False,\n    stream_cls: type[_StreamT] | None = None\n) -&gt; ResponseT | _StreamT\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.SyncHttpxClientWrapper","title":"SyncHttpxClientWrapper","text":""},{"location":"_base_client/#src.openai._base_client.get_architecture","title":"get_architecture","text":"<pre><code>get_architecture() -&gt; Arch\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.get_platform","title":"get_platform","text":"<pre><code>get_platform() -&gt; Platform\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.get_python_runtime","title":"get_python_runtime","text":"<pre><code>get_python_runtime() -&gt; str\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.get_python_version","title":"get_python_version","text":"<pre><code>get_python_version() -&gt; str\n</code></pre>"},{"location":"_base_client/#src.openai._base_client.make_request_options","title":"make_request_options","text":"<pre><code>make_request_options(\n    *,\n    query: Query | None = None,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    idempotency_key: str | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    post_parser: PostParser | NotGiven = NOT_GIVEN\n) -&gt; RequestOptions\n</code></pre> <p>Create a dict of type RequestOptions without keys of NotGiven values.</p>"},{"location":"_base_client/#src.openai._base_client.platform_headers","title":"platform_headers  <code>cached</code>","text":"<pre><code>platform_headers(version: str) -&gt; Dict[str, str]\n</code></pre>"},{"location":"_client/","title":"client","text":""},{"location":"_client/#src.openai._client","title":"_client","text":""},{"location":"_client/#src.openai._client.AsyncClient","title":"AsyncClient  <code>module-attribute</code>","text":"<pre><code>AsyncClient = AsyncOpenAI\n</code></pre>"},{"location":"_client/#src.openai._client.Client","title":"Client  <code>module-attribute</code>","text":"<pre><code>Client = OpenAI\n</code></pre>"},{"location":"_client/#src.openai._client.ProxiesTypes","title":"ProxiesTypes  <code>module-attribute</code>","text":"<pre><code>ProxiesTypes = Union[str, Proxy, ProxiesDict]\n</code></pre>"},{"location":"_client/#src.openai._client.Transport","title":"Transport  <code>module-attribute</code>","text":"<pre><code>Transport = BaseTransport\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI","title":"AsyncOpenAI","text":"<pre><code>AsyncOpenAI(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: Union[\n        float, Timeout, None, NotGiven\n    ] = NOT_GIVEN,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    http_client: AsyncClient | None = None,\n    _strict_response_validation: bool = False\n)\n</code></pre> <p>This automatically infers the following arguments from their corresponding environment variables if they are not provided: - <code>api_key</code> from <code>OPENAI_API_KEY</code> - <code>organization</code> from <code>OPENAI_ORG_ID</code></p>"},{"location":"_client/#src.openai._client.AsyncOpenAI.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: str = api_key\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: AsyncAudio = AsyncAudio(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.auth_headers","title":"auth_headers  <code>property</code>","text":"<pre><code>auth_headers: dict[str, str]\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta: AsyncBeta = AsyncBeta(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat: AsyncChat = AsyncChat(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions: AsyncCompletions = AsyncCompletions(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.default_headers","title":"default_headers  <code>property</code>","text":"<pre><code>default_headers: dict[str, str | Omit]\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings: AsyncEmbeddings = AsyncEmbeddings(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: AsyncFiles = AsyncFiles(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning: AsyncFineTuning = AsyncFineTuning(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images: AsyncImages = AsyncImages(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models: AsyncModels = AsyncModels(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations: AsyncModerations = AsyncModerations(self)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.organization","title":"organization  <code>instance-attribute</code>","text":"<pre><code>organization: str | None = organization\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.qs","title":"qs  <code>property</code>","text":"<pre><code>qs: Querystring\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.with_options","title":"with_options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>with_options = copy\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.with_raw_response","title":"with_raw_response  <code>instance-attribute</code>","text":"<pre><code>with_raw_response: AsyncOpenAIWithRawResponse = (\n    AsyncOpenAIWithRawResponse(self)\n)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.with_streaming_response","title":"with_streaming_response  <code>instance-attribute</code>","text":"<pre><code>with_streaming_response: AsyncOpenAIWithStreamedResponse = (\n    AsyncOpenAIWithStreamedResponse(self)\n)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAI.copy","title":"copy","text":"<pre><code>copy(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    http_client: AsyncClient | None = None,\n    max_retries: int | NotGiven = NOT_GIVEN,\n    default_headers: Mapping[str, str] | None = None,\n    set_default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    set_default_query: Mapping[str, object] | None = None,\n    _extra_kwargs: Mapping[str, Any] = {}\n) -&gt; Self\n</code></pre> <p>Create a new client instance re-using the same options given to the current client with optional overriding.</p>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse","title":"AsyncOpenAIWithRawResponse","text":"<pre><code>AsyncOpenAIWithRawResponse(client: AsyncOpenAI)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio = AsyncAudioWithRawResponse(audio)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = AsyncBetaWithRawResponse(beta)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat = AsyncChatWithRawResponse(chat)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions = AsyncCompletionsWithRawResponse(completions)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings = AsyncEmbeddingsWithRawResponse(embeddings)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files = AsyncFilesWithRawResponse(files)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning = AsyncFineTuningWithRawResponse(fine_tuning)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images = AsyncImagesWithRawResponse(images)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = AsyncModelsWithRawResponse(models)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithRawResponse.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations = AsyncModerationsWithRawResponse(moderations)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse","title":"AsyncOpenAIWithStreamedResponse","text":"<pre><code>AsyncOpenAIWithStreamedResponse(client: AsyncOpenAI)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio = AsyncAudioWithStreamingResponse(audio)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = AsyncBetaWithStreamingResponse(beta)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat = AsyncChatWithStreamingResponse(chat)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions = AsyncCompletionsWithStreamingResponse(\n    completions\n)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings = AsyncEmbeddingsWithStreamingResponse(\n    embeddings\n)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files = AsyncFilesWithStreamingResponse(files)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning = AsyncFineTuningWithStreamingResponse(\n    fine_tuning\n)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images = AsyncImagesWithStreamingResponse(images)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = AsyncModelsWithStreamingResponse(models)\n</code></pre>"},{"location":"_client/#src.openai._client.AsyncOpenAIWithStreamedResponse.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations = AsyncModerationsWithStreamingResponse(\n    moderations\n)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI","title":"OpenAI","text":"<pre><code>OpenAI(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: Union[\n        float, Timeout, None, NotGiven\n    ] = NOT_GIVEN,\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    http_client: Client | None = None,\n    _strict_response_validation: bool = False\n)\n</code></pre> <p>This automatically infers the following arguments from their corresponding environment variables if they are not provided: - <code>api_key</code> from <code>OPENAI_API_KEY</code> - <code>organization</code> from <code>OPENAI_ORG_ID</code></p>"},{"location":"_client/#src.openai._client.OpenAI.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: str = api_key\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: Audio = Audio(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.auth_headers","title":"auth_headers  <code>property</code>","text":"<pre><code>auth_headers: dict[str, str]\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta: Beta = Beta(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat: Chat = Chat(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions: Completions = Completions(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.default_headers","title":"default_headers  <code>property</code>","text":"<pre><code>default_headers: dict[str, str | Omit]\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings: Embeddings = Embeddings(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: Files = Files(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning: FineTuning = FineTuning(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images: Images = Images(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models: Models = Models(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations: Moderations = Moderations(self)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.organization","title":"organization  <code>instance-attribute</code>","text":"<pre><code>organization: str | None = organization\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.qs","title":"qs  <code>property</code>","text":"<pre><code>qs: Querystring\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.with_options","title":"with_options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>with_options = copy\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.with_raw_response","title":"with_raw_response  <code>instance-attribute</code>","text":"<pre><code>with_raw_response: OpenAIWithRawResponse = (\n    OpenAIWithRawResponse(self)\n)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.with_streaming_response","title":"with_streaming_response  <code>instance-attribute</code>","text":"<pre><code>with_streaming_response: OpenAIWithStreamedResponse = (\n    OpenAIWithStreamedResponse(self)\n)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAI.copy","title":"copy","text":"<pre><code>copy(\n    *,\n    api_key: str | None = None,\n    organization: str | None = None,\n    base_url: str | URL | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN,\n    http_client: Client | None = None,\n    max_retries: int | NotGiven = NOT_GIVEN,\n    default_headers: Mapping[str, str] | None = None,\n    set_default_headers: Mapping[str, str] | None = None,\n    default_query: Mapping[str, object] | None = None,\n    set_default_query: Mapping[str, object] | None = None,\n    _extra_kwargs: Mapping[str, Any] = {}\n) -&gt; Self\n</code></pre> <p>Create a new client instance re-using the same options given to the current client with optional overriding.</p>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse","title":"OpenAIWithRawResponse","text":"<pre><code>OpenAIWithRawResponse(client: OpenAI)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio = AudioWithRawResponse(audio)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = BetaWithRawResponse(beta)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat = ChatWithRawResponse(chat)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions = CompletionsWithRawResponse(completions)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings = EmbeddingsWithRawResponse(embeddings)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files = FilesWithRawResponse(files)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning = FineTuningWithRawResponse(fine_tuning)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images = ImagesWithRawResponse(images)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = ModelsWithRawResponse(models)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithRawResponse.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations = ModerationsWithRawResponse(moderations)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse","title":"OpenAIWithStreamedResponse","text":"<pre><code>OpenAIWithStreamedResponse(client: OpenAI)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio = AudioWithStreamingResponse(audio)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = BetaWithStreamingResponse(beta)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.chat","title":"chat  <code>instance-attribute</code>","text":"<pre><code>chat = ChatWithStreamingResponse(chat)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.completions","title":"completions  <code>instance-attribute</code>","text":"<pre><code>completions = CompletionsWithStreamingResponse(completions)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings = EmbeddingsWithStreamingResponse(embeddings)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files = FilesWithStreamingResponse(files)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.fine_tuning","title":"fine_tuning  <code>instance-attribute</code>","text":"<pre><code>fine_tuning = FineTuningWithStreamingResponse(fine_tuning)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images = ImagesWithStreamingResponse(images)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = ModelsWithStreamingResponse(models)\n</code></pre>"},{"location":"_client/#src.openai._client.OpenAIWithStreamedResponse.moderations","title":"moderations  <code>instance-attribute</code>","text":"<pre><code>moderations = ModerationsWithStreamingResponse(moderations)\n</code></pre>"},{"location":"_client/#src.openai._client.RequestOptions","title":"RequestOptions","text":""},{"location":"_client/#src.openai._client.RequestOptions.extra_json","title":"extra_json  <code>instance-attribute</code>","text":"<pre><code>extra_json: AnyMapping\n</code></pre>"},{"location":"_client/#src.openai._client.RequestOptions.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: Headers\n</code></pre>"},{"location":"_client/#src.openai._client.RequestOptions.idempotency_key","title":"idempotency_key  <code>instance-attribute</code>","text":"<pre><code>idempotency_key: str\n</code></pre>"},{"location":"_client/#src.openai._client.RequestOptions.max_retries","title":"max_retries  <code>instance-attribute</code>","text":"<pre><code>max_retries: int\n</code></pre>"},{"location":"_client/#src.openai._client.RequestOptions.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Query\n</code></pre>"},{"location":"_client/#src.openai._client.RequestOptions.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: float | Timeout | None\n</code></pre>"},{"location":"_compat/","title":"compat","text":""},{"location":"_compat/#src.openai._compat","title":"_compat","text":""},{"location":"_compat/#src.openai._compat.PYDANTIC_V2","title":"PYDANTIC_V2  <code>module-attribute</code>","text":"<pre><code>PYDANTIC_V2 = startswith('2.')\n</code></pre>"},{"location":"_compat/#src.openai._compat.GenericModel","title":"GenericModel","text":""},{"location":"_compat/#src.openai._compat.typed_cached_property","title":"typed_cached_property","text":"<pre><code>typed_cached_property(func: Callable[[Any], _T])\n</code></pre>"},{"location":"_compat/#src.openai._compat.typed_cached_property.attrname","title":"attrname  <code>instance-attribute</code>","text":"<pre><code>attrname: str | None\n</code></pre>"},{"location":"_compat/#src.openai._compat.typed_cached_property.func","title":"func  <code>instance-attribute</code>","text":"<pre><code>func: Callable[[Any], _T]\n</code></pre>"},{"location":"_compat/#src.openai._compat.field_get_default","title":"field_get_default","text":"<pre><code>field_get_default(field: FieldInfo) -&gt; Any\n</code></pre>"},{"location":"_compat/#src.openai._compat.field_is_required","title":"field_is_required","text":"<pre><code>field_is_required(field: FieldInfo) -&gt; bool\n</code></pre>"},{"location":"_compat/#src.openai._compat.field_outer_type","title":"field_outer_type","text":"<pre><code>field_outer_type(field: FieldInfo) -&gt; Any\n</code></pre>"},{"location":"_compat/#src.openai._compat.get_model_config","title":"get_model_config","text":"<pre><code>get_model_config(model: type[BaseModel]) -&gt; Any\n</code></pre>"},{"location":"_compat/#src.openai._compat.get_model_fields","title":"get_model_fields","text":"<pre><code>get_model_fields(\n    model: type[BaseModel],\n) -&gt; dict[str, FieldInfo]\n</code></pre>"},{"location":"_compat/#src.openai._compat.model_copy","title":"model_copy","text":"<pre><code>model_copy(model: _ModelT) -&gt; _ModelT\n</code></pre>"},{"location":"_compat/#src.openai._compat.model_dump","title":"model_dump","text":"<pre><code>model_dump(\n    model: BaseModel,\n    *,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False\n) -&gt; dict[str, Any]\n</code></pre>"},{"location":"_compat/#src.openai._compat.model_json","title":"model_json","text":"<pre><code>model_json(\n    model: BaseModel, *, indent: int | None = None\n) -&gt; str\n</code></pre>"},{"location":"_compat/#src.openai._compat.model_parse","title":"model_parse","text":"<pre><code>model_parse(model: type[_ModelT], data: Any) -&gt; _ModelT\n</code></pre>"},{"location":"_compat/#src.openai._compat.parse_obj","title":"parse_obj","text":"<pre><code>parse_obj(model: type[_ModelT], value: object) -&gt; _ModelT\n</code></pre>"},{"location":"_constants/","title":"constants","text":""},{"location":"_constants/#src.openai._constants","title":"_constants","text":""},{"location":"_constants/#src.openai._constants.DEFAULT_LIMITS","title":"DEFAULT_LIMITS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_LIMITS = Limits(\n    max_connections=100, max_keepalive_connections=20\n)\n</code></pre>"},{"location":"_constants/#src.openai._constants.DEFAULT_MAX_RETRIES","title":"DEFAULT_MAX_RETRIES  <code>module-attribute</code>","text":"<pre><code>DEFAULT_MAX_RETRIES = 2\n</code></pre>"},{"location":"_constants/#src.openai._constants.DEFAULT_TIMEOUT","title":"DEFAULT_TIMEOUT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TIMEOUT = Timeout(timeout=600.0, connect=5.0)\n</code></pre>"},{"location":"_constants/#src.openai._constants.INITIAL_RETRY_DELAY","title":"INITIAL_RETRY_DELAY  <code>module-attribute</code>","text":"<pre><code>INITIAL_RETRY_DELAY = 0.5\n</code></pre>"},{"location":"_constants/#src.openai._constants.MAX_RETRY_DELAY","title":"MAX_RETRY_DELAY  <code>module-attribute</code>","text":"<pre><code>MAX_RETRY_DELAY = 8.0\n</code></pre>"},{"location":"_constants/#src.openai._constants.OVERRIDE_CAST_TO_HEADER","title":"OVERRIDE_CAST_TO_HEADER  <code>module-attribute</code>","text":"<pre><code>OVERRIDE_CAST_TO_HEADER = '____stainless_override_cast_to'\n</code></pre>"},{"location":"_constants/#src.openai._constants.RAW_RESPONSE_HEADER","title":"RAW_RESPONSE_HEADER  <code>module-attribute</code>","text":"<pre><code>RAW_RESPONSE_HEADER = 'X-Stainless-Raw-Response'\n</code></pre>"},{"location":"_exceptions/","title":"exceptions","text":""},{"location":"_exceptions/#src.openai._exceptions","title":"_exceptions","text":""},{"location":"_exceptions/#src.openai._exceptions.APIConnectionError","title":"APIConnectionError","text":"<pre><code>APIConnectionError(\n    *, message: str = \"Connection error.\", request: Request\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIError","title":"APIError","text":"<pre><code>APIError(\n    message: str, request: Request, *, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIError.body","title":"body  <code>instance-attribute</code>","text":"<pre><code>body: object | None = body\n</code></pre> <p>The API response body.</p> <p>If the API responded with a valid JSON structure then this property will be the decoded result.</p> <p>If it isn't a valid JSON structure then this will be the raw response.</p> <p>If there was no response associated with this error then it will be <code>None</code>.</p>"},{"location":"_exceptions/#src.openai._exceptions.APIError.code","title":"code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>code: Optional[str] = None\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIError.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str = message\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIError.param","title":"param  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>param: Optional[str] = None\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIError.request","title":"request  <code>instance-attribute</code>","text":"<pre><code>request: Request = request\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIError.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Optional[str]\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIResponseValidationError","title":"APIResponseValidationError","text":"<pre><code>APIResponseValidationError(\n    response: Response,\n    body: object | None,\n    *,\n    message: str | None = None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIResponseValidationError.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIResponseValidationError.status_code","title":"status_code  <code>instance-attribute</code>","text":"<pre><code>status_code: int = status_code\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIStatusError","title":"APIStatusError","text":"<pre><code>APIStatusError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre> <p>Raised when an API response has a status code of 4xx or 5xx.</p>"},{"location":"_exceptions/#src.openai._exceptions.APIStatusError.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APIStatusError.status_code","title":"status_code  <code>instance-attribute</code>","text":"<pre><code>status_code: int = status_code\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.APITimeoutError","title":"APITimeoutError","text":"<pre><code>APITimeoutError(request: Request)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.AuthenticationError","title":"AuthenticationError","text":"<pre><code>AuthenticationError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.AuthenticationError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[401] = 401\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.BadRequestError","title":"BadRequestError","text":"<pre><code>BadRequestError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.BadRequestError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[400] = 400\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.ConflictError","title":"ConflictError","text":"<pre><code>ConflictError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.ConflictError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[409] = 409\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.InternalServerError","title":"InternalServerError","text":"<pre><code>InternalServerError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.NotFoundError","title":"NotFoundError","text":"<pre><code>NotFoundError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.NotFoundError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[404] = 404\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.OpenAIError","title":"OpenAIError","text":""},{"location":"_exceptions/#src.openai._exceptions.PermissionDeniedError","title":"PermissionDeniedError","text":"<pre><code>PermissionDeniedError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.PermissionDeniedError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[403] = 403\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.RateLimitError","title":"RateLimitError","text":"<pre><code>RateLimitError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.RateLimitError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[429] = 429\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.UnprocessableEntityError","title":"UnprocessableEntityError","text":"<pre><code>UnprocessableEntityError(\n    message: str, *, response: Response, body: object | None\n)\n</code></pre>"},{"location":"_exceptions/#src.openai._exceptions.UnprocessableEntityError.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_code: Literal[422] = 422\n</code></pre>"},{"location":"_files/","title":"files","text":""},{"location":"_files/#src.openai._files","title":"_files","text":""},{"location":"_files/#src.openai._files.assert_is_file_content","title":"assert_is_file_content","text":"<pre><code>assert_is_file_content(\n    obj: object, *, key: str | None = None\n) -&gt; None\n</code></pre>"},{"location":"_files/#src.openai._files.async_to_httpx_files","title":"async_to_httpx_files  <code>async</code>","text":"<pre><code>async_to_httpx_files(\n    files: RequestFiles | None,\n) -&gt; HttpxRequestFiles | None\n</code></pre>"},{"location":"_files/#src.openai._files.is_file_content","title":"is_file_content","text":"<pre><code>is_file_content(obj: object) -&gt; TypeGuard[FileContent]\n</code></pre>"},{"location":"_files/#src.openai._files.to_httpx_files","title":"to_httpx_files","text":"<pre><code>to_httpx_files(\n    files: RequestFiles | None,\n) -&gt; HttpxRequestFiles | None\n</code></pre>"},{"location":"_legacy_response/","title":"legacy response","text":""},{"location":"_legacy_response/#src.openai._legacy_response","title":"_legacy_response","text":""},{"location":"_legacy_response/#src.openai._legacy_response.P","title":"P  <code>module-attribute</code>","text":"<pre><code>P = ParamSpec('P')\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.R","title":"R  <code>module-attribute</code>","text":"<pre><code>R = TypeVar('R')\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.log","title":"log  <code>module-attribute</code>","text":"<pre><code>log: Logger = getLogger(__name__)\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent","title":"HttpxBinaryResponseContent","text":"<pre><code>HttpxBinaryResponseContent(response: Response)\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.charset_encoding","title":"charset_encoding  <code>property</code>","text":"<pre><code>charset_encoding: str | None\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.content","title":"content  <code>property</code>","text":"<pre><code>content: bytes\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str | None\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.aclose","title":"aclose  <code>async</code>","text":"<pre><code>aclose() -&gt; None\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.aiter_bytes","title":"aiter_bytes  <code>async</code>","text":"<pre><code>aiter_bytes(\n    chunk_size: int | None = None,\n) -&gt; AsyncIterator[bytes]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.aiter_lines","title":"aiter_lines  <code>async</code>","text":"<pre><code>aiter_lines() -&gt; AsyncIterator[str]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.aiter_raw","title":"aiter_raw  <code>async</code>","text":"<pre><code>aiter_raw(\n    chunk_size: int | None = None,\n) -&gt; AsyncIterator[bytes]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.aiter_text","title":"aiter_text  <code>async</code>","text":"<pre><code>aiter_text(\n    chunk_size: int | None = None,\n) -&gt; AsyncIterator[str]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.aread","title":"aread  <code>async</code>","text":"<pre><code>aread() -&gt; bytes\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.astream_to_file","title":"astream_to_file  <code>async</code>","text":"<pre><code>astream_to_file(\n    file: str | PathLike[str],\n    *,\n    chunk_size: int | None = None\n) -&gt; None\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.iter_bytes","title":"iter_bytes","text":"<pre><code>iter_bytes(\n    chunk_size: int | None = None,\n) -&gt; Iterator[bytes]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.iter_lines","title":"iter_lines","text":"<pre><code>iter_lines() -&gt; Iterator[str]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.iter_raw","title":"iter_raw","text":"<pre><code>iter_raw(chunk_size: int | None = None) -&gt; Iterator[bytes]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.iter_text","title":"iter_text","text":"<pre><code>iter_text(chunk_size: int | None = None) -&gt; Iterator[str]\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.json","title":"json","text":"<pre><code>json(**kwargs: Any) -&gt; Any\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.read","title":"read","text":"<pre><code>read() -&gt; bytes\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.stream_to_file","title":"stream_to_file","text":"<pre><code>stream_to_file(\n    file: str | PathLike[str],\n    *,\n    chunk_size: int | None = None\n) -&gt; None\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.HttpxBinaryResponseContent.write_to_file","title":"write_to_file","text":"<pre><code>write_to_file(file: str | PathLike[str]) -&gt; None\n</code></pre> <p>Write the output to the given file.</p> <p>Accepts a filename or any path-like object, e.g. pathlib.Path</p> <p>Note: if you want to stream the data to the file instead of writing all at once then you should use <code>.with_streaming_response</code> when making the API request, e.g. <code>client.with_streaming_response.foo().stream_to_file('my_filename.txt')</code></p>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse","title":"LegacyAPIResponse","text":"<pre><code>LegacyAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre> <p>This is a legacy class as it will be replaced by <code>APIResponse</code> and <code>AsyncAPIResponse</code> in the <code>_response.py</code> file in the next major release.</p> <p>For the sync client this will mostly be the same with the exception of <code>content</code> &amp; <code>text</code> will be methods instead of properties. In the async client, all methods will be async.</p> <p>A migration script will be provided &amp; the migration in general should be smooth.</p>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: bytes\n</code></pre> <p>Return the binary response content.</p> <p>NOTE: this will be removed in favour of <code>.read()</code> in the next major version.</p>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.elapsed","title":"elapsed  <code>property</code>","text":"<pre><code>elapsed: timedelta\n</code></pre> <p>The time taken for the complete request/response cycle to complete.</p>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: Headers\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.http_request","title":"http_request  <code>property</code>","text":"<pre><code>http_request: Request\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.http_response","title":"http_response  <code>instance-attribute</code>","text":"<pre><code>http_response: Response = raw\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.http_version","title":"http_version  <code>property</code>","text":"<pre><code>http_version: str\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.is_closed","title":"is_closed  <code>property</code>","text":"<pre><code>is_closed: bool\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.method","title":"method  <code>property</code>","text":"<pre><code>method: str\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.status_code","title":"status_code  <code>property</code>","text":"<pre><code>status_code: int\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre> <p>Return the decoded response content.</p> <p>NOTE: this will be turned into a method in the next major version.</p>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.url","title":"url  <code>property</code>","text":"<pre><code>url: URL\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.LegacyAPIResponse.parse","title":"parse","text":"<pre><code>parse(*, to: type[_T] | None = None) -&gt; R | _T\n</code></pre> <p>Returns the rich python representation of this response's data.</p> <p>NOTE: For the async client: this will become a coroutine in the next major version.</p> <p>For lower-level control, see <code>.read()</code>, <code>.json()</code>, <code>.iter_bytes()</code>.</p> <p>You can customise the type that the response is parsed into through the <code>to</code> argument, e.g.</p> <pre><code>from openai import BaseModel\n\n\nclass MyModel(BaseModel):\n    foo: str\n\n\nobj = response.parse(to=MyModel)\nprint(obj.foo)\n</code></pre> We support parsing <ul> <li><code>BaseModel</code></li> <li><code>dict</code></li> <li><code>list</code></li> <li><code>Union</code></li> <li><code>str</code></li> <li><code>httpx.Response</code></li> </ul>"},{"location":"_legacy_response/#src.openai._legacy_response.MissingStreamClassError","title":"MissingStreamClassError","text":"<pre><code>MissingStreamClassError()\n</code></pre>"},{"location":"_legacy_response/#src.openai._legacy_response.async_to_raw_response_wrapper","title":"async_to_raw_response_wrapper","text":"<pre><code>async_to_raw_response_wrapper(\n    func: Callable[P, Awaitable[R]]\n) -&gt; Callable[P, Awaitable[LegacyAPIResponse[R]]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and wraps it to support returning the raw <code>APIResponse</code> object directly.</p>"},{"location":"_legacy_response/#src.openai._legacy_response.to_raw_response_wrapper","title":"to_raw_response_wrapper","text":"<pre><code>to_raw_response_wrapper(\n    func: Callable[P, R]\n) -&gt; Callable[P, LegacyAPIResponse[R]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and wraps it to support returning the raw <code>APIResponse</code> object directly.</p>"},{"location":"_models/","title":"models","text":""},{"location":"_models/#src.openai._models","title":"_models","text":""},{"location":"_models/#src.openai._models.BaseModel","title":"BaseModel","text":""},{"location":"_models/#src.openai._models.BaseModel.model_config","title":"model_config  <code>class-attribute</code>","text":"<pre><code>model_config: ConfigDict = ConfigDict(extra='allow')\n</code></pre>"},{"location":"_models/#src.openai._models.BaseModel.model_construct","title":"model_construct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_construct = construct\n</code></pre>"},{"location":"_models/#src.openai._models.BaseModel.model_fields_set","title":"model_fields_set  <code>property</code>","text":"<pre><code>model_fields_set: set[str]\n</code></pre>"},{"location":"_models/#src.openai._models.BaseModel.Config","title":"Config","text":""},{"location":"_models/#src.openai._models.BaseModel.Config.extra","title":"extra  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra: Any = allow\n</code></pre>"},{"location":"_models/#src.openai._models.BaseModel.construct","title":"construct  <code>classmethod</code>","text":"<pre><code>construct(\n    _fields_set: set[str] | None = None, **values: object\n) -&gt; ModelT\n</code></pre>"},{"location":"_models/#src.openai._models.BaseModel.model_dump","title":"model_dump","text":"<pre><code>model_dump(\n    *,\n    mode: Literal[\"json\", \"python\"] | str = \"python\",\n    include: IncEx = None,\n    exclude: IncEx = None,\n    by_alias: bool = False,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    round_trip: bool = False,\n    warnings: bool = True\n) -&gt; dict[str, Any]\n</code></pre> <p>Usage docs: https://docs.pydantic.dev/2.4/concepts/serialization/#modelmodel_dump</p> <p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['json', 'python'] | str</code> <p>The mode in which <code>to_python</code> should run. If mode is 'json', the dictionary will only contain JSON serializable types. If mode is 'python', the dictionary may contain any Python objects.</p> <code>'python'</code> <code>include</code> <code>IncEx</code> <p>A list of fields to include in the output.</p> <code>None</code> <code>exclude</code> <code>IncEx</code> <p>A list of fields to exclude from the output.</p> <code>None</code> <code>by_alias</code> <code>bool</code> <p>Whether to use the field's alias in the dictionary key if defined.</p> <code>False</code> <code>exclude_unset</code> <code>bool</code> <p>Whether to exclude fields that are unset or None from the output.</p> <code>False</code> <code>exclude_defaults</code> <code>bool</code> <p>Whether to exclude fields that are set to their default value from the output.</p> <code>False</code> <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields that have a value of <code>None</code> from the output.</p> <code>False</code> <code>round_trip</code> <code>bool</code> <p>Whether to enable serialization and deserialization round-trip support.</p> <code>False</code> <code>warnings</code> <code>bool</code> <p>Whether to log warnings when invalid fields are encountered.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representation of the model.</p>"},{"location":"_models/#src.openai._models.BaseModel.model_dump_json","title":"model_dump_json","text":"<pre><code>model_dump_json(\n    *,\n    indent: int | None = None,\n    include: IncEx = None,\n    exclude: IncEx = None,\n    by_alias: bool = False,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    round_trip: bool = False,\n    warnings: bool = True\n) -&gt; str\n</code></pre> <p>Usage docs: https://docs.pydantic.dev/2.4/concepts/serialization/#modelmodel_dump_json</p> <p>Generates a JSON representation of the model using Pydantic's <code>to_json</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int | None</code> <p>Indentation to use in the JSON output. If None is passed, the output will be compact.</p> <code>None</code> <code>include</code> <code>IncEx</code> <p>Field(s) to include in the JSON output. Can take either a string or set of strings.</p> <code>None</code> <code>exclude</code> <code>IncEx</code> <p>Field(s) to exclude from the JSON output. Can take either a string or set of strings.</p> <code>None</code> <code>by_alias</code> <code>bool</code> <p>Whether to serialize using field aliases.</p> <code>False</code> <code>exclude_unset</code> <code>bool</code> <p>Whether to exclude fields that have not been explicitly set.</p> <code>False</code> <code>exclude_defaults</code> <code>bool</code> <p>Whether to exclude fields that have the default value.</p> <code>False</code> <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields that have a value of <code>None</code>.</p> <code>False</code> <code>round_trip</code> <code>bool</code> <p>Whether to use serialization/deserialization between JSON and class instance.</p> <code>False</code> <code>warnings</code> <code>bool</code> <p>Whether to show any warnings that occurred during serialization.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string representation of the model.</p>"},{"location":"_models/#src.openai._models.FinalRequestOptions","title":"FinalRequestOptions","text":""},{"location":"_models/#src.openai._models.FinalRequestOptions.extra_json","title":"extra_json  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_json: Union[AnyMapping, None] = None\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.files","title":"files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>files: Union[HttpxRequestFiles, None] = None\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.headers","title":"headers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>headers: Union[Headers, NotGiven] = NotGiven()\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.idempotency_key","title":"idempotency_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>idempotency_key: Union[str, None] = None\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.json_data","title":"json_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>json_data: Union[Body, None] = None\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.max_retries","title":"max_retries  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_retries: Union[int, NotGiven] = NotGiven()\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.model_config","title":"model_config  <code>class-attribute</code>","text":"<pre><code>model_config: ConfigDict = ConfigDict(\n    arbitrary_types_allowed=True\n)\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.model_construct","title":"model_construct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_construct = construct\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: Query = {}\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.post_parser","title":"post_parser  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>post_parser: Union[Callable[[Any], Any], NotGiven] = (\n    NotGiven()\n)\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: Union[float, Timeout, None, NotGiven] = NotGiven()\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.Config","title":"Config","text":""},{"location":"_models/#src.openai._models.FinalRequestOptions.Config.arbitrary_types_allowed","title":"arbitrary_types_allowed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arbitrary_types_allowed: bool = True\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.construct","title":"construct  <code>classmethod</code>","text":"<pre><code>construct(\n    _fields_set: set[str] | None = None,\n    **values: Unpack[FinalRequestOptionsInput]\n) -&gt; FinalRequestOptions\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptions.get_max_retries","title":"get_max_retries","text":"<pre><code>get_max_retries(max_retries: int) -&gt; int\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput","title":"FinalRequestOptionsInput","text":""},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.extra_json","title":"extra_json  <code>instance-attribute</code>","text":"<pre><code>extra_json: AnyMapping\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.files","title":"files  <code>instance-attribute</code>","text":"<pre><code>files: HttpxRequestFiles | None\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: Headers\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.idempotency_key","title":"idempotency_key  <code>instance-attribute</code>","text":"<pre><code>idempotency_key: str\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.json_data","title":"json_data  <code>instance-attribute</code>","text":"<pre><code>json_data: Body\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.max_retries","title":"max_retries  <code>instance-attribute</code>","text":"<pre><code>max_retries: int\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: Required[str]\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Query\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: float | Timeout | None\n</code></pre>"},{"location":"_models/#src.openai._models.FinalRequestOptionsInput.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: Required[str]\n</code></pre>"},{"location":"_models/#src.openai._models.GenericModel","title":"GenericModel","text":""},{"location":"_models/#src.openai._models.RootModel","title":"RootModel","text":"<p>Used as a placeholder to easily convert runtime types to a Pydantic format to provide validation.</p> <p>For example: <pre><code>validated = RootModel[int](__root__=\"5\").__root__\n# validated: 5\n</code></pre></p>"},{"location":"_models/#src.openai._models.construct_type","title":"construct_type","text":"<pre><code>construct_type(*, value: object, type_: type) -&gt; object\n</code></pre> <p>Loose coercion to the expected type with construction of nested values.</p> <p>If the given value does not match the expected type then it is returned as-is.</p>"},{"location":"_models/#src.openai._models.is_basemodel","title":"is_basemodel","text":"<pre><code>is_basemodel(type_: type) -&gt; bool\n</code></pre> <p>Returns whether or not the given type is either a <code>BaseModel</code> or a union of <code>BaseModel</code></p>"},{"location":"_models/#src.openai._models.validate_type","title":"validate_type","text":"<pre><code>validate_type(*, type_: type[_T], value: object) -&gt; _T\n</code></pre> <p>Strict validation that the given value matches the expected type</p>"},{"location":"_module_client/","title":"module client","text":""},{"location":"_module_client/#src.openai._module_client","title":"_module_client","text":""},{"location":"_module_client/#src.openai._module_client.audio","title":"audio  <code>module-attribute</code>","text":"<pre><code>audio: Audio = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.beta","title":"beta  <code>module-attribute</code>","text":"<pre><code>beta: Beta = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.chat","title":"chat  <code>module-attribute</code>","text":"<pre><code>chat: Chat = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.completions","title":"completions  <code>module-attribute</code>","text":"<pre><code>completions: Completions = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.embeddings","title":"embeddings  <code>module-attribute</code>","text":"<pre><code>embeddings: Embeddings = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.files","title":"files  <code>module-attribute</code>","text":"<pre><code>files: Files = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.fine_tuning","title":"fine_tuning  <code>module-attribute</code>","text":"<pre><code>fine_tuning: FineTuning = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.images","title":"images  <code>module-attribute</code>","text":"<pre><code>images: Images = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.models","title":"models  <code>module-attribute</code>","text":"<pre><code>models: Models = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.moderations","title":"moderations  <code>module-attribute</code>","text":"<pre><code>moderations: Moderations = __as_proxied__()\n</code></pre>"},{"location":"_module_client/#src.openai._module_client.AudioProxy","title":"AudioProxy","text":""},{"location":"_module_client/#src.openai._module_client.BetaProxy","title":"BetaProxy","text":""},{"location":"_module_client/#src.openai._module_client.ChatProxy","title":"ChatProxy","text":""},{"location":"_module_client/#src.openai._module_client.CompletionsProxy","title":"CompletionsProxy","text":""},{"location":"_module_client/#src.openai._module_client.EmbeddingsProxy","title":"EmbeddingsProxy","text":""},{"location":"_module_client/#src.openai._module_client.FilesProxy","title":"FilesProxy","text":""},{"location":"_module_client/#src.openai._module_client.FineTuningProxy","title":"FineTuningProxy","text":""},{"location":"_module_client/#src.openai._module_client.ImagesProxy","title":"ImagesProxy","text":""},{"location":"_module_client/#src.openai._module_client.ModelsProxy","title":"ModelsProxy","text":""},{"location":"_module_client/#src.openai._module_client.ModerationsProxy","title":"ModerationsProxy","text":""},{"location":"_qs/","title":"qs","text":""},{"location":"_qs/#src.openai._qs","title":"_qs","text":""},{"location":"_qs/#src.openai._qs.ArrayFormat","title":"ArrayFormat  <code>module-attribute</code>","text":"<pre><code>ArrayFormat = Literal[\n    \"comma\", \"repeat\", \"indices\", \"brackets\"\n]\n</code></pre>"},{"location":"_qs/#src.openai._qs.Data","title":"Data  <code>module-attribute</code>","text":"<pre><code>Data = Union[\n    PrimitiveData,\n    List[Any],\n    Tuple[Any],\n    \"Mapping[str, Any]\",\n]\n</code></pre>"},{"location":"_qs/#src.openai._qs.NestedFormat","title":"NestedFormat  <code>module-attribute</code>","text":"<pre><code>NestedFormat = Literal['dots', 'brackets']\n</code></pre>"},{"location":"_qs/#src.openai._qs.Params","title":"Params  <code>module-attribute</code>","text":"<pre><code>Params = Mapping[str, Data]\n</code></pre>"},{"location":"_qs/#src.openai._qs.PrimitiveData","title":"PrimitiveData  <code>module-attribute</code>","text":"<pre><code>PrimitiveData = Union[str, int, float, bool, None]\n</code></pre>"},{"location":"_qs/#src.openai._qs.parse","title":"parse  <code>module-attribute</code>","text":"<pre><code>parse = parse\n</code></pre>"},{"location":"_qs/#src.openai._qs.stringify","title":"stringify  <code>module-attribute</code>","text":"<pre><code>stringify = stringify\n</code></pre>"},{"location":"_qs/#src.openai._qs.stringify_items","title":"stringify_items  <code>module-attribute</code>","text":"<pre><code>stringify_items = stringify_items\n</code></pre>"},{"location":"_qs/#src.openai._qs.Options","title":"Options","text":"<pre><code>Options(\n    qs: Querystring = _qs,\n    *,\n    array_format: NotGivenOr[ArrayFormat] = NOT_GIVEN,\n    nested_format: NotGivenOr[NestedFormat] = NOT_GIVEN\n)\n</code></pre>"},{"location":"_qs/#src.openai._qs.Options.array_format","title":"array_format  <code>instance-attribute</code>","text":"<pre><code>array_format: ArrayFormat = (\n    array_format\n    if isinstance(array_format, NotGiven)\n    else array_format\n)\n</code></pre>"},{"location":"_qs/#src.openai._qs.Options.nested_format","title":"nested_format  <code>instance-attribute</code>","text":"<pre><code>nested_format: NestedFormat = (\n    nested_format\n    if isinstance(nested_format, NotGiven)\n    else nested_format\n)\n</code></pre>"},{"location":"_qs/#src.openai._qs.Querystring","title":"Querystring","text":"<pre><code>Querystring(\n    *,\n    array_format: ArrayFormat = \"repeat\",\n    nested_format: NestedFormat = \"brackets\"\n)\n</code></pre>"},{"location":"_qs/#src.openai._qs.Querystring.array_format","title":"array_format  <code>instance-attribute</code>","text":"<pre><code>array_format: ArrayFormat = array_format\n</code></pre>"},{"location":"_qs/#src.openai._qs.Querystring.nested_format","title":"nested_format  <code>instance-attribute</code>","text":"<pre><code>nested_format: NestedFormat = nested_format\n</code></pre>"},{"location":"_qs/#src.openai._qs.Querystring.parse","title":"parse","text":"<pre><code>parse(query: str) -&gt; Mapping[str, object]\n</code></pre>"},{"location":"_qs/#src.openai._qs.Querystring.stringify","title":"stringify","text":"<pre><code>stringify(\n    params: Params,\n    *,\n    array_format: NotGivenOr[ArrayFormat] = NOT_GIVEN,\n    nested_format: NotGivenOr[NestedFormat] = NOT_GIVEN\n) -&gt; str\n</code></pre>"},{"location":"_qs/#src.openai._qs.Querystring.stringify_items","title":"stringify_items","text":"<pre><code>stringify_items(\n    params: Params,\n    *,\n    array_format: NotGivenOr[ArrayFormat] = NOT_GIVEN,\n    nested_format: NotGivenOr[NestedFormat] = NOT_GIVEN\n) -&gt; list[tuple[str, str]]\n</code></pre>"},{"location":"_resource/","title":"resource","text":""},{"location":"_resource/#src.openai._resource","title":"_resource","text":""},{"location":"_resource/#src.openai._resource.AsyncAPIResource","title":"AsyncAPIResource","text":"<pre><code>AsyncAPIResource(client: AsyncOpenAI)\n</code></pre>"},{"location":"_resource/#src.openai._resource.SyncAPIResource","title":"SyncAPIResource","text":"<pre><code>SyncAPIResource(client: OpenAI)\n</code></pre>"},{"location":"_response/","title":"response","text":""},{"location":"_response/#src.openai._response","title":"_response","text":""},{"location":"_response/#src.openai._response.P","title":"P  <code>module-attribute</code>","text":"<pre><code>P = ParamSpec('P')\n</code></pre>"},{"location":"_response/#src.openai._response.R","title":"R  <code>module-attribute</code>","text":"<pre><code>R = TypeVar('R')\n</code></pre>"},{"location":"_response/#src.openai._response.log","title":"log  <code>module-attribute</code>","text":"<pre><code>log: Logger = getLogger(__name__)\n</code></pre>"},{"location":"_response/#src.openai._response.APIResponse","title":"APIResponse","text":"<pre><code>APIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre>"},{"location":"_response/#src.openai._response.APIResponse.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the response and release the connection.</p> <p>Automatically called if the response body is read to completion.</p>"},{"location":"_response/#src.openai._response.APIResponse.iter_bytes","title":"iter_bytes","text":"<pre><code>iter_bytes(\n    chunk_size: int | None = None,\n) -&gt; Iterator[bytes]\n</code></pre> <p>A byte-iterator over the decoded response content.</p> <p>This automatically handles gzip, deflate and brotli encoded responses.</p>"},{"location":"_response/#src.openai._response.APIResponse.iter_lines","title":"iter_lines","text":"<pre><code>iter_lines() -&gt; Iterator[str]\n</code></pre> <p>Like <code>iter_text()</code> but will only yield chunks for each line</p>"},{"location":"_response/#src.openai._response.APIResponse.iter_text","title":"iter_text","text":"<pre><code>iter_text(chunk_size: int | None = None) -&gt; Iterator[str]\n</code></pre> <p>A str-iterator over the decoded response content that handles both gzip, deflate, etc but also detects the content's string encoding.</p>"},{"location":"_response/#src.openai._response.APIResponse.json","title":"json","text":"<pre><code>json() -&gt; object\n</code></pre> <p>Read and decode the JSON response content.</p>"},{"location":"_response/#src.openai._response.APIResponse.parse","title":"parse","text":"<pre><code>parse(*, to: type[_T] | None = None) -&gt; R | _T\n</code></pre> <p>Returns the rich python representation of this response's data.</p> <p>For lower-level control, see <code>.read()</code>, <code>.json()</code>, <code>.iter_bytes()</code>.</p> <p>You can customise the type that the response is parsed into through the <code>to</code> argument, e.g.</p> <pre><code>from openai import BaseModel\n\n\nclass MyModel(BaseModel):\n    foo: str\n\n\nobj = response.parse(to=MyModel)\nprint(obj.foo)\n</code></pre> We support parsing <ul> <li><code>BaseModel</code></li> <li><code>dict</code></li> <li><code>list</code></li> <li><code>Union</code></li> <li><code>str</code></li> <li><code>httpx.Response</code></li> </ul>"},{"location":"_response/#src.openai._response.APIResponse.read","title":"read","text":"<pre><code>read() -&gt; bytes\n</code></pre> <p>Read and return the binary response content.</p>"},{"location":"_response/#src.openai._response.APIResponse.text","title":"text","text":"<pre><code>text() -&gt; str\n</code></pre> <p>Read and decode the response content into a string.</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse","title":"AsyncAPIResponse","text":"<pre><code>AsyncAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the response and release the connection.</p> <p>Automatically called if the response body is read to completion.</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.iter_bytes","title":"iter_bytes  <code>async</code>","text":"<pre><code>iter_bytes(\n    chunk_size: int | None = None,\n) -&gt; AsyncIterator[bytes]\n</code></pre> <p>A byte-iterator over the decoded response content.</p> <p>This automatically handles gzip, deflate and brotli encoded responses.</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.iter_lines","title":"iter_lines  <code>async</code>","text":"<pre><code>iter_lines() -&gt; AsyncIterator[str]\n</code></pre> <p>Like <code>iter_text()</code> but will only yield chunks for each line</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.iter_text","title":"iter_text  <code>async</code>","text":"<pre><code>iter_text(\n    chunk_size: int | None = None,\n) -&gt; AsyncIterator[str]\n</code></pre> <p>A str-iterator over the decoded response content that handles both gzip, deflate, etc but also detects the content's string encoding.</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.json","title":"json  <code>async</code>","text":"<pre><code>json() -&gt; object\n</code></pre> <p>Read and decode the JSON response content.</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.parse","title":"parse  <code>async</code>","text":"<pre><code>parse(*, to: type[_T] | None = None) -&gt; R | _T\n</code></pre> <p>Returns the rich python representation of this response's data.</p> <p>For lower-level control, see <code>.read()</code>, <code>.json()</code>, <code>.iter_bytes()</code>.</p> <p>You can customise the type that the response is parsed into through the <code>to</code> argument, e.g.</p> <pre><code>from openai import BaseModel\n\n\nclass MyModel(BaseModel):\n    foo: str\n\n\nobj = response.parse(to=MyModel)\nprint(obj.foo)\n</code></pre> We support parsing <ul> <li><code>BaseModel</code></li> <li><code>dict</code></li> <li><code>list</code></li> <li><code>Union</code></li> <li><code>str</code></li> <li><code>httpx.Response</code></li> </ul>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.read","title":"read  <code>async</code>","text":"<pre><code>read() -&gt; bytes\n</code></pre> <p>Read and return the binary response content.</p>"},{"location":"_response/#src.openai._response.AsyncAPIResponse.text","title":"text  <code>async</code>","text":"<pre><code>text() -&gt; str\n</code></pre> <p>Read and decode the response content into a string.</p>"},{"location":"_response/#src.openai._response.AsyncBinaryAPIResponse","title":"AsyncBinaryAPIResponse","text":"<pre><code>AsyncBinaryAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre> <p>Subclass of APIResponse providing helpers for dealing with binary data.</p> <p>Note: If you want to stream the response data instead of eagerly reading it all at once then you should use <code>.with_streaming_response</code> when making the API request, e.g. <code>.with_streaming_response.get_binary_response()</code></p>"},{"location":"_response/#src.openai._response.AsyncBinaryAPIResponse.write_to_file","title":"write_to_file  <code>async</code>","text":"<pre><code>write_to_file(file: str | PathLike[str]) -&gt; None\n</code></pre> <p>Write the output to the given file.</p> <p>Accepts a filename or any path-like object, e.g. pathlib.Path</p> <p>Note: if you want to stream the data to the file instead of writing all at once then you should use <code>.with_streaming_response</code> when making the API request, e.g. <code>.with_streaming_response.get_binary_response()</code></p>"},{"location":"_response/#src.openai._response.AsyncResponseContextManager","title":"AsyncResponseContextManager","text":"<pre><code>AsyncResponseContextManager(\n    api_request: Awaitable[_AsyncAPIResponseT],\n)\n</code></pre> <p>Context manager for ensuring that a request is not made until it is entered and that the response will always be closed when the context manager exits</p>"},{"location":"_response/#src.openai._response.AsyncStreamedBinaryAPIResponse","title":"AsyncStreamedBinaryAPIResponse","text":"<pre><code>AsyncStreamedBinaryAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre>"},{"location":"_response/#src.openai._response.AsyncStreamedBinaryAPIResponse.stream_to_file","title":"stream_to_file  <code>async</code>","text":"<pre><code>stream_to_file(\n    file: str | PathLike[str],\n    *,\n    chunk_size: int | None = None\n) -&gt; None\n</code></pre> <p>Streams the output to the given file.</p> <p>Accepts a filename or any path-like object, e.g. pathlib.Path</p>"},{"location":"_response/#src.openai._response.BaseAPIResponse","title":"BaseAPIResponse","text":"<pre><code>BaseAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre>"},{"location":"_response/#src.openai._response.BaseAPIResponse.elapsed","title":"elapsed  <code>property</code>","text":"<pre><code>elapsed: timedelta\n</code></pre> <p>The time taken for the complete request/response cycle to complete.</p>"},{"location":"_response/#src.openai._response.BaseAPIResponse.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: Headers\n</code></pre>"},{"location":"_response/#src.openai._response.BaseAPIResponse.http_request","title":"http_request  <code>property</code>","text":"<pre><code>http_request: Request\n</code></pre> <p>Returns the httpx Request instance associated with the current response.</p>"},{"location":"_response/#src.openai._response.BaseAPIResponse.http_response","title":"http_response  <code>instance-attribute</code>","text":"<pre><code>http_response: Response = raw\n</code></pre>"},{"location":"_response/#src.openai._response.BaseAPIResponse.http_version","title":"http_version  <code>property</code>","text":"<pre><code>http_version: str\n</code></pre>"},{"location":"_response/#src.openai._response.BaseAPIResponse.is_closed","title":"is_closed  <code>property</code>","text":"<pre><code>is_closed: bool\n</code></pre> <p>Whether or not the response body has been closed.</p> <p>If this is False then there is response data that has not been read yet. You must either fully consume the response body or call <code>.close()</code> before discarding the response to prevent resource leaks.</p>"},{"location":"_response/#src.openai._response.BaseAPIResponse.method","title":"method  <code>property</code>","text":"<pre><code>method: str\n</code></pre>"},{"location":"_response/#src.openai._response.BaseAPIResponse.status_code","title":"status_code  <code>property</code>","text":"<pre><code>status_code: int\n</code></pre>"},{"location":"_response/#src.openai._response.BaseAPIResponse.url","title":"url  <code>property</code>","text":"<pre><code>url: URL\n</code></pre> <p>Returns the URL for which the request was made.</p>"},{"location":"_response/#src.openai._response.BinaryAPIResponse","title":"BinaryAPIResponse","text":"<pre><code>BinaryAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre> <p>Subclass of APIResponse providing helpers for dealing with binary data.</p> <p>Note: If you want to stream the response data instead of eagerly reading it all at once then you should use <code>.with_streaming_response</code> when making the API request, e.g. <code>.with_streaming_response.get_binary_response()</code></p>"},{"location":"_response/#src.openai._response.BinaryAPIResponse.write_to_file","title":"write_to_file","text":"<pre><code>write_to_file(file: str | PathLike[str]) -&gt; None\n</code></pre> <p>Write the output to the given file.</p> <p>Accepts a filename or any path-like object, e.g. pathlib.Path</p> <p>Note: if you want to stream the data to the file instead of writing all at once then you should use <code>.with_streaming_response</code> when making the API request, e.g. <code>.with_streaming_response.get_binary_response()</code></p>"},{"location":"_response/#src.openai._response.MissingStreamClassError","title":"MissingStreamClassError","text":"<pre><code>MissingStreamClassError()\n</code></pre>"},{"location":"_response/#src.openai._response.ResponseContextManager","title":"ResponseContextManager","text":"<pre><code>ResponseContextManager(\n    request_func: Callable[[], _APIResponseT]\n)\n</code></pre> <p>Context manager for ensuring that a request is not made until it is entered and that the response will always be closed when the context manager exits</p>"},{"location":"_response/#src.openai._response.StreamAlreadyConsumed","title":"StreamAlreadyConsumed","text":"<pre><code>StreamAlreadyConsumed()\n</code></pre> <p>Attempted to read or stream content, but the content has already been streamed.</p> <p>This can happen if you use a method like <code>.iter_lines()</code> and then attempt to read th entire response body afterwards, e.g.</p> <pre><code>response = await client.post(...)\nasync for line in response.iter_lines():\n    ...  # do something with `line`\n\ncontent = await response.read()\n# ^ error\n</code></pre> <p>If you want this behaviour you'll need to either manually accumulate the response content or call <code>await response.read()</code> before iterating over the stream.</p>"},{"location":"_response/#src.openai._response.StreamedBinaryAPIResponse","title":"StreamedBinaryAPIResponse","text":"<pre><code>StreamedBinaryAPIResponse(\n    *,\n    raw: Response,\n    cast_to: type[R],\n    client: BaseClient[Any, Any],\n    stream: bool,\n    stream_cls: (\n        type[Stream[Any]] | type[AsyncStream[Any]] | None\n    ),\n    options: FinalRequestOptions\n)\n</code></pre>"},{"location":"_response/#src.openai._response.StreamedBinaryAPIResponse.stream_to_file","title":"stream_to_file","text":"<pre><code>stream_to_file(\n    file: str | PathLike[str],\n    *,\n    chunk_size: int | None = None\n) -&gt; None\n</code></pre> <p>Streams the output to the given file.</p> <p>Accepts a filename or any path-like object, e.g. pathlib.Path</p>"},{"location":"_response/#src.openai._response.async_to_custom_raw_response_wrapper","title":"async_to_custom_raw_response_wrapper","text":"<pre><code>async_to_custom_raw_response_wrapper(\n    func: Callable[P, Awaitable[object]],\n    response_cls: type[_AsyncAPIResponseT],\n) -&gt; Callable[P, Awaitable[_AsyncAPIResponseT]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and an <code>APIResponse</code> class and wraps the method to support returning the given response class directly.</p> <p>Note: the given <code>response_cls</code> must be concrete, e.g. <code>class BinaryAPIResponse(APIResponse[bytes])</code></p>"},{"location":"_response/#src.openai._response.async_to_custom_streamed_response_wrapper","title":"async_to_custom_streamed_response_wrapper","text":"<pre><code>async_to_custom_streamed_response_wrapper(\n    func: Callable[P, Awaitable[object]],\n    response_cls: type[_AsyncAPIResponseT],\n) -&gt; Callable[\n    P, AsyncResponseContextManager[_AsyncAPIResponseT]\n]\n</code></pre> <p>Higher order function that takes one of our bound API methods and an <code>APIResponse</code> class and wraps the method to support streaming and returning the given response class directly.</p> <p>Note: the given <code>response_cls</code> must be concrete, e.g. <code>class BinaryAPIResponse(APIResponse[bytes])</code></p>"},{"location":"_response/#src.openai._response.async_to_raw_response_wrapper","title":"async_to_raw_response_wrapper","text":"<pre><code>async_to_raw_response_wrapper(\n    func: Callable[P, Awaitable[R]]\n) -&gt; Callable[P, Awaitable[AsyncAPIResponse[R]]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and wraps it to support returning the raw <code>APIResponse</code> object directly.</p>"},{"location":"_response/#src.openai._response.async_to_streamed_response_wrapper","title":"async_to_streamed_response_wrapper","text":"<pre><code>async_to_streamed_response_wrapper(\n    func: Callable[P, Awaitable[R]]\n) -&gt; Callable[\n    P, AsyncResponseContextManager[AsyncAPIResponse[R]]\n]\n</code></pre> <p>Higher order function that takes one of our bound API methods and wraps it to support streaming and returning the raw <code>APIResponse</code> object directly.</p>"},{"location":"_response/#src.openai._response.extract_response_type","title":"extract_response_type","text":"<pre><code>extract_response_type(\n    typ: type[BaseAPIResponse[Any]],\n) -&gt; type\n</code></pre> <p>Given a type like <code>APIResponse[T]</code>, returns the generic type variable <code>T</code>.</p> <p>This also handles the case where a concrete subclass is given, e.g. <pre><code>class MyResponse(APIResponse[bytes]):\n    ...\n\nextract_response_type(MyResponse) -&gt; bytes\n</code></pre></p>"},{"location":"_response/#src.openai._response.to_custom_raw_response_wrapper","title":"to_custom_raw_response_wrapper","text":"<pre><code>to_custom_raw_response_wrapper(\n    func: Callable[P, object],\n    response_cls: type[_APIResponseT],\n) -&gt; Callable[P, _APIResponseT]\n</code></pre> <p>Higher order function that takes one of our bound API methods and an <code>APIResponse</code> class and wraps the method to support returning the given response class directly.</p> <p>Note: the given <code>response_cls</code> must be concrete, e.g. <code>class BinaryAPIResponse(APIResponse[bytes])</code></p>"},{"location":"_response/#src.openai._response.to_custom_streamed_response_wrapper","title":"to_custom_streamed_response_wrapper","text":"<pre><code>to_custom_streamed_response_wrapper(\n    func: Callable[P, object],\n    response_cls: type[_APIResponseT],\n) -&gt; Callable[P, ResponseContextManager[_APIResponseT]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and an <code>APIResponse</code> class and wraps the method to support streaming and returning the given response class directly.</p> <p>Note: the given <code>response_cls</code> must be concrete, e.g. <code>class BinaryAPIResponse(APIResponse[bytes])</code></p>"},{"location":"_response/#src.openai._response.to_raw_response_wrapper","title":"to_raw_response_wrapper","text":"<pre><code>to_raw_response_wrapper(\n    func: Callable[P, R]\n) -&gt; Callable[P, APIResponse[R]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and wraps it to support returning the raw <code>APIResponse</code> object directly.</p>"},{"location":"_response/#src.openai._response.to_streamed_response_wrapper","title":"to_streamed_response_wrapper","text":"<pre><code>to_streamed_response_wrapper(\n    func: Callable[P, R]\n) -&gt; Callable[P, ResponseContextManager[APIResponse[R]]]\n</code></pre> <p>Higher order function that takes one of our bound API methods and wraps it to support streaming and returning the raw <code>APIResponse</code> object directly.</p>"},{"location":"_streaming/","title":"streaming","text":""},{"location":"_streaming/#src.openai._streaming","title":"_streaming","text":""},{"location":"_streaming/#src.openai._streaming.AsyncStream","title":"AsyncStream","text":"<pre><code>AsyncStream(\n    *,\n    cast_to: type[_T],\n    response: Response,\n    client: AsyncOpenAI\n)\n</code></pre> <p>Provides the core interface to iterate over an asynchronous stream response.</p>"},{"location":"_streaming/#src.openai._streaming.AsyncStream.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.AsyncStream.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the response and release the connection.</p> <p>Automatically called if the response body is read to completion.</p>"},{"location":"_streaming/#src.openai._streaming.SSEDecoder","title":"SSEDecoder","text":"<pre><code>SSEDecoder()\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.SSEDecoder.aiter","title":"aiter  <code>async</code>","text":"<pre><code>aiter(\n    iterator: AsyncIterator[str],\n) -&gt; AsyncIterator[ServerSentEvent]\n</code></pre> <p>Given an async iterator that yields lines, iterate over it &amp; yield every event encountered</p>"},{"location":"_streaming/#src.openai._streaming.SSEDecoder.decode","title":"decode","text":"<pre><code>decode(line: str) -&gt; ServerSentEvent | None\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.SSEDecoder.iter","title":"iter","text":"<pre><code>iter(iterator: Iterator[str]) -&gt; Iterator[ServerSentEvent]\n</code></pre> <p>Given an iterator that yields lines, iterate over it &amp; yield every event encountered</p>"},{"location":"_streaming/#src.openai._streaming.ServerSentEvent","title":"ServerSentEvent","text":"<pre><code>ServerSentEvent(\n    *,\n    event: str | None = None,\n    data: str | None = None,\n    id: str | None = None,\n    retry: int | None = None\n)\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.ServerSentEvent.data","title":"data  <code>property</code>","text":"<pre><code>data: str\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.ServerSentEvent.event","title":"event  <code>property</code>","text":"<pre><code>event: str | None\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.ServerSentEvent.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.ServerSentEvent.retry","title":"retry  <code>property</code>","text":"<pre><code>retry: int | None\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.ServerSentEvent.json","title":"json","text":"<pre><code>json() -&gt; Any\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.Stream","title":"Stream","text":"<pre><code>Stream(\n    *, cast_to: type[_T], response: Response, client: OpenAI\n)\n</code></pre> <p>Provides the core interface to iterate over a synchronous stream response.</p>"},{"location":"_streaming/#src.openai._streaming.Stream.response","title":"response  <code>instance-attribute</code>","text":"<pre><code>response: Response = response\n</code></pre>"},{"location":"_streaming/#src.openai._streaming.Stream.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the response and release the connection.</p> <p>Automatically called if the response body is read to completion.</p>"},{"location":"_streaming/#src.openai._streaming.extract_stream_chunk_type","title":"extract_stream_chunk_type","text":"<pre><code>extract_stream_chunk_type(\n    stream_cls: type, *, failure_message: str | None = None\n) -&gt; type\n</code></pre> <p>Given a type like <code>Stream[T]</code>, returns the generic type variable <code>T</code>.</p> <p>This also handles the case where a concrete subclass is given, e.g. <pre><code>class MyStream(Stream[bytes]):\n    ...\n\nextract_stream_chunk_type(MyStream) -&gt; bytes\n</code></pre></p>"},{"location":"_streaming/#src.openai._streaming.is_stream_class_type","title":"is_stream_class_type","text":"<pre><code>is_stream_class_type(\n    typ: type,\n) -&gt; TypeGuard[\n    type[Stream[object]] | type[AsyncStream[object]]\n]\n</code></pre> <p>TypeGuard for determining whether or not the given type is a subclass of <code>Stream</code> / <code>AsyncStream</code></p>"},{"location":"_types/","title":"types","text":""},{"location":"_types/#src.openai._types","title":"_types","text":""},{"location":"_types/#src.openai._types.AnyMapping","title":"AnyMapping  <code>module-attribute</code>","text":"<pre><code>AnyMapping = Mapping[str, object]\n</code></pre>"},{"location":"_types/#src.openai._types.AsyncTransport","title":"AsyncTransport  <code>module-attribute</code>","text":"<pre><code>AsyncTransport = AsyncBaseTransport\n</code></pre>"},{"location":"_types/#src.openai._types.Body","title":"Body  <code>module-attribute</code>","text":"<pre><code>Body = object\n</code></pre>"},{"location":"_types/#src.openai._types.FileContent","title":"FileContent  <code>module-attribute</code>","text":"<pre><code>FileContent = Union[IO[bytes], bytes, PathLike[str]]\n</code></pre>"},{"location":"_types/#src.openai._types.FileTypes","title":"FileTypes  <code>module-attribute</code>","text":"<pre><code>FileTypes = Union[\n    FileContent,\n    Tuple[Optional[str], FileContent],\n    Tuple[Optional[str], FileContent, Optional[str]],\n    Tuple[\n        Optional[str],\n        FileContent,\n        Optional[str],\n        Mapping[str, str],\n    ],\n]\n</code></pre>"},{"location":"_types/#src.openai._types.Headers","title":"Headers  <code>module-attribute</code>","text":"<pre><code>Headers = Mapping[str, Union[str, Omit]]\n</code></pre>"},{"location":"_types/#src.openai._types.HeadersLike","title":"HeadersLike  <code>module-attribute</code>","text":"<pre><code>HeadersLike = Union[Headers, HeadersLikeProtocol]\n</code></pre>"},{"location":"_types/#src.openai._types.HttpxFileContent","title":"HttpxFileContent  <code>module-attribute</code>","text":"<pre><code>HttpxFileContent = Union[IO[bytes], bytes]\n</code></pre>"},{"location":"_types/#src.openai._types.HttpxFileTypes","title":"HttpxFileTypes  <code>module-attribute</code>","text":"<pre><code>HttpxFileTypes = Union[\n    HttpxFileContent,\n    Tuple[Optional[str], HttpxFileContent],\n    Tuple[Optional[str], HttpxFileContent, Optional[str]],\n    Tuple[\n        Optional[str],\n        HttpxFileContent,\n        Optional[str],\n        Mapping[str, str],\n    ],\n]\n</code></pre>"},{"location":"_types/#src.openai._types.HttpxRequestFiles","title":"HttpxRequestFiles  <code>module-attribute</code>","text":"<pre><code>HttpxRequestFiles = Union[\n    Mapping[str, HttpxFileTypes],\n    Sequence[Tuple[str, HttpxFileTypes]],\n]\n</code></pre>"},{"location":"_types/#src.openai._types.IncEx","title":"IncEx  <code>module-attribute</code>","text":"<pre><code>IncEx: TypeAlias = (\n    \"set[int] | set[str] | dict[int, Any] | dict[str, Any] | None\"\n)\n</code></pre>"},{"location":"_types/#src.openai._types.ModelT","title":"ModelT  <code>module-attribute</code>","text":"<pre><code>ModelT = TypeVar('ModelT', bound=BaseModel)\n</code></pre>"},{"location":"_types/#src.openai._types.NOT_GIVEN","title":"NOT_GIVEN  <code>module-attribute</code>","text":"<pre><code>NOT_GIVEN = NotGiven()\n</code></pre>"},{"location":"_types/#src.openai._types.NoneType","title":"NoneType  <code>module-attribute</code>","text":"<pre><code>NoneType: Type[None]\n</code></pre>"},{"location":"_types/#src.openai._types.NotGivenOr","title":"NotGivenOr  <code>module-attribute</code>","text":"<pre><code>NotGivenOr = Union[_T, NotGiven]\n</code></pre>"},{"location":"_types/#src.openai._types.PostParser","title":"PostParser  <code>module-attribute</code>","text":"<pre><code>PostParser = Callable[[Any], Any]\n</code></pre>"},{"location":"_types/#src.openai._types.ProxiesDict","title":"ProxiesDict  <code>module-attribute</code>","text":"<pre><code>ProxiesDict = Dict[\n    \"str | URL\", Union[None, str, URL, Proxy]\n]\n</code></pre>"},{"location":"_types/#src.openai._types.ProxiesTypes","title":"ProxiesTypes  <code>module-attribute</code>","text":"<pre><code>ProxiesTypes = Union[str, Proxy, ProxiesDict]\n</code></pre>"},{"location":"_types/#src.openai._types.Query","title":"Query  <code>module-attribute</code>","text":"<pre><code>Query = Mapping[str, object]\n</code></pre>"},{"location":"_types/#src.openai._types.RequestFiles","title":"RequestFiles  <code>module-attribute</code>","text":"<pre><code>RequestFiles = Union[\n    Mapping[str, FileTypes], Sequence[Tuple[str, FileTypes]]\n]\n</code></pre>"},{"location":"_types/#src.openai._types.ResponseT","title":"ResponseT  <code>module-attribute</code>","text":"<pre><code>ResponseT = TypeVar(\n    \"ResponseT\",\n    bound=Union[\n        object,\n        str,\n        None,\n        \"BaseModel\",\n        List[Any],\n        Dict[str, Any],\n        Response,\n        ModelBuilderProtocol,\n        \"APIResponse[Any]\",\n        \"AsyncAPIResponse[Any]\",\n        \"HttpxBinaryResponseContent\",\n    ],\n)\n</code></pre>"},{"location":"_types/#src.openai._types.StrBytesIntFloat","title":"StrBytesIntFloat  <code>module-attribute</code>","text":"<pre><code>StrBytesIntFloat = Union[str, bytes, int, float]\n</code></pre>"},{"location":"_types/#src.openai._types.Transport","title":"Transport  <code>module-attribute</code>","text":"<pre><code>Transport = BaseTransport\n</code></pre>"},{"location":"_types/#src.openai._types.HeadersLikeProtocol","title":"HeadersLikeProtocol","text":""},{"location":"_types/#src.openai._types.HeadersLikeProtocol.get","title":"get","text":"<pre><code>get(__key: str) -&gt; str | None\n</code></pre>"},{"location":"_types/#src.openai._types.HttpxSendArgs","title":"HttpxSendArgs","text":""},{"location":"_types/#src.openai._types.HttpxSendArgs.auth","title":"auth  <code>instance-attribute</code>","text":"<pre><code>auth: Auth\n</code></pre>"},{"location":"_types/#src.openai._types.InheritsGeneric","title":"InheritsGeneric","text":"<p>Represents a type that has inherited from <code>Generic</code></p> <p>The <code>__orig_bases__</code> property can be used to determine the resolved type variable for a given base class.</p>"},{"location":"_types/#src.openai._types.ModelBuilderProtocol","title":"ModelBuilderProtocol","text":""},{"location":"_types/#src.openai._types.ModelBuilderProtocol.build","title":"build  <code>classmethod</code>","text":"<pre><code>build(*, response: Response, data: object) -&gt; _T\n</code></pre>"},{"location":"_types/#src.openai._types.NotGiven","title":"NotGiven","text":"<p>A sentinel singleton class used to distinguish omitted keyword arguments from those passed in with the value None (which may have different behavior).</p> <p>For example:</p> <pre><code>def get(timeout: Union[int, NotGiven, None] = NotGiven()) -&gt; Response:\n    ...\n\n\nget(timeout=1)  # 1s timeout\nget(timeout=None)  # No timeout\nget()  # Default timeout behavior, which may not be statically known at the method definition.\n</code></pre>"},{"location":"_types/#src.openai._types.Omit","title":"Omit","text":"<p>In certain situations you need to be able to represent a case where a default value has to be explicitly removed and <code>None</code> is not an appropriate substitute, for example:</p> <pre><code># as the default `Content-Type` header is `application/json` that will be sent\nclient.post(\"/upload/files\", files={\"file\": b\"my raw file content\"})\n\n# you can't explicitly override the header as it has to be dynamically generated\n# to look something like: 'multipart/form-data; boundary=0d8382fcf5f8c3be01ca2e11002d2983'\nclient.post(..., headers={\"Content-Type\": \"multipart/form-data\"})\n\n# instead you can remove the default `application/json` header by passing Omit\nclient.post(..., headers={\"Content-Type\": Omit()})\n</code></pre>"},{"location":"_types/#src.openai._types.RequestOptions","title":"RequestOptions","text":""},{"location":"_types/#src.openai._types.RequestOptions.extra_json","title":"extra_json  <code>instance-attribute</code>","text":"<pre><code>extra_json: AnyMapping\n</code></pre>"},{"location":"_types/#src.openai._types.RequestOptions.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: Headers\n</code></pre>"},{"location":"_types/#src.openai._types.RequestOptions.idempotency_key","title":"idempotency_key  <code>instance-attribute</code>","text":"<pre><code>idempotency_key: str\n</code></pre>"},{"location":"_types/#src.openai._types.RequestOptions.max_retries","title":"max_retries  <code>instance-attribute</code>","text":"<pre><code>max_retries: int\n</code></pre>"},{"location":"_types/#src.openai._types.RequestOptions.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Query\n</code></pre>"},{"location":"_types/#src.openai._types.RequestOptions.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: float | Timeout | None\n</code></pre>"},{"location":"_version/","title":"version","text":""},{"location":"_version/#src.openai._version","title":"_version","text":""},{"location":"get_started/","title":"Get started","text":"<p>Quote</p> <p>What follows is a slightly modified version of the README.md file in the upstream repo. \u2014Marsh</p> <p></p> <p>The OpenAI Python library provides access to the OpenAI REST API from within applications based on Python 3.7 and above. The library includes type definitions for all request params and response fields, offering clients for both synchronous and asynchronous operations powered by httpx.</p> <p>The OpenAI Python library is generated from OpenAI's OpenAPI specification with Stainless.</p>"},{"location":"get_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>OpenAI API key</li> </ul>"},{"location":"get_started/#installation","title":"Installation","text":"<p>You can install the openai package from PyPi with <code>pip</code>:</p> <pre><code># Install the package\npip install openai\n</code></pre>"},{"location":"get_started/#migration","title":"Migration","text":"<p>Released on November 6th 2023, the OpenAI Python library was rewritten for v1. If your project used a pre-v1 version of the library, see the v1 migration guide for information and scripts that can help you update your code.</p>"},{"location":"get_started/#usage","title":"Usage","text":"<p>To connect to the OpenAI API:</p> <ol> <li>Populate an <code>OPENAI_API_KEY</code> environment variable with your OpenAI API key.</li> <li>Create a synchronous or asynchronous <code>OpenAI</code> client object.</li> </ol>"},{"location":"get_started/#synchronous-client","title":"Synchronous client","text":"<p>Create an instance of the OpenAI client:</p> <pre><code>import os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is default behavior - you can omit the 'api_key' parameter\n    # if the OPENAI_API_KEY environment variable contains a valid key.\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre> <p>Tip</p> <p>To reduce the risk of committing your OpenAI API key to source control, we recommend using python-dotenv and adding <code>OPENAI_API_KEY=\"YOUR_API_KEY_HERE\"</code> to your <code>.env</code> file.</p>"},{"location":"get_started/#asynchronous-client","title":"Asynchronous client","text":"<p>Create an instance of the AsyncOpenAI client and <code>await</code> each API call. Functionality between the synchronous and asynchronous clients is otherwise identical.</p> <pre><code>import os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is default behavior - you can omit the 'api_key' parameter\n    # if the OPENAI_API_KEY environment variable contains a valid key.\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -&gt; None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"get_started/#streaming-responses","title":"Streaming Responses","text":"<p>We provide support for streaming responses using Server Side Events (SSE).</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n</code></pre> <p>The async client uses the exact same interface.</p> <pre><code>from openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"get_started/#module-level-client","title":"Module-level client","text":"<p>Important</p> <p>We highly recommend instantiating client instances instead of relying on the global client.</p> <p>We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.</p> <pre><code>import openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n</code></pre> <p>The API is the exact same as the standard client instance based API.</p> <p>This is intended to be used within REPLs or notebooks for faster iteration, not in application code.</p> <p>We recommend that you always instantiate a client (e.g., with <code>client = OpenAI()</code>) in application code because:</p> <ul> <li>It can be difficult to reason about where client options are configured</li> <li>It's not possible to change certain client options without potentially causing race conditions</li> <li>It's harder to mock for testing purposes</li> <li>It's not possible to control cleanup of network connections</li> </ul>"},{"location":"get_started/#using-types","title":"Using types","text":"<p>Nested request parameters are TypedDicts. Responses are Pydantic models, which provide helper methods for things like:</p> <ul> <li>Serializing back into JSON, <code>model.model_dump_json(indent=2, exclude_unset=True)</code></li> <li>Converting to a dictionary, <code>model.model_dump(exclude_unset=True)</code></li> </ul> <p>Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set <code>python.analysis.typeCheckingMode</code> to <code>basic</code>.</p>"},{"location":"get_started/#pagination","title":"Pagination","text":"<p>List methods in the OpenAI API are paginated.</p> <p>This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:</p> <pre><code>import openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n</code></pre> <p>Or, asynchronously:</p> <pre><code>import asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -&gt; None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n</code></pre> <p>Alternatively, you can use the <code>.has_next_page()</code>, <code>.next_page_info()</code>, or <code>.get_next_page()</code> methods for more granular control working with pages:</p> <pre><code>first_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n</code></pre> <p>Or just work directly with the returned data:</p> <pre><code>first_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # =&gt; \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n</code></pre>"},{"location":"get_started/#nested-params","title":"Nested params","text":"<p>Nested parameters are dictionaries, typed using <code>TypedDict</code>, for example:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n</code></pre>"},{"location":"get_started/#file-uploads","title":"File Uploads","text":"<p>Request parameters that correspond to file uploads can be passed as <code>bytes</code>, a <code>PathLike</code> instance or a tuple of <code>(filename, contents, media type)</code>.</p> <pre><code>from pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n</code></pre> <p>The async client uses the exact same interface. If you pass a <code>PathLike</code> instance, the file contents will be read asynchronously automatically.</p>"},{"location":"get_started/#handling-errors","title":"Handling errors","text":"<p>When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of <code>openai.APIConnectionError</code> is raised.</p> <p>When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of <code>openai.APIStatusError</code> is raised, containing <code>status_code</code> and <code>response</code> properties.</p> <p>All errors inherit from <code>openai.APIError</code>.</p> <pre><code>import openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n</code></pre> <p>Error codes are as followed:</p> Status Code Error Type 400 <code>BadRequestError</code> 401 <code>AuthenticationError</code> 403 <code>PermissionDeniedError</code> 404 <code>NotFoundError</code> 422 <code>UnprocessableEntityError</code> 429 <code>RateLimitError</code> &gt;=500 <code>InternalServerError</code> N/A <code>APIConnectionError</code>"},{"location":"get_started/#retries","title":"Retries","text":"<p>Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &gt;=500 Internal errors are all retried by default.</p> <p>You can use the <code>max_retries</code> option to configure or disable retry settings:</p> <pre><code>from openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre>"},{"location":"get_started/#timeouts","title":"Timeouts","text":"<p>By default requests time out after 10 minutes. You can configure this with a <code>timeout</code> option, which accepts a float or an <code>httpx.Timeout</code> object:</p> <pre><code>from openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre> <p>On timeout, an <code>APITimeoutError</code> is thrown.</p> <p>Note that requests that time out are retried twice by default.</p>"},{"location":"get_started/#advanced","title":"Advanced","text":""},{"location":"get_started/#logging","title":"Logging","text":"<p>We use the standard library <code>logging</code> module.</p> <p>You can enable logging by setting the environment variable <code>OPENAI_LOG</code> to <code>debug</code>.</p> <pre><code>$ export OPENAI_LOG=debug\n</code></pre>"},{"location":"get_started/#how-to-tell-whether-none-means-null-or-missing","title":"How to tell whether <code>None</code> means <code>null</code> or missing","text":"<p>In an API response, a field may be explicitly <code>null</code>, or missing entirely; in either case, its value is <code>None</code> in this library. You can differentiate the two cases with <code>.model_fields_set</code>:</p> <pre><code>if response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n</code></pre>"},{"location":"get_started/#accessing-raw-response-data-eg-headers","title":"Accessing raw response data (e.g. headers)","text":"<p>The \"raw\" Response object can be accessed by prefixing <code>.with_raw_response.</code> to any HTTP method call, e.g.,</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n</code></pre> <p>These methods return an <code>LegacyAPIResponse</code> object. This is a legacy class as we're changing it slightly in the next major version.</p> <p>For the sync client this will mostly be the same with the exception of <code>content</code> &amp; <code>text</code> will be methods instead of properties. In the async client, all methods will be async.</p> <p>A migration script will be provided &amp; the migration in general should be smooth.</p>"},{"location":"get_started/#with_streaming_response","title":"<code>.with_streaming_response</code>","text":"<p>The above interface eagerly reads the full response body when you make the request, which may not always be what you want.</p> <p>To stream the response body, use <code>.with_streaming_response</code> instead, which requires a context manager and only reads the response body once you call <code>.read()</code>, <code>.text()</code>, <code>.json()</code>, <code>.iter_bytes()</code>, <code>.iter_text()</code>, <code>.iter_lines()</code> or <code>.parse()</code>. In the async client, these are async methods.</p> <p>As such, <code>.with_streaming_response</code> methods return a different <code>APIResponse</code> object, and the async client returns an <code>AsyncAPIResponse</code> object.</p> <pre><code>with client.chat.completions.with_streaming_response.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n) as response:\n    print(response.headers.get(\"X-My-Header\"))\n\n    for line in response.iter_lines():\n        print(line)\n</code></pre> <p>The context manager is required so that the response will reliably be closed.</p>"},{"location":"get_started/#configuring-the-http-client","title":"Configuring the HTTP client","text":"<p>You can directly override the httpx client to customize it for your use case, including:</p> <ul> <li>Support for proxies</li> <li>Custom transports</li> <li>Additional advanced functionality</li> </ul> <pre><code>import httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n</code></pre>"},{"location":"get_started/#managing-http-resources","title":"Managing HTTP resources","text":"<p>By default the library closes underlying HTTP connections whenever the client is garbage collected. You can manually close the client using the <code>.close()</code> method if desired, or with a context manager that closes when exiting.</p>"},{"location":"get_started/#microsoft-azure-openai","title":"Microsoft Azure OpenAI","text":"<p>To use this library with Azure OpenAI, use the <code>AzureOpenAI</code> class instead of the <code>OpenAI</code> class.</p> <p>[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.</p> <pre><code>from openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n</code></pre> <p>In addition to the options provided in the base <code>OpenAI</code> client, the following options are provided:</p> <ul> <li><code>azure_endpoint</code> (or the <code>AZURE_OPENAI_ENDPOINT</code> environment variable)</li> <li><code>azure_deployment</code></li> <li><code>api_version</code> (or the <code>OPENAI_API_VERSION</code> environment variable)</li> <li><code>azure_ad_token</code> (or the <code>AZURE_OPENAI_AD_TOKEN</code> environment variable)</li> <li><code>azure_ad_token_provider</code></li> </ul> <p>An example of using the client with Azure Active Directory can be found here.</p>"},{"location":"get_started/#versioning","title":"Versioning","text":"<p>This package generally follows SemVer conventions, though certain backwards-incompatible changes may be released as minor versions:</p> <ol> <li>Changes that only affect static types, without breaking runtime behavior.</li> <li>Changes to library internals which are technically public but not intended or documented for external use. (Please open a GitHub issue to let us know if you are relying on such internals).</li> <li>Changes that we do not expect to impact the vast majority of users in practice.</li> </ol> <p>We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.</p> <p>We are keen for your feedback; please open an issue with questions, bugs, or suggestions.</p>"},{"location":"pagination/","title":"Pagination","text":""},{"location":"pagination/#src.openai.pagination","title":"pagination","text":""},{"location":"pagination/#src.openai.pagination.AsyncCursorPage","title":"AsyncCursorPage","text":""},{"location":"pagination/#src.openai.pagination.AsyncCursorPage.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[_T]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.AsyncCursorPage.next_page_info","title":"next_page_info","text":"<pre><code>next_page_info() -&gt; Optional[PageInfo]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.AsyncPage","title":"AsyncPage","text":"<p>Note: no pagination actually occurs yet, this is for forwards-compatibility.</p>"},{"location":"pagination/#src.openai.pagination.AsyncPage.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[_T]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.AsyncPage.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: str\n</code></pre>"},{"location":"pagination/#src.openai.pagination.AsyncPage.next_page_info","title":"next_page_info","text":"<pre><code>next_page_info() -&gt; None\n</code></pre> <p>This page represents a response that isn't actually paginated at the API level so there will never be a next page.</p>"},{"location":"pagination/#src.openai.pagination.CursorPageItem","title":"CursorPageItem","text":""},{"location":"pagination/#src.openai.pagination.CursorPageItem.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: Optional[str]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.SyncCursorPage","title":"SyncCursorPage","text":""},{"location":"pagination/#src.openai.pagination.SyncCursorPage.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[_T]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.SyncCursorPage.next_page_info","title":"next_page_info","text":"<pre><code>next_page_info() -&gt; Optional[PageInfo]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.SyncPage","title":"SyncPage","text":"<p>Note: no pagination actually occurs yet, this is for forwards-compatibility.</p>"},{"location":"pagination/#src.openai.pagination.SyncPage.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[_T]\n</code></pre>"},{"location":"pagination/#src.openai.pagination.SyncPage.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: str\n</code></pre>"},{"location":"pagination/#src.openai.pagination.SyncPage.next_page_info","title":"next_page_info","text":"<pre><code>next_page_info() -&gt; None\n</code></pre> <p>This page represents a response that isn't actually paginated at the API level so there will never be a next page.</p>"},{"location":"version/","title":"Version","text":""},{"location":"version/#src.openai.version","title":"version","text":""},{"location":"version/#src.openai.version.VERSION","title":"VERSION  <code>module-attribute</code>","text":"<pre><code>VERSION: str = __version__\n</code></pre>"},{"location":"_extras/__init__/","title":"init","text":""},{"location":"_extras/__init__/#src.openai._extras","title":"_extras","text":""},{"location":"_extras/_common/","title":"common","text":""},{"location":"_extras/_common/#src.openai._extras._common","title":"_common","text":""},{"location":"_extras/_common/#src.openai._extras._common.INSTRUCTIONS","title":"INSTRUCTIONS  <code>module-attribute</code>","text":"<pre><code>INSTRUCTIONS = \"\\n\\nOpenAI error:\\n\\n    missing `{library}`\\n\\nThis feature requires additional dependencies:\\n\\n    $ pip install openai[{extra}]\\n\\n\"\n</code></pre>"},{"location":"_extras/_common/#src.openai._extras._common.MissingDependencyError","title":"MissingDependencyError","text":""},{"location":"_extras/_common/#src.openai._extras._common.format_instructions","title":"format_instructions","text":"<pre><code>format_instructions(*, library: str, extra: str) -&gt; str\n</code></pre>"},{"location":"_extras/numpy_proxy/","title":"Numpy proxy","text":""},{"location":"_extras/numpy_proxy/#src.openai._extras.numpy_proxy","title":"numpy_proxy","text":""},{"location":"_extras/numpy_proxy/#src.openai._extras.numpy_proxy.NUMPY_INSTRUCTIONS","title":"NUMPY_INSTRUCTIONS  <code>module-attribute</code>","text":"<pre><code>NUMPY_INSTRUCTIONS = format_instructions(\n    library=\"numpy\", extra=\"datalib\"\n)\n</code></pre>"},{"location":"_extras/numpy_proxy/#src.openai._extras.numpy_proxy.NumpyProxy","title":"NumpyProxy","text":""},{"location":"_extras/numpy_proxy/#src.openai._extras.numpy_proxy.has_numpy","title":"has_numpy","text":"<pre><code>has_numpy() -&gt; bool\n</code></pre>"},{"location":"_extras/pandas_proxy/","title":"Pandas proxy","text":""},{"location":"_extras/pandas_proxy/#src.openai._extras.pandas_proxy","title":"pandas_proxy","text":""},{"location":"_extras/pandas_proxy/#src.openai._extras.pandas_proxy.PANDAS_INSTRUCTIONS","title":"PANDAS_INSTRUCTIONS  <code>module-attribute</code>","text":"<pre><code>PANDAS_INSTRUCTIONS = format_instructions(\n    library=\"pandas\", extra=\"datalib\"\n)\n</code></pre>"},{"location":"_extras/pandas_proxy/#src.openai._extras.pandas_proxy.PandasProxy","title":"PandasProxy","text":""},{"location":"_utils/__init__/","title":"init","text":""},{"location":"_utils/__init__/#src.openai._utils","title":"_utils","text":""},{"location":"_utils/_logs/","title":"logs","text":""},{"location":"_utils/_logs/#src.openai._utils._logs","title":"_logs","text":""},{"location":"_utils/_logs/#src.openai._utils._logs.httpx_logger","title":"httpx_logger  <code>module-attribute</code>","text":"<pre><code>httpx_logger: Logger = getLogger('httpx')\n</code></pre>"},{"location":"_utils/_logs/#src.openai._utils._logs.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger: Logger = getLogger('openai')\n</code></pre>"},{"location":"_utils/_logs/#src.openai._utils._logs.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging() -&gt; None\n</code></pre>"},{"location":"_utils/_proxy/","title":"proxy","text":""},{"location":"_utils/_proxy/#src.openai._utils._proxy","title":"_proxy","text":""},{"location":"_utils/_proxy/#src.openai._utils._proxy.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"_utils/_proxy/#src.openai._utils._proxy.LazyProxy","title":"LazyProxy","text":"<p>Implements data methods to pretend that an instance is another instance.</p> <p>This includes forwarding attribute access and othe methods.</p>"},{"location":"_utils/_streams/","title":"streams","text":""},{"location":"_utils/_streams/#src.openai._utils._streams","title":"_streams","text":""},{"location":"_utils/_streams/#src.openai._utils._streams.consume_async_iterator","title":"consume_async_iterator  <code>async</code>","text":"<pre><code>consume_async_iterator(\n    iterator: AsyncIterator[Any],\n) -&gt; None\n</code></pre>"},{"location":"_utils/_streams/#src.openai._utils._streams.consume_sync_iterator","title":"consume_sync_iterator","text":"<pre><code>consume_sync_iterator(iterator: Iterator[Any]) -&gt; None\n</code></pre>"},{"location":"_utils/_sync/","title":"sync","text":""},{"location":"_utils/_sync/#src.openai._utils._sync","title":"_sync","text":""},{"location":"_utils/_sync/#src.openai._utils._sync.T_ParamSpec","title":"T_ParamSpec  <code>module-attribute</code>","text":"<pre><code>T_ParamSpec = ParamSpec('T_ParamSpec')\n</code></pre>"},{"location":"_utils/_sync/#src.openai._utils._sync.T_Retval","title":"T_Retval  <code>module-attribute</code>","text":"<pre><code>T_Retval = TypeVar('T_Retval')\n</code></pre>"},{"location":"_utils/_sync/#src.openai._utils._sync.asyncify","title":"asyncify","text":"<pre><code>asyncify(\n    function: Callable[T_ParamSpec, T_Retval],\n    *,\n    cancellable: bool = False,\n    limiter: CapacityLimiter | None = None\n) -&gt; Callable[T_ParamSpec, Awaitable[T_Retval]]\n</code></pre> <p>Take a blocking function and create an async one that receives the same positional and keyword arguments, and that when called, calls the original function in a worker thread using <code>anyio.to_thread.run_sync()</code>. Internally, <code>asyncer.asyncify()</code> uses the same <code>anyio.to_thread.run_sync()</code>, but it supports keyword arguments additional to positional arguments and it adds better support for autocompletion and inline errors for the arguments of the function called and the return value.</p> <p>If the <code>cancellable</code> option is enabled and the task waiting for its completion is cancelled, the thread will still run its course but its return value (or any raised exception) will be ignored.</p> <p>Use it like this:</p> <pre><code>def do_work(arg1, arg2, kwarg1=\"\", kwarg2=\"\") -&gt; str:\n    # Do work\n    return \"Some result\"\n\n\nresult = await to_thread.asyncify(do_work)(\"spam\", \"ham\", kwarg1=\"a\", kwarg2=\"b\")\nprint(result)\n</code></pre>"},{"location":"_utils/_sync/#src.openai._utils._sync.asyncify--arguments","title":"Arguments","text":"<p><code>function</code>: a blocking regular callable (e.g. a function) <code>cancellable</code>: <code>True</code> to allow cancellation of the operation <code>limiter</code>: capacity limiter to use to limit the total amount of threads running     (if omitted, the default limiter is used)</p>"},{"location":"_utils/_sync/#src.openai._utils._sync.asyncify--return","title":"Return","text":"<p>An async function that takes the same positional and keyword arguments as the original one, that when called runs the same original function in a thread worker and returns the result.</p>"},{"location":"_utils/_transform/","title":"transform","text":""},{"location":"_utils/_transform/#src.openai._utils._transform","title":"_transform","text":""},{"location":"_utils/_transform/#src.openai._utils._transform.PropertyFormat","title":"PropertyFormat  <code>module-attribute</code>","text":"<pre><code>PropertyFormat = Literal['iso8601', 'custom']\n</code></pre>"},{"location":"_utils/_transform/#src.openai._utils._transform.PropertyInfo","title":"PropertyInfo","text":"<pre><code>PropertyInfo(\n    *,\n    alias: str | None = None,\n    format: PropertyFormat | None = None,\n    format_template: str | None = None\n)\n</code></pre> <p>Metadata class to be used in Annotated types to provide information about a given type.</p> <p>For example:</p> <p>class MyParams(TypedDict):     account_holder_name: Annotated[str, PropertyInfo(alias='accountHolderName')]</p> <p>This means that {'account_holder_name': 'Robert'} will be transformed to {'accountHolderName': 'Robert'} before being sent to the API.</p>"},{"location":"_utils/_transform/#src.openai._utils._transform.PropertyInfo.alias","title":"alias  <code>instance-attribute</code>","text":"<pre><code>alias: str | None = alias\n</code></pre>"},{"location":"_utils/_transform/#src.openai._utils._transform.PropertyInfo.format","title":"format  <code>instance-attribute</code>","text":"<pre><code>format: PropertyFormat | None = format\n</code></pre>"},{"location":"_utils/_transform/#src.openai._utils._transform.PropertyInfo.format_template","title":"format_template  <code>instance-attribute</code>","text":"<pre><code>format_template: str | None = format_template\n</code></pre>"},{"location":"_utils/_transform/#src.openai._utils._transform.maybe_transform","title":"maybe_transform","text":"<pre><code>maybe_transform(\n    data: object, expected_type: object\n) -&gt; Any | None\n</code></pre> <p>Wrapper over <code>transform()</code> that allows <code>None</code> to be passed.</p> <p>See <code>transform()</code> for more details.</p>"},{"location":"_utils/_transform/#src.openai._utils._transform.transform","title":"transform","text":"<pre><code>transform(data: _T, expected_type: object) -&gt; _T\n</code></pre> <p>Transform dictionaries based off of type information from the given type, for example:</p> <pre><code>class Params(TypedDict, total=False):\n    card_id: Required[Annotated[str, PropertyInfo(alias=\"cardID\")]]\n\n\ntransformed = transform({\"card_id\": \"&lt;my card ID&gt;\"}, Params)\n# {'cardID': '&lt;my card ID&gt;'}\n</code></pre> <p>Any keys / data that does not have type information given will be included as is.</p> <p>It should be noted that the transformations that this function does are not represented in the type system.</p>"},{"location":"_utils/_typing/","title":"typing","text":""},{"location":"_utils/_typing/#src.openai._utils._typing","title":"_typing","text":""},{"location":"_utils/_typing/#src.openai._utils._typing.extract_type_arg","title":"extract_type_arg","text":"<pre><code>extract_type_arg(typ: type, index: int) -&gt; type\n</code></pre>"},{"location":"_utils/_typing/#src.openai._utils._typing.extract_type_var_from_base","title":"extract_type_var_from_base","text":"<pre><code>extract_type_var_from_base(\n    typ: type,\n    *,\n    generic_bases: tuple[type, ...],\n    index: int,\n    failure_message: str | None = None\n) -&gt; type\n</code></pre> <p>Given a type like <code>Foo[T]</code>, returns the generic type variable <code>T</code>.</p> <p>This also handles the case where a concrete subclass is given, e.g. <pre><code>class MyResponse(Foo[bytes]):\n    ...\n\nextract_type_var(MyResponse, bases=(Foo,), index=0) -&gt; bytes\n</code></pre></p> <p>And where a generic subclass is given: <pre><code>_T = TypeVar('_T')\nclass MyResponse(Foo[_T]):\n    ...\n\nextract_type_var(MyResponse[bytes], bases=(Foo,), index=0) -&gt; bytes\n</code></pre></p>"},{"location":"_utils/_typing/#src.openai._utils._typing.is_annotated_type","title":"is_annotated_type","text":"<pre><code>is_annotated_type(typ: type) -&gt; bool\n</code></pre>"},{"location":"_utils/_typing/#src.openai._utils._typing.is_iterable_type","title":"is_iterable_type","text":"<pre><code>is_iterable_type(typ: type) -&gt; bool\n</code></pre> <p>If the given type is <code>typing.Iterable[T]</code></p>"},{"location":"_utils/_typing/#src.openai._utils._typing.is_list_type","title":"is_list_type","text":"<pre><code>is_list_type(typ: type) -&gt; bool\n</code></pre>"},{"location":"_utils/_typing/#src.openai._utils._typing.is_required_type","title":"is_required_type","text":"<pre><code>is_required_type(typ: type) -&gt; bool\n</code></pre>"},{"location":"_utils/_typing/#src.openai._utils._typing.is_typevar","title":"is_typevar","text":"<pre><code>is_typevar(typ: type) -&gt; bool\n</code></pre>"},{"location":"_utils/_typing/#src.openai._utils._typing.is_union_type","title":"is_union_type","text":"<pre><code>is_union_type(typ: type) -&gt; bool\n</code></pre>"},{"location":"_utils/_typing/#src.openai._utils._typing.strip_annotated_type","title":"strip_annotated_type","text":"<pre><code>strip_annotated_type(typ: type) -&gt; type\n</code></pre>"},{"location":"_utils/_utils/","title":"utils","text":""},{"location":"_utils/_utils/#src.openai._utils._utils","title":"_utils","text":""},{"location":"_utils/_utils/#src.openai._utils._utils.CallableT","title":"CallableT  <code>module-attribute</code>","text":"<pre><code>CallableT = TypeVar('CallableT', bound=Callable[..., Any])\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.coerce_boolean","title":"coerce_boolean","text":"<pre><code>coerce_boolean(val: str) -&gt; bool\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.coerce_float","title":"coerce_float","text":"<pre><code>coerce_float(val: str) -&gt; float\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.coerce_integer","title":"coerce_integer","text":"<pre><code>coerce_integer(val: str) -&gt; int\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.deepcopy_minimal","title":"deepcopy_minimal","text":"<pre><code>deepcopy_minimal(item: _T) -&gt; _T\n</code></pre> <p>Minimal reimplementation of copy.deepcopy() that will only copy certain object types:</p> <ul> <li>mappings, e.g. <code>dict</code></li> <li>list</li> </ul> <p>This is done for performance reasons.</p>"},{"location":"_utils/_utils/#src.openai._utils._utils.extract_files","title":"extract_files","text":"<pre><code>extract_files(\n    query: Mapping[str, object],\n    *,\n    paths: Sequence[Sequence[str]]\n) -&gt; list[tuple[str, FileTypes]]\n</code></pre> <p>Recursively extract files from the given dictionary based on specified paths.</p> <p>A path may look like this ['foo', 'files', '', 'data']. <p>Note: this mutates the given dictionary.</p>"},{"location":"_utils/_utils/#src.openai._utils._utils.file_from_path","title":"file_from_path","text":"<pre><code>file_from_path(path: str) -&gt; FileTypes\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.flatten","title":"flatten","text":"<pre><code>flatten(t: Iterable[Iterable[_T]]) -&gt; list[_T]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.get_async_library","title":"get_async_library","text":"<pre><code>get_async_library() -&gt; str\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.get_required_header","title":"get_required_header","text":"<pre><code>get_required_header(\n    headers: HeadersLike, header: str\n) -&gt; str\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.human_join","title":"human_join","text":"<pre><code>human_join(\n    seq: Sequence[str],\n    *,\n    delim: str = \", \",\n    final: str = \"or\"\n) -&gt; str\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_dict","title":"is_dict","text":"<pre><code>is_dict(obj: object) -&gt; TypeGuard[dict[object, object]]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_given","title":"is_given","text":"<pre><code>is_given(obj: NotGivenOr[_T]) -&gt; TypeGuard[_T]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_iterable","title":"is_iterable","text":"<pre><code>is_iterable(obj: object) -&gt; TypeGuard[Iterable[object]]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_list","title":"is_list","text":"<pre><code>is_list(obj: object) -&gt; TypeGuard[list[object]]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_mapping","title":"is_mapping","text":"<pre><code>is_mapping(obj: object) -&gt; TypeGuard[Mapping[str, object]]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_mapping_t","title":"is_mapping_t","text":"<pre><code>is_mapping_t(\n    obj: _MappingT | object,\n) -&gt; TypeGuard[_MappingT]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_sequence","title":"is_sequence","text":"<pre><code>is_sequence(obj: object) -&gt; TypeGuard[Sequence[object]]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_sequence_t","title":"is_sequence_t","text":"<pre><code>is_sequence_t(\n    obj: _SequenceT | object,\n) -&gt; TypeGuard[_SequenceT]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_tuple","title":"is_tuple","text":"<pre><code>is_tuple(obj: object) -&gt; TypeGuard[tuple[object, ...]]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.is_tuple_t","title":"is_tuple_t","text":"<pre><code>is_tuple_t(obj: _TupleT | object) -&gt; TypeGuard[_TupleT]\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.maybe_coerce_boolean","title":"maybe_coerce_boolean","text":"<pre><code>maybe_coerce_boolean(val: str | None) -&gt; bool | None\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.maybe_coerce_float","title":"maybe_coerce_float","text":"<pre><code>maybe_coerce_float(val: str | None) -&gt; float | None\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.maybe_coerce_integer","title":"maybe_coerce_integer","text":"<pre><code>maybe_coerce_integer(val: str | None) -&gt; int | None\n</code></pre>"},{"location":"_utils/_utils/#src.openai._utils._utils.quote","title":"quote","text":"<pre><code>quote(string: str) -&gt; str\n</code></pre> <p>Add single quotation marks around the given string. Does not do any escaping.</p>"},{"location":"_utils/_utils/#src.openai._utils._utils.removeprefix","title":"removeprefix","text":"<pre><code>removeprefix(string: str, prefix: str) -&gt; str\n</code></pre> <p>Remove a prefix from a string.</p> <p>Backport of <code>str.removeprefix</code> for Python &lt; 3.9</p>"},{"location":"_utils/_utils/#src.openai._utils._utils.removesuffix","title":"removesuffix","text":"<pre><code>removesuffix(string: str, suffix: str) -&gt; str\n</code></pre> <p>Remove a suffix from a string.</p> <p>Backport of <code>str.removesuffix</code> for Python &lt; 3.9</p>"},{"location":"_utils/_utils/#src.openai._utils._utils.required_args","title":"required_args","text":"<pre><code>required_args(\n    *variants: Sequence[str],\n) -&gt; Callable[[CallableT], CallableT]\n</code></pre> <p>Decorator to enforce a given set of arguments or variants of arguments are passed to the decorated function.</p> <p>Useful for enforcing runtime validation of overloaded functions.</p> <p>Example usage: <pre><code>@overload\ndef foo(*, a: str) -&gt; str:\n    ...\n\n\n@overload\ndef foo(*, b: bool) -&gt; str:\n    ...\n\n\n# This enforces the same constraints that a static type checker would\n# i.e. that either a or b must be passed to the function\n@required_args([\"a\"], [\"b\"])\ndef foo(*, a: str | None = None, b: bool | None = None) -&gt; str:\n    ...\n</code></pre></p>"},{"location":"_utils/_utils/#src.openai._utils._utils.strip_not_given","title":"strip_not_given","text":"<pre><code>strip_not_given(obj: object | None) -&gt; object\n</code></pre> <p>Remove all top-level keys where their values are instances of <code>NotGiven</code></p>"},{"location":"resources/__init__/","title":"openai.resources","text":""},{"location":"resources/__init__/#src.openai.resources","title":"resources","text":""},{"location":"resources/__init__/#src.openai.resources.AsyncAudio","title":"AsyncAudio","text":"<pre><code>AsyncAudio(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudio.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeech\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudio.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; AsyncTranscriptions\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudio.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslations\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudio.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncAudioWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudio.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncAudioWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithRawResponse","title":"AsyncAudioWithRawResponse","text":"<pre><code>AsyncAudioWithRawResponse(audio: AsyncAudio)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithRawResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeechWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithRawResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; AsyncTranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithRawResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslationsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithStreamingResponse","title":"AsyncAudioWithStreamingResponse","text":"<pre><code>AsyncAudioWithStreamingResponse(audio: AsyncAudio)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithStreamingResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithStreamingResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; (\n    AsyncTranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncAudioWithStreamingResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslationsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBeta","title":"AsyncBeta","text":"<pre><code>AsyncBeta(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBeta.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistants\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBeta.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreads\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBeta.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncBetaWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBeta.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncBetaWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBetaWithRawResponse","title":"AsyncBetaWithRawResponse","text":"<pre><code>AsyncBetaWithRawResponse(beta: AsyncBeta)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBetaWithRawResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistantsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBetaWithRawResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreadsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBetaWithStreamingResponse","title":"AsyncBetaWithStreamingResponse","text":"<pre><code>AsyncBetaWithStreamingResponse(beta: AsyncBeta)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBetaWithStreamingResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistantsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncBetaWithStreamingResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChat","title":"AsyncChat","text":"<pre><code>AsyncChat(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChat.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletions\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChat.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncChatWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChat.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncChatWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChatWithRawResponse","title":"AsyncChatWithRawResponse","text":"<pre><code>AsyncChatWithRawResponse(chat: AsyncChat)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChatWithRawResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChatWithStreamingResponse","title":"AsyncChatWithStreamingResponse","text":"<pre><code>AsyncChatWithStreamingResponse(chat: AsyncChat)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncChatWithStreamingResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletionsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletions","title":"AsyncCompletions","text":"<pre><code>AsyncCompletions(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletions.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"gpt-3.5-turbo-instruct\",\n            \"davinci-002\",\n            \"babbage-002\",\n        ],\n    ],\n    prompt: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n        None,\n    ],\n    best_of: Optional[int] | NotGiven = NOT_GIVEN,\n    echo: Optional[bool] | NotGiven = NOT_GIVEN,\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str], None] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Completion | AsyncStream[Completion]\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncCompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletionsWithRawResponse","title":"AsyncCompletionsWithRawResponse","text":"<pre><code>AsyncCompletionsWithRawResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletionsWithStreamingResponse","title":"AsyncCompletionsWithStreamingResponse","text":"<pre><code>AsyncCompletionsWithStreamingResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncCompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddings","title":"AsyncEmbeddings","text":"<pre><code>AsyncEmbeddings(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddings.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    input: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n    ],\n    model: Union[\n        str,\n        Literal[\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-small\",\n            \"text-embedding-3-large\",\n        ],\n    ],\n    dimensions: int | NotGiven = NOT_GIVEN,\n    encoding_format: (\n        Literal[\"float\", \"base64\"] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; CreateEmbeddingResponse\n</code></pre> <p>Creates an embedding vector representing the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]</code> <p>Input text to embed, encoded as a string or array of tokens. To embed multiple   inputs in a single request, pass an array of strings or array of token arrays.   The input must not exceed the max input tokens for the model (8192 tokens for   <code>text-embedding-ada-002</code>), cannot be an empty string, and any array must be 2048   dimensions or less.   Example Python code   for counting tokens.</p> required <code>model</code> <code>Union[str, Literal['text-embedding-ada-002', 'text-embedding-3-small', 'text-embedding-3-large']]</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>dimensions</code> <code>int | NotGiven</code> <p>The number of dimensions the resulting output embeddings should have. Only   supported in <code>text-embedding-3</code> and later models.</p> <code>NOT_GIVEN</code> <code>encoding_format</code> <code>Literal['float', 'base64'] | NotGiven</code> <p>The format to return the embeddings in. Can be either <code>float</code> or   <code>base64</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddings.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncEmbeddingsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddings.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncEmbeddingsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddingsWithRawResponse","title":"AsyncEmbeddingsWithRawResponse","text":"<pre><code>AsyncEmbeddingsWithRawResponse(embeddings: AsyncEmbeddings)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddingsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddingsWithStreamingResponse","title":"AsyncEmbeddingsWithStreamingResponse","text":"<pre><code>AsyncEmbeddingsWithStreamingResponse(\n    embeddings: AsyncEmbeddings,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncEmbeddingsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles","title":"AsyncFiles","text":"<pre><code>AsyncFiles(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.content","title":"content  <code>async</code>","text":"<pre><code>content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    purpose: Literal[\"fine-tune\", \"assistants\"],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Upload a file that can be used across various endpoints.</p> <p>The size of all the files uploaded by one organization can be up to 100 GB.</p> <p>The size of individual files can be a maximum of 512 MB or 2 million tokens for Assistants. See the Assistants Tools guide to learn more about the types of files supported. The Fine-tuning API only supports <code>.jsonl</code> files.</p> <p>Please contact us if you need to increase these storage limits.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The File object (not file name) to be uploaded.</p> required <code>purpose</code> <code>Literal['fine-tune', 'assistants']</code> <p>The intended purpose of the uploaded file.</p> <p>Use \"fine-tune\" for   Fine-tuning and   \"assistants\" for   Assistants and   Messages. This allows   us to validate the format of the uploaded file is correct for fine-tuning.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleted\n</code></pre> <p>Delete a file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.list","title":"list","text":"<pre><code>list(\n    *,\n    purpose: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[FileObject, AsyncPage[FileObject]]\n</code></pre> <p>Returns a list of files that belong to the user's organization.</p> <p>Parameters:</p> Name Type Description Default <code>purpose</code> <code>str | NotGiven</code> <p>Only return files with the given purpose.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Returns information about a specific file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.retrieve_content","title":"retrieve_content  <code>async</code>","text":"<pre><code>retrieve_content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; str\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.wait_for_processing","title":"wait_for_processing  <code>async</code>","text":"<pre><code>wait_for_processing(\n    id: str,\n    *,\n    poll_interval: float = 5.0,\n    max_wait_seconds: float = 30 * 60\n) -&gt; FileObject\n</code></pre> <p>Waits for the given file to be processed, default timeout is 30 mins.</p>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFiles.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFilesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse","title":"AsyncFilesWithRawResponse","text":"<pre><code>AsyncFilesWithRawResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = async_to_raw_response_wrapper(content)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithRawResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = async_to_raw_response_wrapper(\n    retrieve_content\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse","title":"AsyncFilesWithStreamingResponse","text":"<pre><code>AsyncFilesWithStreamingResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = async_to_custom_streamed_response_wrapper(\n    content, AsyncStreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFilesWithStreamingResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = async_to_streamed_response_wrapper(\n    retrieve_content\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuning","title":"AsyncFineTuning","text":"<pre><code>AsyncFineTuning(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuning.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobs\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuning.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFineTuningWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuning.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFineTuningWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuningWithRawResponse","title":"AsyncFineTuningWithRawResponse","text":"<pre><code>AsyncFineTuningWithRawResponse(\n    fine_tuning: AsyncFineTuning,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuningWithRawResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuningWithStreamingResponse","title":"AsyncFineTuningWithStreamingResponse","text":"<pre><code>AsyncFineTuningWithStreamingResponse(\n    fine_tuning: AsyncFineTuning,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncFineTuningWithStreamingResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImages","title":"AsyncImages","text":"<pre><code>AsyncImages(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImages.create_variation","title":"create_variation  <code>async</code>","text":"<pre><code>create_variation(\n    *,\n    image: FileTypes,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates a variation of a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to use as the basis for the variation(s). Must be a valid PNG file,   less than 4MB, and square.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncImages.edit","title":"edit  <code>async</code>","text":"<pre><code>edit(\n    *,\n    image: FileTypes,\n    prompt: str,\n    mask: FileTypes | NotGiven = NOT_GIVEN,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an edited or extended image given an original image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask   is not provided, image must have transparency, which will be used as the mask.</p> required <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters.</p> required <code>mask</code> <code>FileTypes | NotGiven</code> <p>An additional image whose fully transparent areas (e.g. where alpha is zero)   indicate where <code>image</code> should be edited. Must be a valid PNG file, less than   4MB, and have the same dimensions as <code>image</code>.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncImages.generate","title":"generate  <code>async</code>","text":"<pre><code>generate(\n    *,\n    prompt: str,\n    model: (\n        Union[str, Literal[\"dall-e-2\", \"dall-e-3\"], None]\n        | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    quality: (\n        Literal[\"standard\", \"hd\"] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[\n            Literal[\n                \"256x256\",\n                \"512x512\",\n                \"1024x1024\",\n                \"1792x1024\",\n                \"1024x1792\",\n            ]\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    style: (\n        Optional[Literal[\"vivid\", \"natural\"]] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an image given a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters for <code>dall-e-2</code> and 4000 characters for <code>dall-e-3</code>.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2', 'dall-e-3'], None] | NotGiven</code> <p>The model to use for image generation.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>quality</code> <code>Literal['standard', 'hd'] | NotGiven</code> <p>The quality of the image that will be generated. <code>hd</code> creates images with finer   details and greater consistency across the image. This param is only supported   for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024', '1792x1024', '1024x1792']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or   <code>1024x1792</code> for <code>dall-e-3</code> models.</p> <code>NOT_GIVEN</code> <code>style</code> <code>Optional[Literal['vivid', 'natural']] | NotGiven</code> <p>The style of the generated images. Must be one of <code>vivid</code> or <code>natural</code>. Vivid   causes the model to lean towards generating hyper-real and dramatic images.   Natural causes the model to produce more natural, less hyper-real looking   images. This param is only supported for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncImages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncImagesWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncImagesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithRawResponse","title":"AsyncImagesWithRawResponse","text":"<pre><code>AsyncImagesWithRawResponse(images: AsyncImages)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithRawResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = async_to_raw_response_wrapper(\n    create_variation\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithRawResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = async_to_raw_response_wrapper(edit)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithRawResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = async_to_raw_response_wrapper(generate)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithStreamingResponse","title":"AsyncImagesWithStreamingResponse","text":"<pre><code>AsyncImagesWithStreamingResponse(images: AsyncImages)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithStreamingResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = async_to_streamed_response_wrapper(\n    create_variation\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithStreamingResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = async_to_streamed_response_wrapper(edit)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncImagesWithStreamingResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = async_to_streamed_response_wrapper(generate)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModels","title":"AsyncModels","text":"<pre><code>AsyncModels(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModels.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModelDeleted\n</code></pre> <p>Delete a fine-tuned model.</p> <p>You must have the Owner role in your organization to delete a model.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncModels.list","title":"list","text":"<pre><code>list(\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Model, AsyncPage[Model]]\n</code></pre> <p>Lists the currently available models, and provides basic information about each one such as the owner and availability.</p>"},{"location":"resources/__init__/#src.openai.resources.AsyncModels.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Model\n</code></pre> <p>Retrieves a model instance, providing basic information about the model such as the owner and permissioning.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncModels.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncModelsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModels.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncModelsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithRawResponse","title":"AsyncModelsWithRawResponse","text":"<pre><code>AsyncModelsWithRawResponse(models: AsyncModels)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithStreamingResponse","title":"AsyncModelsWithStreamingResponse","text":"<pre><code>AsyncModelsWithStreamingResponse(models: AsyncModels)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModelsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerations","title":"AsyncModerations","text":"<pre><code>AsyncModerations(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerations.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    input: Union[str, List[str]],\n    model: (\n        Union[\n            str,\n            Literal[\n                \"text-moderation-latest\",\n                \"text-moderation-stable\",\n            ],\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModerationCreateResponse\n</code></pre> <p>Classifies if text violates OpenAI's Content Policy</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str]]</code> <p>The input text to classify</p> required <code>model</code> <code>Union[str, Literal['text-moderation-latest', 'text-moderation-stable']] | NotGiven</code> <p>Two content moderations models are available: <code>text-moderation-stable</code> and   <code>text-moderation-latest</code>.</p> <p>The default is <code>text-moderation-latest</code> which will be automatically upgraded   over time. This ensures you are always using our most accurate model. If you use   <code>text-moderation-stable</code>, we will provide advanced notice before updating the   model. Accuracy of <code>text-moderation-stable</code> may be slightly lower than for   <code>text-moderation-latest</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncModerationsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncModerationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerationsWithRawResponse","title":"AsyncModerationsWithRawResponse","text":"<pre><code>AsyncModerationsWithRawResponse(\n    moderations: AsyncModerations,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerationsWithStreamingResponse","title":"AsyncModerationsWithStreamingResponse","text":"<pre><code>AsyncModerationsWithStreamingResponse(\n    moderations: AsyncModerations,\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AsyncModerationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Audio","title":"Audio","text":"<pre><code>Audio(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Audio.speech","title":"speech","text":"<pre><code>speech() -&gt; Speech\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Audio.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; Transcriptions\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Audio.translations","title":"translations","text":"<pre><code>translations() -&gt; Translations\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Audio.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AudioWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Audio.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AudioWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithRawResponse","title":"AudioWithRawResponse","text":"<pre><code>AudioWithRawResponse(audio: Audio)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithRawResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; SpeechWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithRawResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; TranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithRawResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; TranslationsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithStreamingResponse","title":"AudioWithStreamingResponse","text":"<pre><code>AudioWithStreamingResponse(audio: Audio)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithStreamingResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; SpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithStreamingResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; TranscriptionsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.AudioWithStreamingResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; TranslationsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Beta","title":"Beta","text":"<pre><code>Beta(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Beta.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; Assistants\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Beta.threads","title":"threads","text":"<pre><code>threads() -&gt; Threads\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Beta.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; BetaWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Beta.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; BetaWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.BetaWithRawResponse","title":"BetaWithRawResponse","text":"<pre><code>BetaWithRawResponse(beta: Beta)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.BetaWithRawResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AssistantsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.BetaWithRawResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; ThreadsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.BetaWithStreamingResponse","title":"BetaWithStreamingResponse","text":"<pre><code>BetaWithStreamingResponse(beta: Beta)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.BetaWithStreamingResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AssistantsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.BetaWithStreamingResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; ThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Chat","title":"Chat","text":"<pre><code>Chat(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Chat.completions","title":"completions","text":"<pre><code>completions() -&gt; Completions\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Chat.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ChatWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Chat.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ChatWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ChatWithRawResponse","title":"ChatWithRawResponse","text":"<pre><code>ChatWithRawResponse(chat: Chat)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ChatWithRawResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ChatWithStreamingResponse","title":"ChatWithStreamingResponse","text":"<pre><code>ChatWithStreamingResponse(chat: Chat)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ChatWithStreamingResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; CompletionsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Completions","title":"Completions","text":"<pre><code>Completions(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Completions.create","title":"create","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"gpt-3.5-turbo-instruct\",\n            \"davinci-002\",\n            \"babbage-002\",\n        ],\n    ],\n    prompt: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n        None,\n    ],\n    best_of: Optional[int] | NotGiven = NOT_GIVEN,\n    echo: Optional[bool] | NotGiven = NOT_GIVEN,\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str], None] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Completion | Stream[Completion]\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Completions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Completions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    CompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.CompletionsWithRawResponse","title":"CompletionsWithRawResponse","text":"<pre><code>CompletionsWithRawResponse(completions: Completions)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.CompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.CompletionsWithStreamingResponse","title":"CompletionsWithStreamingResponse","text":"<pre><code>CompletionsWithStreamingResponse(completions: Completions)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.CompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Embeddings","title":"Embeddings","text":"<pre><code>Embeddings(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Embeddings.create","title":"create","text":"<pre><code>create(\n    *,\n    input: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n    ],\n    model: Union[\n        str,\n        Literal[\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-small\",\n            \"text-embedding-3-large\",\n        ],\n    ],\n    dimensions: int | NotGiven = NOT_GIVEN,\n    encoding_format: (\n        Literal[\"float\", \"base64\"] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; CreateEmbeddingResponse\n</code></pre> <p>Creates an embedding vector representing the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]</code> <p>Input text to embed, encoded as a string or array of tokens. To embed multiple   inputs in a single request, pass an array of strings or array of token arrays.   The input must not exceed the max input tokens for the model (8192 tokens for   <code>text-embedding-ada-002</code>), cannot be an empty string, and any array must be 2048   dimensions or less.   Example Python code   for counting tokens.</p> required <code>model</code> <code>Union[str, Literal['text-embedding-ada-002', 'text-embedding-3-small', 'text-embedding-3-large']]</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>dimensions</code> <code>int | NotGiven</code> <p>The number of dimensions the resulting output embeddings should have. Only   supported in <code>text-embedding-3</code> and later models.</p> <code>NOT_GIVEN</code> <code>encoding_format</code> <code>Literal['float', 'base64'] | NotGiven</code> <p>The format to return the embeddings in. Can be either <code>float</code> or   <code>base64</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Embeddings.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; EmbeddingsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Embeddings.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    EmbeddingsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.EmbeddingsWithRawResponse","title":"EmbeddingsWithRawResponse","text":"<pre><code>EmbeddingsWithRawResponse(embeddings: Embeddings)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.EmbeddingsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.EmbeddingsWithStreamingResponse","title":"EmbeddingsWithStreamingResponse","text":"<pre><code>EmbeddingsWithStreamingResponse(embeddings: Embeddings)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.EmbeddingsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Files","title":"Files","text":"<pre><code>Files(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Files.content","title":"content","text":"<pre><code>content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Files.create","title":"create","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    purpose: Literal[\"fine-tune\", \"assistants\"],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Upload a file that can be used across various endpoints.</p> <p>The size of all the files uploaded by one organization can be up to 100 GB.</p> <p>The size of individual files can be a maximum of 512 MB or 2 million tokens for Assistants. See the Assistants Tools guide to learn more about the types of files supported. The Fine-tuning API only supports <code>.jsonl</code> files.</p> <p>Please contact us if you need to increase these storage limits.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The File object (not file name) to be uploaded.</p> required <code>purpose</code> <code>Literal['fine-tune', 'assistants']</code> <p>The intended purpose of the uploaded file.</p> <p>Use \"fine-tune\" for   Fine-tuning and   \"assistants\" for   Assistants and   Messages. This allows   us to validate the format of the uploaded file is correct for fine-tuning.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Files.delete","title":"delete","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleted\n</code></pre> <p>Delete a file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Files.list","title":"list","text":"<pre><code>list(\n    *,\n    purpose: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncPage[FileObject]\n</code></pre> <p>Returns a list of files that belong to the user's organization.</p> <p>Parameters:</p> Name Type Description Default <code>purpose</code> <code>str | NotGiven</code> <p>Only return files with the given purpose.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Files.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Returns information about a specific file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Files.retrieve_content","title":"retrieve_content","text":"<pre><code>retrieve_content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; str\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Files.wait_for_processing","title":"wait_for_processing","text":"<pre><code>wait_for_processing(\n    id: str,\n    *,\n    poll_interval: float = 5.0,\n    max_wait_seconds: float = 30 * 60\n) -&gt; FileObject\n</code></pre> <p>Waits for the given file to be processed, default timeout is 30 mins.</p>"},{"location":"resources/__init__/#src.openai.resources.Files.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Files.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse","title":"FilesWithRawResponse","text":"<pre><code>FilesWithRawResponse(files: Files)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = to_raw_response_wrapper(content)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithRawResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = to_raw_response_wrapper(retrieve_content)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse","title":"FilesWithStreamingResponse","text":"<pre><code>FilesWithStreamingResponse(files: Files)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = to_custom_streamed_response_wrapper(\n    content, StreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FilesWithStreamingResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = to_streamed_response_wrapper(\n    retrieve_content\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuning","title":"FineTuning","text":"<pre><code>FineTuning(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuning.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; Jobs\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuning.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FineTuningWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuning.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    FineTuningWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuningWithRawResponse","title":"FineTuningWithRawResponse","text":"<pre><code>FineTuningWithRawResponse(fine_tuning: FineTuning)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuningWithRawResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; JobsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuningWithStreamingResponse","title":"FineTuningWithStreamingResponse","text":"<pre><code>FineTuningWithStreamingResponse(fine_tuning: FineTuning)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.FineTuningWithStreamingResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; JobsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Images","title":"Images","text":"<pre><code>Images(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Images.create_variation","title":"create_variation","text":"<pre><code>create_variation(\n    *,\n    image: FileTypes,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates a variation of a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to use as the basis for the variation(s). Must be a valid PNG file,   less than 4MB, and square.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Images.edit","title":"edit","text":"<pre><code>edit(\n    *,\n    image: FileTypes,\n    prompt: str,\n    mask: FileTypes | NotGiven = NOT_GIVEN,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an edited or extended image given an original image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask   is not provided, image must have transparency, which will be used as the mask.</p> required <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters.</p> required <code>mask</code> <code>FileTypes | NotGiven</code> <p>An additional image whose fully transparent areas (e.g. where alpha is zero)   indicate where <code>image</code> should be edited. Must be a valid PNG file, less than   4MB, and have the same dimensions as <code>image</code>.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Images.generate","title":"generate","text":"<pre><code>generate(\n    *,\n    prompt: str,\n    model: (\n        Union[str, Literal[\"dall-e-2\", \"dall-e-3\"], None]\n        | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    quality: (\n        Literal[\"standard\", \"hd\"] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[\n            Literal[\n                \"256x256\",\n                \"512x512\",\n                \"1024x1024\",\n                \"1792x1024\",\n                \"1024x1792\",\n            ]\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    style: (\n        Optional[Literal[\"vivid\", \"natural\"]] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an image given a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters for <code>dall-e-2</code> and 4000 characters for <code>dall-e-3</code>.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2', 'dall-e-3'], None] | NotGiven</code> <p>The model to use for image generation.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>quality</code> <code>Literal['standard', 'hd'] | NotGiven</code> <p>The quality of the image that will be generated. <code>hd</code> creates images with finer   details and greater consistency across the image. This param is only supported   for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024', '1792x1024', '1024x1792']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or   <code>1024x1792</code> for <code>dall-e-3</code> models.</p> <code>NOT_GIVEN</code> <code>style</code> <code>Optional[Literal['vivid', 'natural']] | NotGiven</code> <p>The style of the generated images. Must be one of <code>vivid</code> or <code>natural</code>. Vivid   causes the model to lean towards generating hyper-real and dramatic images.   Natural causes the model to produce more natural, less hyper-real looking   images. This param is only supported for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Images.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ImagesWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Images.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ImagesWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithRawResponse","title":"ImagesWithRawResponse","text":"<pre><code>ImagesWithRawResponse(images: Images)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithRawResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = to_raw_response_wrapper(create_variation)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithRawResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = to_raw_response_wrapper(edit)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithRawResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = to_raw_response_wrapper(generate)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithStreamingResponse","title":"ImagesWithStreamingResponse","text":"<pre><code>ImagesWithStreamingResponse(images: Images)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithStreamingResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = to_streamed_response_wrapper(\n    create_variation\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithStreamingResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = to_streamed_response_wrapper(edit)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ImagesWithStreamingResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = to_streamed_response_wrapper(generate)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Models","title":"Models","text":"<pre><code>Models(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Models.delete","title":"delete","text":"<pre><code>delete(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModelDeleted\n</code></pre> <p>Delete a fine-tuned model.</p> <p>You must have the Owner role in your organization to delete a model.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Models.list","title":"list","text":"<pre><code>list(\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncPage[Model]\n</code></pre> <p>Lists the currently available models, and provides basic information about each one such as the owner and availability.</p>"},{"location":"resources/__init__/#src.openai.resources.Models.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Model\n</code></pre> <p>Retrieves a model instance, providing basic information about the model such as the owner and permissioning.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Models.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ModelsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Models.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ModelsWithStreamingResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithRawResponse","title":"ModelsWithRawResponse","text":"<pre><code>ModelsWithRawResponse(models: Models)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithStreamingResponse","title":"ModelsWithStreamingResponse","text":"<pre><code>ModelsWithStreamingResponse(models: Models)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModelsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Moderations","title":"Moderations","text":"<pre><code>Moderations(client: OpenAI)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Moderations.create","title":"create","text":"<pre><code>create(\n    *,\n    input: Union[str, List[str]],\n    model: (\n        Union[\n            str,\n            Literal[\n                \"text-moderation-latest\",\n                \"text-moderation-stable\",\n            ],\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModerationCreateResponse\n</code></pre> <p>Classifies if text violates OpenAI's Content Policy</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str]]</code> <p>The input text to classify</p> required <code>model</code> <code>Union[str, Literal['text-moderation-latest', 'text-moderation-stable']] | NotGiven</code> <p>Two content moderations models are available: <code>text-moderation-stable</code> and   <code>text-moderation-latest</code>.</p> <p>The default is <code>text-moderation-latest</code> which will be automatically upgraded   over time. This ensures you are always using our most accurate model. If you use   <code>text-moderation-stable</code>, we will provide advanced notice before updating the   model. Accuracy of <code>text-moderation-stable</code> may be slightly lower than for   <code>text-moderation-latest</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/__init__/#src.openai.resources.Moderations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ModerationsWithRawResponse\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.Moderations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    ModerationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModerationsWithRawResponse","title":"ModerationsWithRawResponse","text":"<pre><code>ModerationsWithRawResponse(moderations: Moderations)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModerationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModerationsWithStreamingResponse","title":"ModerationsWithStreamingResponse","text":"<pre><code>ModerationsWithStreamingResponse(moderations: Moderations)\n</code></pre>"},{"location":"resources/__init__/#src.openai.resources.ModerationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/completions/","title":"Completions","text":""},{"location":"resources/completions/#src.openai.resources.completions","title":"completions","text":""},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletions","title":"AsyncCompletions","text":"<pre><code>AsyncCompletions(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletions.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"gpt-3.5-turbo-instruct\",\n            \"davinci-002\",\n            \"babbage-002\",\n        ],\n    ],\n    prompt: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n        None,\n    ],\n    best_of: Optional[int] | NotGiven = NOT_GIVEN,\n    echo: Optional[bool] | NotGiven = NOT_GIVEN,\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str], None] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Completion | AsyncStream[Completion]\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncCompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletionsWithRawResponse","title":"AsyncCompletionsWithRawResponse","text":"<pre><code>AsyncCompletionsWithRawResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletionsWithStreamingResponse","title":"AsyncCompletionsWithStreamingResponse","text":"<pre><code>AsyncCompletionsWithStreamingResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.AsyncCompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.Completions","title":"Completions","text":"<pre><code>Completions(client: OpenAI)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.Completions.create","title":"create","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"gpt-3.5-turbo-instruct\",\n            \"davinci-002\",\n            \"babbage-002\",\n        ],\n    ],\n    prompt: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n        None,\n    ],\n    best_of: Optional[int] | NotGiven = NOT_GIVEN,\n    echo: Optional[bool] | NotGiven = NOT_GIVEN,\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str], None] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Completion | Stream[Completion]\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.Completions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.Completions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    CompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.CompletionsWithRawResponse","title":"CompletionsWithRawResponse","text":"<pre><code>CompletionsWithRawResponse(completions: Completions)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.CompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.CompletionsWithStreamingResponse","title":"CompletionsWithStreamingResponse","text":"<pre><code>CompletionsWithStreamingResponse(completions: Completions)\n</code></pre>"},{"location":"resources/completions/#src.openai.resources.completions.CompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/embeddings/","title":"Embeddings","text":""},{"location":"resources/embeddings/#src.openai.resources.embeddings","title":"embeddings","text":""},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddings","title":"AsyncEmbeddings","text":"<pre><code>AsyncEmbeddings(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddings.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    input: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n    ],\n    model: Union[\n        str,\n        Literal[\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-small\",\n            \"text-embedding-3-large\",\n        ],\n    ],\n    dimensions: int | NotGiven = NOT_GIVEN,\n    encoding_format: (\n        Literal[\"float\", \"base64\"] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; CreateEmbeddingResponse\n</code></pre> <p>Creates an embedding vector representing the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]</code> <p>Input text to embed, encoded as a string or array of tokens. To embed multiple   inputs in a single request, pass an array of strings or array of token arrays.   The input must not exceed the max input tokens for the model (8192 tokens for   <code>text-embedding-ada-002</code>), cannot be an empty string, and any array must be 2048   dimensions or less.   Example Python code   for counting tokens.</p> required <code>model</code> <code>Union[str, Literal['text-embedding-ada-002', 'text-embedding-3-small', 'text-embedding-3-large']]</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>dimensions</code> <code>int | NotGiven</code> <p>The number of dimensions the resulting output embeddings should have. Only   supported in <code>text-embedding-3</code> and later models.</p> <code>NOT_GIVEN</code> <code>encoding_format</code> <code>Literal['float', 'base64'] | NotGiven</code> <p>The format to return the embeddings in. Can be either <code>float</code> or   <code>base64</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddings.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncEmbeddingsWithRawResponse\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddings.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncEmbeddingsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddingsWithRawResponse","title":"AsyncEmbeddingsWithRawResponse","text":"<pre><code>AsyncEmbeddingsWithRawResponse(embeddings: AsyncEmbeddings)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddingsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddingsWithStreamingResponse","title":"AsyncEmbeddingsWithStreamingResponse","text":"<pre><code>AsyncEmbeddingsWithStreamingResponse(\n    embeddings: AsyncEmbeddings,\n)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.AsyncEmbeddingsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.Embeddings","title":"Embeddings","text":"<pre><code>Embeddings(client: OpenAI)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.Embeddings.create","title":"create","text":"<pre><code>create(\n    *,\n    input: Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n    ],\n    model: Union[\n        str,\n        Literal[\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-small\",\n            \"text-embedding-3-large\",\n        ],\n    ],\n    dimensions: int | NotGiven = NOT_GIVEN,\n    encoding_format: (\n        Literal[\"float\", \"base64\"] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; CreateEmbeddingResponse\n</code></pre> <p>Creates an embedding vector representing the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]</code> <p>Input text to embed, encoded as a string or array of tokens. To embed multiple   inputs in a single request, pass an array of strings or array of token arrays.   The input must not exceed the max input tokens for the model (8192 tokens for   <code>text-embedding-ada-002</code>), cannot be an empty string, and any array must be 2048   dimensions or less.   Example Python code   for counting tokens.</p> required <code>model</code> <code>Union[str, Literal['text-embedding-ada-002', 'text-embedding-3-small', 'text-embedding-3-large']]</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>dimensions</code> <code>int | NotGiven</code> <p>The number of dimensions the resulting output embeddings should have. Only   supported in <code>text-embedding-3</code> and later models.</p> <code>NOT_GIVEN</code> <code>encoding_format</code> <code>Literal['float', 'base64'] | NotGiven</code> <p>The format to return the embeddings in. Can be either <code>float</code> or   <code>base64</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.Embeddings.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; EmbeddingsWithRawResponse\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.Embeddings.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    EmbeddingsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.EmbeddingsWithRawResponse","title":"EmbeddingsWithRawResponse","text":"<pre><code>EmbeddingsWithRawResponse(embeddings: Embeddings)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.EmbeddingsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.EmbeddingsWithStreamingResponse","title":"EmbeddingsWithStreamingResponse","text":"<pre><code>EmbeddingsWithStreamingResponse(embeddings: Embeddings)\n</code></pre>"},{"location":"resources/embeddings/#src.openai.resources.embeddings.EmbeddingsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/files/","title":"Files","text":""},{"location":"resources/files/#src.openai.resources.files","title":"files","text":""},{"location":"resources/files/#src.openai.resources.files.AsyncFiles","title":"AsyncFiles","text":"<pre><code>AsyncFiles(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.content","title":"content  <code>async</code>","text":"<pre><code>content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    purpose: Literal[\"fine-tune\", \"assistants\"],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Upload a file that can be used across various endpoints.</p> <p>The size of all the files uploaded by one organization can be up to 100 GB.</p> <p>The size of individual files can be a maximum of 512 MB or 2 million tokens for Assistants. See the Assistants Tools guide to learn more about the types of files supported. The Fine-tuning API only supports <code>.jsonl</code> files.</p> <p>Please contact us if you need to increase these storage limits.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The File object (not file name) to be uploaded.</p> required <code>purpose</code> <code>Literal['fine-tune', 'assistants']</code> <p>The intended purpose of the uploaded file.</p> <p>Use \"fine-tune\" for   Fine-tuning and   \"assistants\" for   Assistants and   Messages. This allows   us to validate the format of the uploaded file is correct for fine-tuning.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleted\n</code></pre> <p>Delete a file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.list","title":"list","text":"<pre><code>list(\n    *,\n    purpose: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[FileObject, AsyncPage[FileObject]]\n</code></pre> <p>Returns a list of files that belong to the user's organization.</p> <p>Parameters:</p> Name Type Description Default <code>purpose</code> <code>str | NotGiven</code> <p>Only return files with the given purpose.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Returns information about a specific file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.retrieve_content","title":"retrieve_content  <code>async</code>","text":"<pre><code>retrieve_content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; str\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.wait_for_processing","title":"wait_for_processing  <code>async</code>","text":"<pre><code>wait_for_processing(\n    id: str,\n    *,\n    poll_interval: float = 5.0,\n    max_wait_seconds: float = 30 * 60\n) -&gt; FileObject\n</code></pre> <p>Waits for the given file to be processed, default timeout is 30 mins.</p>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFiles.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFilesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse","title":"AsyncFilesWithRawResponse","text":"<pre><code>AsyncFilesWithRawResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = async_to_raw_response_wrapper(content)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithRawResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = async_to_raw_response_wrapper(\n    retrieve_content\n)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse","title":"AsyncFilesWithStreamingResponse","text":"<pre><code>AsyncFilesWithStreamingResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = async_to_custom_streamed_response_wrapper(\n    content, AsyncStreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.AsyncFilesWithStreamingResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = async_to_streamed_response_wrapper(\n    retrieve_content\n)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.Files","title":"Files","text":"<pre><code>Files(client: OpenAI)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.Files.content","title":"content","text":"<pre><code>content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.Files.create","title":"create","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    purpose: Literal[\"fine-tune\", \"assistants\"],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Upload a file that can be used across various endpoints.</p> <p>The size of all the files uploaded by one organization can be up to 100 GB.</p> <p>The size of individual files can be a maximum of 512 MB or 2 million tokens for Assistants. See the Assistants Tools guide to learn more about the types of files supported. The Fine-tuning API only supports <code>.jsonl</code> files.</p> <p>Please contact us if you need to increase these storage limits.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The File object (not file name) to be uploaded.</p> required <code>purpose</code> <code>Literal['fine-tune', 'assistants']</code> <p>The intended purpose of the uploaded file.</p> <p>Use \"fine-tune\" for   Fine-tuning and   \"assistants\" for   Assistants and   Messages. This allows   us to validate the format of the uploaded file is correct for fine-tuning.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.Files.delete","title":"delete","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleted\n</code></pre> <p>Delete a file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.Files.list","title":"list","text":"<pre><code>list(\n    *,\n    purpose: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncPage[FileObject]\n</code></pre> <p>Returns a list of files that belong to the user's organization.</p> <p>Parameters:</p> Name Type Description Default <code>purpose</code> <code>str | NotGiven</code> <p>Only return files with the given purpose.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.Files.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileObject\n</code></pre> <p>Returns information about a specific file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.Files.retrieve_content","title":"retrieve_content","text":"<pre><code>retrieve_content(\n    file_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; str\n</code></pre> <p>Returns the contents of the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/files/#src.openai.resources.files.Files.wait_for_processing","title":"wait_for_processing","text":"<pre><code>wait_for_processing(\n    id: str,\n    *,\n    poll_interval: float = 5.0,\n    max_wait_seconds: float = 30 * 60\n) -&gt; FileObject\n</code></pre> <p>Waits for the given file to be processed, default timeout is 30 mins.</p>"},{"location":"resources/files/#src.openai.resources.files.Files.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.Files.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse","title":"FilesWithRawResponse","text":"<pre><code>FilesWithRawResponse(files: Files)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = to_raw_response_wrapper(content)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithRawResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = to_raw_response_wrapper(retrieve_content)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse","title":"FilesWithStreamingResponse","text":"<pre><code>FilesWithStreamingResponse(files: Files)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content = to_custom_streamed_response_wrapper(\n    content, StreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/files/#src.openai.resources.files.FilesWithStreamingResponse.retrieve_content","title":"retrieve_content  <code>instance-attribute</code>","text":"<pre><code>retrieve_content = to_streamed_response_wrapper(\n    retrieve_content\n)\n</code></pre>"},{"location":"resources/images/","title":"Images","text":""},{"location":"resources/images/#src.openai.resources.images","title":"images","text":""},{"location":"resources/images/#src.openai.resources.images.AsyncImages","title":"AsyncImages","text":"<pre><code>AsyncImages(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImages.create_variation","title":"create_variation  <code>async</code>","text":"<pre><code>create_variation(\n    *,\n    image: FileTypes,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates a variation of a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to use as the basis for the variation(s). Must be a valid PNG file,   less than 4MB, and square.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/images/#src.openai.resources.images.AsyncImages.edit","title":"edit  <code>async</code>","text":"<pre><code>edit(\n    *,\n    image: FileTypes,\n    prompt: str,\n    mask: FileTypes | NotGiven = NOT_GIVEN,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an edited or extended image given an original image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask   is not provided, image must have transparency, which will be used as the mask.</p> required <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters.</p> required <code>mask</code> <code>FileTypes | NotGiven</code> <p>An additional image whose fully transparent areas (e.g. where alpha is zero)   indicate where <code>image</code> should be edited. Must be a valid PNG file, less than   4MB, and have the same dimensions as <code>image</code>.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/images/#src.openai.resources.images.AsyncImages.generate","title":"generate  <code>async</code>","text":"<pre><code>generate(\n    *,\n    prompt: str,\n    model: (\n        Union[str, Literal[\"dall-e-2\", \"dall-e-3\"], None]\n        | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    quality: (\n        Literal[\"standard\", \"hd\"] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[\n            Literal[\n                \"256x256\",\n                \"512x512\",\n                \"1024x1024\",\n                \"1792x1024\",\n                \"1024x1792\",\n            ]\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    style: (\n        Optional[Literal[\"vivid\", \"natural\"]] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an image given a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters for <code>dall-e-2</code> and 4000 characters for <code>dall-e-3</code>.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2', 'dall-e-3'], None] | NotGiven</code> <p>The model to use for image generation.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>quality</code> <code>Literal['standard', 'hd'] | NotGiven</code> <p>The quality of the image that will be generated. <code>hd</code> creates images with finer   details and greater consistency across the image. This param is only supported   for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024', '1792x1024', '1024x1792']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or   <code>1024x1792</code> for <code>dall-e-3</code> models.</p> <code>NOT_GIVEN</code> <code>style</code> <code>Optional[Literal['vivid', 'natural']] | NotGiven</code> <p>The style of the generated images. Must be one of <code>vivid</code> or <code>natural</code>. Vivid   causes the model to lean towards generating hyper-real and dramatic images.   Natural causes the model to produce more natural, less hyper-real looking   images. This param is only supported for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/images/#src.openai.resources.images.AsyncImages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncImagesWithRawResponse\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncImagesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithRawResponse","title":"AsyncImagesWithRawResponse","text":"<pre><code>AsyncImagesWithRawResponse(images: AsyncImages)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithRawResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = async_to_raw_response_wrapper(\n    create_variation\n)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithRawResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = async_to_raw_response_wrapper(edit)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithRawResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = async_to_raw_response_wrapper(generate)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithStreamingResponse","title":"AsyncImagesWithStreamingResponse","text":"<pre><code>AsyncImagesWithStreamingResponse(images: AsyncImages)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithStreamingResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = async_to_streamed_response_wrapper(\n    create_variation\n)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithStreamingResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = async_to_streamed_response_wrapper(edit)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.AsyncImagesWithStreamingResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = async_to_streamed_response_wrapper(generate)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.Images","title":"Images","text":"<pre><code>Images(client: OpenAI)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.Images.create_variation","title":"create_variation","text":"<pre><code>create_variation(\n    *,\n    image: FileTypes,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates a variation of a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to use as the basis for the variation(s). Must be a valid PNG file,   less than 4MB, and square.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/images/#src.openai.resources.images.Images.edit","title":"edit","text":"<pre><code>edit(\n    *,\n    image: FileTypes,\n    prompt: str,\n    mask: FileTypes | NotGiven = NOT_GIVEN,\n    model: (\n        Union[str, Literal[\"dall-e-2\"], None] | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[Literal[\"256x256\", \"512x512\", \"1024x1024\"]]\n        | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an edited or extended image given an original image and a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileTypes</code> <p>The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask   is not provided, image must have transparency, which will be used as the mask.</p> required <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters.</p> required <code>mask</code> <code>FileTypes | NotGiven</code> <p>An additional image whose fully transparent areas (e.g. where alpha is zero)   indicate where <code>image</code> should be edited. Must be a valid PNG file, less than   4MB, and have the same dimensions as <code>image</code>.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Union[str, Literal['dall-e-2'], None] | NotGiven</code> <p>The model to use for image generation. Only <code>dall-e-2</code> is supported at this   time.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/images/#src.openai.resources.images.Images.generate","title":"generate","text":"<pre><code>generate(\n    *,\n    prompt: str,\n    model: (\n        Union[str, Literal[\"dall-e-2\", \"dall-e-3\"], None]\n        | NotGiven\n    ) = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    quality: (\n        Literal[\"standard\", \"hd\"] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: (\n        Optional[Literal[\"url\", \"b64_json\"]] | NotGiven\n    ) = NOT_GIVEN,\n    size: (\n        Optional[\n            Literal[\n                \"256x256\",\n                \"512x512\",\n                \"1024x1024\",\n                \"1792x1024\",\n                \"1024x1792\",\n            ]\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    style: (\n        Optional[Literal[\"vivid\", \"natural\"]] | NotGiven\n    ) = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ImagesResponse\n</code></pre> <p>Creates an image given a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text description of the desired image(s). The maximum length is 1000   characters for <code>dall-e-2</code> and 4000 characters for <code>dall-e-3</code>.</p> required <code>model</code> <code>Union[str, Literal['dall-e-2', 'dall-e-3'], None] | NotGiven</code> <p>The model to use for image generation.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Optional[int] | NotGiven</code> <p>The number of images to generate. Must be between 1 and 10. For <code>dall-e-3</code>, only   <code>n=1</code> is supported.</p> <code>NOT_GIVEN</code> <code>quality</code> <code>Literal['standard', 'hd'] | NotGiven</code> <p>The quality of the image that will be generated. <code>hd</code> creates images with finer   details and greater consistency across the image. This param is only supported   for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Optional[Literal['url', 'b64_json']] | NotGiven</code> <p>The format in which the generated images are returned. Must be one of <code>url</code> or   <code>b64_json</code>.</p> <code>NOT_GIVEN</code> <code>size</code> <code>Optional[Literal['256x256', '512x512', '1024x1024', '1792x1024', '1024x1792']] | NotGiven</code> <p>The size of the generated images. Must be one of <code>256x256</code>, <code>512x512</code>, or   <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or   <code>1024x1792</code> for <code>dall-e-3</code> models.</p> <code>NOT_GIVEN</code> <code>style</code> <code>Optional[Literal['vivid', 'natural']] | NotGiven</code> <p>The style of the generated images. Must be one of <code>vivid</code> or <code>natural</code>. Vivid   causes the model to lean towards generating hyper-real and dramatic images.   Natural causes the model to produce more natural, less hyper-real looking   images. This param is only supported for <code>dall-e-3</code>.</p> <code>NOT_GIVEN</code> <code>user</code> <code>str | NotGiven</code> <p>A unique identifier representing your end-user, which can help OpenAI to monitor   and detect abuse.   Learn more.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/images/#src.openai.resources.images.Images.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ImagesWithRawResponse\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.Images.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ImagesWithStreamingResponse\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithRawResponse","title":"ImagesWithRawResponse","text":"<pre><code>ImagesWithRawResponse(images: Images)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithRawResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = to_raw_response_wrapper(create_variation)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithRawResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = to_raw_response_wrapper(edit)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithRawResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = to_raw_response_wrapper(generate)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithStreamingResponse","title":"ImagesWithStreamingResponse","text":"<pre><code>ImagesWithStreamingResponse(images: Images)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithStreamingResponse.create_variation","title":"create_variation  <code>instance-attribute</code>","text":"<pre><code>create_variation = to_streamed_response_wrapper(\n    create_variation\n)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithStreamingResponse.edit","title":"edit  <code>instance-attribute</code>","text":"<pre><code>edit = to_streamed_response_wrapper(edit)\n</code></pre>"},{"location":"resources/images/#src.openai.resources.images.ImagesWithStreamingResponse.generate","title":"generate  <code>instance-attribute</code>","text":"<pre><code>generate = to_streamed_response_wrapper(generate)\n</code></pre>"},{"location":"resources/models/","title":"Models","text":""},{"location":"resources/models/#src.openai.resources.models","title":"models","text":""},{"location":"resources/models/#src.openai.resources.models.AsyncModels","title":"AsyncModels","text":"<pre><code>AsyncModels(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModels.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModelDeleted\n</code></pre> <p>Delete a fine-tuned model.</p> <p>You must have the Owner role in your organization to delete a model.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/models/#src.openai.resources.models.AsyncModels.list","title":"list","text":"<pre><code>list(\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Model, AsyncPage[Model]]\n</code></pre> <p>Lists the currently available models, and provides basic information about each one such as the owner and availability.</p>"},{"location":"resources/models/#src.openai.resources.models.AsyncModels.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Model\n</code></pre> <p>Retrieves a model instance, providing basic information about the model such as the owner and permissioning.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/models/#src.openai.resources.models.AsyncModels.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncModelsWithRawResponse\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModels.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncModelsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithRawResponse","title":"AsyncModelsWithRawResponse","text":"<pre><code>AsyncModelsWithRawResponse(models: AsyncModels)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithStreamingResponse","title":"AsyncModelsWithStreamingResponse","text":"<pre><code>AsyncModelsWithStreamingResponse(models: AsyncModels)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.AsyncModelsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.Models","title":"Models","text":"<pre><code>Models(client: OpenAI)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.Models.delete","title":"delete","text":"<pre><code>delete(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModelDeleted\n</code></pre> <p>Delete a fine-tuned model.</p> <p>You must have the Owner role in your organization to delete a model.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/models/#src.openai.resources.models.Models.list","title":"list","text":"<pre><code>list(\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncPage[Model]\n</code></pre> <p>Lists the currently available models, and provides basic information about each one such as the owner and availability.</p>"},{"location":"resources/models/#src.openai.resources.models.Models.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    model: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Model\n</code></pre> <p>Retrieves a model instance, providing basic information about the model such as the owner and permissioning.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/models/#src.openai.resources.models.Models.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ModelsWithRawResponse\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.Models.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ModelsWithStreamingResponse\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithRawResponse","title":"ModelsWithRawResponse","text":"<pre><code>ModelsWithRawResponse(models: Models)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithStreamingResponse","title":"ModelsWithStreamingResponse","text":"<pre><code>ModelsWithStreamingResponse(models: Models)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/models/#src.openai.resources.models.ModelsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/moderations/","title":"Moderations","text":""},{"location":"resources/moderations/#src.openai.resources.moderations","title":"moderations","text":""},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerations","title":"AsyncModerations","text":"<pre><code>AsyncModerations(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerations.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    input: Union[str, List[str]],\n    model: (\n        Union[\n            str,\n            Literal[\n                \"text-moderation-latest\",\n                \"text-moderation-stable\",\n            ],\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModerationCreateResponse\n</code></pre> <p>Classifies if text violates OpenAI's Content Policy</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str]]</code> <p>The input text to classify</p> required <code>model</code> <code>Union[str, Literal['text-moderation-latest', 'text-moderation-stable']] | NotGiven</code> <p>Two content moderations models are available: <code>text-moderation-stable</code> and   <code>text-moderation-latest</code>.</p> <p>The default is <code>text-moderation-latest</code> which will be automatically upgraded   over time. This ensures you are always using our most accurate model. If you use   <code>text-moderation-stable</code>, we will provide advanced notice before updating the   model. Accuracy of <code>text-moderation-stable</code> may be slightly lower than for   <code>text-moderation-latest</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncModerationsWithRawResponse\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncModerationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerationsWithRawResponse","title":"AsyncModerationsWithRawResponse","text":"<pre><code>AsyncModerationsWithRawResponse(\n    moderations: AsyncModerations,\n)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerationsWithStreamingResponse","title":"AsyncModerationsWithStreamingResponse","text":"<pre><code>AsyncModerationsWithStreamingResponse(\n    moderations: AsyncModerations,\n)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.AsyncModerationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.Moderations","title":"Moderations","text":"<pre><code>Moderations(client: OpenAI)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.Moderations.create","title":"create","text":"<pre><code>create(\n    *,\n    input: Union[str, List[str]],\n    model: (\n        Union[\n            str,\n            Literal[\n                \"text-moderation-latest\",\n                \"text-moderation-stable\",\n            ],\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ModerationCreateResponse\n</code></pre> <p>Classifies if text violates OpenAI's Content Policy</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str]]</code> <p>The input text to classify</p> required <code>model</code> <code>Union[str, Literal['text-moderation-latest', 'text-moderation-stable']] | NotGiven</code> <p>Two content moderations models are available: <code>text-moderation-stable</code> and   <code>text-moderation-latest</code>.</p> <p>The default is <code>text-moderation-latest</code> which will be automatically upgraded   over time. This ensures you are always using our most accurate model. If you use   <code>text-moderation-stable</code>, we will provide advanced notice before updating the   model. Accuracy of <code>text-moderation-stable</code> may be slightly lower than for   <code>text-moderation-latest</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/moderations/#src.openai.resources.moderations.Moderations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ModerationsWithRawResponse\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.Moderations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    ModerationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.ModerationsWithRawResponse","title":"ModerationsWithRawResponse","text":"<pre><code>ModerationsWithRawResponse(moderations: Moderations)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.ModerationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.ModerationsWithStreamingResponse","title":"ModerationsWithStreamingResponse","text":"<pre><code>ModerationsWithStreamingResponse(moderations: Moderations)\n</code></pre>"},{"location":"resources/moderations/#src.openai.resources.moderations.ModerationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/","title":"openai.resources.audio","text":""},{"location":"resources/audio/__init__/#src.openai.resources.audio","title":"audio","text":""},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudio","title":"AsyncAudio","text":"<pre><code>AsyncAudio(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudio.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeech\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudio.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; AsyncTranscriptions\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudio.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslations\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudio.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncAudioWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudio.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncAudioWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithRawResponse","title":"AsyncAudioWithRawResponse","text":"<pre><code>AsyncAudioWithRawResponse(audio: AsyncAudio)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithRawResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithRawResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; AsyncTranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithRawResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithStreamingResponse","title":"AsyncAudioWithStreamingResponse","text":"<pre><code>AsyncAudioWithStreamingResponse(audio: AsyncAudio)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithStreamingResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithStreamingResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; (\n    AsyncTranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncAudioWithStreamingResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslationsWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeech","title":"AsyncSpeech","text":"<pre><code>AsyncSpeech(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeech.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    input: str,\n    model: Union[str, Literal[\"tts-1\", \"tts-1-hd\"]],\n    voice: Literal[\n        \"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"\n    ],\n    response_format: (\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\"] | NotGiven\n    ) = NOT_GIVEN,\n    speed: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Generates audio from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The text to generate audio for. The maximum length is 4096 characters.</p> required <code>model</code> <code>Union[str, Literal['tts-1', 'tts-1-hd']]</code> <p>One of the available TTS models:   <code>tts-1</code> or <code>tts-1-hd</code></p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use when generating the audio. Supported voices are <code>alloy</code>,   <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, and <code>shimmer</code>. Previews of the voices are   available in the   Text to speech guide.</p> required <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac'] | NotGiven</code> <p>The format to audio in. Supported formats are <code>mp3</code>, <code>opus</code>, <code>aac</code>, and <code>flac</code>.</p> <code>NOT_GIVEN</code> <code>speed</code> <code>float | NotGiven</code> <p>The speed of the generated audio. Select a value from <code>0.25</code> to <code>4.0</code>. <code>1.0</code> is   the default.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeech.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncSpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeech.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncSpeechWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeechWithRawResponse","title":"AsyncSpeechWithRawResponse","text":"<pre><code>AsyncSpeechWithRawResponse(speech: AsyncSpeech)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeechWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeechWithStreamingResponse","title":"AsyncSpeechWithStreamingResponse","text":"<pre><code>AsyncSpeechWithStreamingResponse(speech: AsyncSpeech)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncSpeechWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_custom_streamed_response_wrapper(\n    create, AsyncStreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptions","title":"AsyncTranscriptions","text":"<pre><code>AsyncTranscriptions(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptions.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    language: str | NotGiven = NOT_GIVEN,\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: (\n        Literal[\n            \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    timestamp_granularities: (\n        List[Literal[\"word\", \"segment\"]] | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Transcription\n</code></pre> <p>Transcribes audio into the input language.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) to transcribe, in one of these formats:   flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>language</code> <code>str | NotGiven</code> <p>The language of the input audio. Supplying the input language in   ISO-639-1 format will   improve accuracy and latency.</p> <code>NOT_GIVEN</code> <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should match the audio language.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Literal['json', 'text', 'srt', 'verbose_json', 'vtt'] | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>timestamp_granularities</code> <code>List[Literal['word', 'segment']] | NotGiven</code> <p>The timestamp granularities to populate for this transcription. Any of these   options: <code>word</code>, or <code>segment</code>. Note: There is no additional latency for segment   timestamps, but generating word timestamps incurs additional latency.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncTranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncTranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptionsWithRawResponse","title":"AsyncTranscriptionsWithRawResponse","text":"<pre><code>AsyncTranscriptionsWithRawResponse(\n    transcriptions: AsyncTranscriptions,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptionsWithStreamingResponse","title":"AsyncTranscriptionsWithStreamingResponse","text":"<pre><code>AsyncTranscriptionsWithStreamingResponse(\n    transcriptions: AsyncTranscriptions,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranscriptionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslations","title":"AsyncTranslations","text":"<pre><code>AsyncTranslations(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslations.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: str | NotGiven = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Translation\n</code></pre> <p>Translates audio into English.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) translate, in one of these formats: flac,   mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should be in English.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>str | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncTranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncTranslationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslationsWithRawResponse","title":"AsyncTranslationsWithRawResponse","text":"<pre><code>AsyncTranslationsWithRawResponse(\n    translations: AsyncTranslations,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslationsWithStreamingResponse","title":"AsyncTranslationsWithStreamingResponse","text":"<pre><code>AsyncTranslationsWithStreamingResponse(\n    translations: AsyncTranslations,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AsyncTranslationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Audio","title":"Audio","text":"<pre><code>Audio(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Audio.speech","title":"speech","text":"<pre><code>speech() -&gt; Speech\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Audio.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; Transcriptions\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Audio.translations","title":"translations","text":"<pre><code>translations() -&gt; Translations\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Audio.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AudioWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Audio.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AudioWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithRawResponse","title":"AudioWithRawResponse","text":"<pre><code>AudioWithRawResponse(audio: Audio)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithRawResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; SpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithRawResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; TranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithRawResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; TranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithStreamingResponse","title":"AudioWithStreamingResponse","text":"<pre><code>AudioWithStreamingResponse(audio: Audio)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithStreamingResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; SpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithStreamingResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; TranscriptionsWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.AudioWithStreamingResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; TranslationsWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Speech","title":"Speech","text":"<pre><code>Speech(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Speech.create","title":"create","text":"<pre><code>create(\n    *,\n    input: str,\n    model: Union[str, Literal[\"tts-1\", \"tts-1-hd\"]],\n    voice: Literal[\n        \"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"\n    ],\n    response_format: (\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\"] | NotGiven\n    ) = NOT_GIVEN,\n    speed: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Generates audio from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The text to generate audio for. The maximum length is 4096 characters.</p> required <code>model</code> <code>Union[str, Literal['tts-1', 'tts-1-hd']]</code> <p>One of the available TTS models:   <code>tts-1</code> or <code>tts-1-hd</code></p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use when generating the audio. Supported voices are <code>alloy</code>,   <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, and <code>shimmer</code>. Previews of the voices are   available in the   Text to speech guide.</p> required <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac'] | NotGiven</code> <p>The format to audio in. Supported formats are <code>mp3</code>, <code>opus</code>, <code>aac</code>, and <code>flac</code>.</p> <code>NOT_GIVEN</code> <code>speed</code> <code>float | NotGiven</code> <p>The speed of the generated audio. Select a value from <code>0.25</code> to <code>4.0</code>. <code>1.0</code> is   the default.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Speech.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; SpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Speech.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; SpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.SpeechWithRawResponse","title":"SpeechWithRawResponse","text":"<pre><code>SpeechWithRawResponse(speech: Speech)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.SpeechWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.SpeechWithStreamingResponse","title":"SpeechWithStreamingResponse","text":"<pre><code>SpeechWithStreamingResponse(speech: Speech)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.SpeechWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_custom_streamed_response_wrapper(\n    create, StreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Transcriptions","title":"Transcriptions","text":"<pre><code>Transcriptions(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Transcriptions.create","title":"create","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    language: str | NotGiven = NOT_GIVEN,\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: (\n        Literal[\n            \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    timestamp_granularities: (\n        List[Literal[\"word\", \"segment\"]] | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Transcription\n</code></pre> <p>Transcribes audio into the input language.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) to transcribe, in one of these formats:   flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>language</code> <code>str | NotGiven</code> <p>The language of the input audio. Supplying the input language in   ISO-639-1 format will   improve accuracy and latency.</p> <code>NOT_GIVEN</code> <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should match the audio language.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Literal['json', 'text', 'srt', 'verbose_json', 'vtt'] | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>timestamp_granularities</code> <code>List[Literal['word', 'segment']] | NotGiven</code> <p>The timestamp granularities to populate for this transcription. Any of these   options: <code>word</code>, or <code>segment</code>. Note: There is no additional latency for segment   timestamps, but generating word timestamps incurs additional latency.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Transcriptions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; TranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Transcriptions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    TranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranscriptionsWithRawResponse","title":"TranscriptionsWithRawResponse","text":"<pre><code>TranscriptionsWithRawResponse(\n    transcriptions: Transcriptions,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranscriptionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranscriptionsWithStreamingResponse","title":"TranscriptionsWithStreamingResponse","text":"<pre><code>TranscriptionsWithStreamingResponse(\n    transcriptions: Transcriptions,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranscriptionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Translations","title":"Translations","text":"<pre><code>Translations(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Translations.create","title":"create","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: str | NotGiven = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Translation\n</code></pre> <p>Translates audio into English.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) translate, in one of these formats: flac,   mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should be in English.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>str | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Translations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; TranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.Translations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    TranslationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranslationsWithRawResponse","title":"TranslationsWithRawResponse","text":"<pre><code>TranslationsWithRawResponse(translations: Translations)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranslationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranslationsWithStreamingResponse","title":"TranslationsWithStreamingResponse","text":"<pre><code>TranslationsWithStreamingResponse(\n    translations: Translations,\n)\n</code></pre>"},{"location":"resources/audio/__init__/#src.openai.resources.audio.TranslationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/audio/","title":"Audio","text":""},{"location":"resources/audio/audio/#src.openai.resources.audio.audio","title":"audio","text":""},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudio","title":"AsyncAudio","text":"<pre><code>AsyncAudio(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudio.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeech\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudio.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; AsyncTranscriptions\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudio.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslations\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudio.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncAudioWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudio.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncAudioWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithRawResponse","title":"AsyncAudioWithRawResponse","text":"<pre><code>AsyncAudioWithRawResponse(audio: AsyncAudio)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithRawResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithRawResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; AsyncTranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithRawResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithStreamingResponse","title":"AsyncAudioWithStreamingResponse","text":"<pre><code>AsyncAudioWithStreamingResponse(audio: AsyncAudio)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithStreamingResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; AsyncSpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithStreamingResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; (\n    AsyncTranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AsyncAudioWithStreamingResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; AsyncTranslationsWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.Audio","title":"Audio","text":"<pre><code>Audio(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.Audio.speech","title":"speech","text":"<pre><code>speech() -&gt; Speech\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.Audio.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; Transcriptions\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.Audio.translations","title":"translations","text":"<pre><code>translations() -&gt; Translations\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.Audio.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AudioWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.Audio.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AudioWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithRawResponse","title":"AudioWithRawResponse","text":"<pre><code>AudioWithRawResponse(audio: Audio)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithRawResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; SpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithRawResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; TranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithRawResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; TranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithStreamingResponse","title":"AudioWithStreamingResponse","text":"<pre><code>AudioWithStreamingResponse(audio: Audio)\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithStreamingResponse.speech","title":"speech","text":"<pre><code>speech() -&gt; SpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithStreamingResponse.transcriptions","title":"transcriptions","text":"<pre><code>transcriptions() -&gt; TranscriptionsWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/audio/#src.openai.resources.audio.audio.AudioWithStreamingResponse.translations","title":"translations","text":"<pre><code>translations() -&gt; TranslationsWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/speech/","title":"Speech","text":""},{"location":"resources/audio/speech/#src.openai.resources.audio.speech","title":"speech","text":""},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeech","title":"AsyncSpeech","text":"<pre><code>AsyncSpeech(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeech.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    input: str,\n    model: Union[str, Literal[\"tts-1\", \"tts-1-hd\"]],\n    voice: Literal[\n        \"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"\n    ],\n    response_format: (\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\"] | NotGiven\n    ) = NOT_GIVEN,\n    speed: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Generates audio from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The text to generate audio for. The maximum length is 4096 characters.</p> required <code>model</code> <code>Union[str, Literal['tts-1', 'tts-1-hd']]</code> <p>One of the available TTS models:   <code>tts-1</code> or <code>tts-1-hd</code></p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use when generating the audio. Supported voices are <code>alloy</code>,   <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, and <code>shimmer</code>. Previews of the voices are   available in the   Text to speech guide.</p> required <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac'] | NotGiven</code> <p>The format to audio in. Supported formats are <code>mp3</code>, <code>opus</code>, <code>aac</code>, and <code>flac</code>.</p> <code>NOT_GIVEN</code> <code>speed</code> <code>float | NotGiven</code> <p>The speed of the generated audio. Select a value from <code>0.25</code> to <code>4.0</code>. <code>1.0</code> is   the default.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeech.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncSpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeech.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncSpeechWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeechWithRawResponse","title":"AsyncSpeechWithRawResponse","text":"<pre><code>AsyncSpeechWithRawResponse(speech: AsyncSpeech)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeechWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeechWithStreamingResponse","title":"AsyncSpeechWithStreamingResponse","text":"<pre><code>AsyncSpeechWithStreamingResponse(speech: AsyncSpeech)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.AsyncSpeechWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_custom_streamed_response_wrapper(\n    create, AsyncStreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.Speech","title":"Speech","text":"<pre><code>Speech(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.Speech.create","title":"create","text":"<pre><code>create(\n    *,\n    input: str,\n    model: Union[str, Literal[\"tts-1\", \"tts-1-hd\"]],\n    voice: Literal[\n        \"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"\n    ],\n    response_format: (\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\"] | NotGiven\n    ) = NOT_GIVEN,\n    speed: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; HttpxBinaryResponseContent\n</code></pre> <p>Generates audio from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The text to generate audio for. The maximum length is 4096 characters.</p> required <code>model</code> <code>Union[str, Literal['tts-1', 'tts-1-hd']]</code> <p>One of the available TTS models:   <code>tts-1</code> or <code>tts-1-hd</code></p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use when generating the audio. Supported voices are <code>alloy</code>,   <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, and <code>shimmer</code>. Previews of the voices are   available in the   Text to speech guide.</p> required <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac'] | NotGiven</code> <p>The format to audio in. Supported formats are <code>mp3</code>, <code>opus</code>, <code>aac</code>, and <code>flac</code>.</p> <code>NOT_GIVEN</code> <code>speed</code> <code>float | NotGiven</code> <p>The speed of the generated audio. Select a value from <code>0.25</code> to <code>4.0</code>. <code>1.0</code> is   the default.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.Speech.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; SpeechWithRawResponse\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.Speech.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; SpeechWithStreamingResponse\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.SpeechWithRawResponse","title":"SpeechWithRawResponse","text":"<pre><code>SpeechWithRawResponse(speech: Speech)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.SpeechWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.SpeechWithStreamingResponse","title":"SpeechWithStreamingResponse","text":"<pre><code>SpeechWithStreamingResponse(speech: Speech)\n</code></pre>"},{"location":"resources/audio/speech/#src.openai.resources.audio.speech.SpeechWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_custom_streamed_response_wrapper(\n    create, StreamedBinaryAPIResponse\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/","title":"Transcriptions","text":""},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions","title":"transcriptions","text":""},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptions","title":"AsyncTranscriptions","text":"<pre><code>AsyncTranscriptions(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptions.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    language: str | NotGiven = NOT_GIVEN,\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: (\n        Literal[\n            \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    timestamp_granularities: (\n        List[Literal[\"word\", \"segment\"]] | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Transcription\n</code></pre> <p>Transcribes audio into the input language.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) to transcribe, in one of these formats:   flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>language</code> <code>str | NotGiven</code> <p>The language of the input audio. Supplying the input language in   ISO-639-1 format will   improve accuracy and latency.</p> <code>NOT_GIVEN</code> <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should match the audio language.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Literal['json', 'text', 'srt', 'verbose_json', 'vtt'] | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>timestamp_granularities</code> <code>List[Literal['word', 'segment']] | NotGiven</code> <p>The timestamp granularities to populate for this transcription. Any of these   options: <code>word</code>, or <code>segment</code>. Note: There is no additional latency for segment   timestamps, but generating word timestamps incurs additional latency.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncTranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncTranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptionsWithRawResponse","title":"AsyncTranscriptionsWithRawResponse","text":"<pre><code>AsyncTranscriptionsWithRawResponse(\n    transcriptions: AsyncTranscriptions,\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptionsWithStreamingResponse","title":"AsyncTranscriptionsWithStreamingResponse","text":"<pre><code>AsyncTranscriptionsWithStreamingResponse(\n    transcriptions: AsyncTranscriptions,\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.AsyncTranscriptionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.Transcriptions","title":"Transcriptions","text":"<pre><code>Transcriptions(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.Transcriptions.create","title":"create","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    language: str | NotGiven = NOT_GIVEN,\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: (\n        Literal[\n            \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n        ]\n        | NotGiven\n    ) = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    timestamp_granularities: (\n        List[Literal[\"word\", \"segment\"]] | NotGiven\n    ) = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Transcription\n</code></pre> <p>Transcribes audio into the input language.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) to transcribe, in one of these formats:   flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>language</code> <code>str | NotGiven</code> <p>The language of the input audio. Supplying the input language in   ISO-639-1 format will   improve accuracy and latency.</p> <code>NOT_GIVEN</code> <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should match the audio language.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Literal['json', 'text', 'srt', 'verbose_json', 'vtt'] | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>timestamp_granularities</code> <code>List[Literal['word', 'segment']] | NotGiven</code> <p>The timestamp granularities to populate for this transcription. Any of these   options: <code>word</code>, or <code>segment</code>. Note: There is no additional latency for segment   timestamps, but generating word timestamps incurs additional latency.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.Transcriptions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; TranscriptionsWithRawResponse\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.Transcriptions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    TranscriptionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.TranscriptionsWithRawResponse","title":"TranscriptionsWithRawResponse","text":"<pre><code>TranscriptionsWithRawResponse(\n    transcriptions: Transcriptions,\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.TranscriptionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.TranscriptionsWithStreamingResponse","title":"TranscriptionsWithStreamingResponse","text":"<pre><code>TranscriptionsWithStreamingResponse(\n    transcriptions: Transcriptions,\n)\n</code></pre>"},{"location":"resources/audio/transcriptions/#src.openai.resources.audio.transcriptions.TranscriptionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/translations/","title":"Translations","text":""},{"location":"resources/audio/translations/#src.openai.resources.audio.translations","title":"translations","text":""},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslations","title":"AsyncTranslations","text":"<pre><code>AsyncTranslations(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslations.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: str | NotGiven = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Translation\n</code></pre> <p>Translates audio into English.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) translate, in one of these formats: flac,   mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should be in English.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>str | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncTranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncTranslationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslationsWithRawResponse","title":"AsyncTranslationsWithRawResponse","text":"<pre><code>AsyncTranslationsWithRawResponse(\n    translations: AsyncTranslations,\n)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslationsWithStreamingResponse","title":"AsyncTranslationsWithStreamingResponse","text":"<pre><code>AsyncTranslationsWithStreamingResponse(\n    translations: AsyncTranslations,\n)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.AsyncTranslationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.Translations","title":"Translations","text":"<pre><code>Translations(client: OpenAI)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.Translations.create","title":"create","text":"<pre><code>create(\n    *,\n    file: FileTypes,\n    model: Union[str, Literal[\"whisper-1\"]],\n    prompt: str | NotGiven = NOT_GIVEN,\n    response_format: str | NotGiven = NOT_GIVEN,\n    temperature: float | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Translation\n</code></pre> <p>Translates audio into English.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileTypes</code> <p>The audio file object (not file name) translate, in one of these formats: flac,   mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p> required <code>model</code> <code>Union[str, Literal['whisper-1']]</code> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p> required <code>prompt</code> <code>str | NotGiven</code> <p>An optional text to guide the model's style or continue a previous audio   segment. The   prompt   should be in English.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>str | NotGiven</code> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>,   <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>float | NotGiven</code> <p>The sampling temperature, between 0 and 1. Higher values like 0.8 will make the   output more random, while lower values like 0.2 will make it more focused and   deterministic. If set to 0, the model will use   log probability to   automatically increase the temperature until certain thresholds are hit.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.Translations.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; TranslationsWithRawResponse\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.Translations.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    TranslationsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.TranslationsWithRawResponse","title":"TranslationsWithRawResponse","text":"<pre><code>TranslationsWithRawResponse(translations: Translations)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.TranslationsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.TranslationsWithStreamingResponse","title":"TranslationsWithStreamingResponse","text":"<pre><code>TranslationsWithStreamingResponse(\n    translations: Translations,\n)\n</code></pre>"},{"location":"resources/audio/translations/#src.openai.resources.audio.translations.TranslationsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/","title":"openai.resources.beta","text":""},{"location":"resources/beta/__init__/#src.openai.resources.beta","title":"beta","text":""},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants","title":"Assistants","text":"<pre><code>Assistants(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.create","title":"create","text":"<pre><code>create(\n    *,\n    model: str,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Create an assistant with a model and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant. The maximum length is 512 characters.</p> <code>NOT_GIVEN</code> <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of file IDs   attached to this assistant. There can be a maximum of 20 files attached to the   assistant. Files are ordered by their creation date in ascending order.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>The system instructions that the assistant uses. The maximum length is 32768   characters.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>name</code> <code>Optional[str] | NotGiven</code> <p>The name of the assistant. The maximum length is 256 characters.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Iterable[Tool] | NotGiven</code> <p>A list of tool enabled on the assistant. There can be a maximum of 128 tools per   assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.delete","title":"delete","text":"<pre><code>delete(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantDeleted\n</code></pre> <p>Delete an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.files","title":"files","text":"<pre><code>files() -&gt; Files\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[Assistant]\n</code></pre> <p>Returns a list of assistants.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Retrieves an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.update","title":"update","text":"<pre><code>update(\n    assistant_id: str,\n    *,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: str | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Modifies an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant.</p> <code>NOT_GIVEN</code> <p>The maximum length is 512 characters.</p> <p>file_ids: A list of File IDs       attached to this assistant. There can be a maximum of 20 files attached to the       assistant. Files are ordered by their creation date in ascending order. If a       file was previously attached to the list but does not show up in the list, it       will be deleted from the assistant.</p> <p>instructions: The system instructions that the assistant uses. The maximum length is 32768       characters.</p> <p>metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful       for storing additional information about the object in a structured format. Keys       can be a maximum of 64 characters long and values can be a maxium of 512       characters long.</p> <p>model: ID of the model to use. You can use the       List models API to       see all of your available models, or see our       Model overview for       descriptions of them.</p> <p>name: The name of the assistant. The maximum length is 256 characters.</p> <p>tools: A list of tool enabled on the assistant. There can be a maximum of 128 tools per       assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Assistants.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AssistantsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse","title":"AssistantsWithRawResponse","text":"<pre><code>AssistantsWithRawResponse(assistants: Assistants)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse","title":"AssistantsWithStreamingResponse","text":"<pre><code>AssistantsWithStreamingResponse(assistants: Assistants)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AssistantsWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants","title":"AsyncAssistants","text":"<pre><code>AsyncAssistants(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: str,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Create an assistant with a model and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant. The maximum length is 512 characters.</p> <code>NOT_GIVEN</code> <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of file IDs   attached to this assistant. There can be a maximum of 20 files attached to the   assistant. Files are ordered by their creation date in ascending order.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>The system instructions that the assistant uses. The maximum length is 32768   characters.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>name</code> <code>Optional[str] | NotGiven</code> <p>The name of the assistant. The maximum length is 256 characters.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Iterable[Tool] | NotGiven</code> <p>A list of tool enabled on the assistant. There can be a maximum of 128 tools per   assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantDeleted\n</code></pre> <p>Delete an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.files","title":"files","text":"<pre><code>files() -&gt; AsyncFiles\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Assistant, AsyncCursorPage[Assistant]]\n</code></pre> <p>Returns a list of assistants.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Retrieves an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    assistant_id: str,\n    *,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: str | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Modifies an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant.</p> <code>NOT_GIVEN</code> <p>The maximum length is 512 characters.</p> <p>file_ids: A list of File IDs       attached to this assistant. There can be a maximum of 20 files attached to the       assistant. Files are ordered by their creation date in ascending order. If a       file was previously attached to the list but does not show up in the list, it       will be deleted from the assistant.</p> <p>instructions: The system instructions that the assistant uses. The maximum length is 32768       characters.</p> <p>metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful       for storing additional information about the object in a structured format. Keys       can be a maximum of 64 characters long and values can be a maxium of 512       characters long.</p> <p>model: ID of the model to use. You can use the       List models API to       see all of your available models, or see our       Model overview for       descriptions of them.</p> <p>name: The name of the assistant. The maximum length is 256 characters.</p> <p>tools: A list of tool enabled on the assistant. There can be a maximum of 128 tools per       assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncAssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistants.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncAssistantsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse","title":"AsyncAssistantsWithRawResponse","text":"<pre><code>AsyncAssistantsWithRawResponse(assistants: AsyncAssistants)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse","title":"AsyncAssistantsWithStreamingResponse","text":"<pre><code>AsyncAssistantsWithStreamingResponse(\n    assistants: AsyncAssistants,\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncAssistantsWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBeta","title":"AsyncBeta","text":"<pre><code>AsyncBeta(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBeta.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistants\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBeta.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreads\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBeta.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncBetaWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBeta.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncBetaWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBetaWithRawResponse","title":"AsyncBetaWithRawResponse","text":"<pre><code>AsyncBetaWithRawResponse(beta: AsyncBeta)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBetaWithRawResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBetaWithRawResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBetaWithStreamingResponse","title":"AsyncBetaWithStreamingResponse","text":"<pre><code>AsyncBetaWithStreamingResponse(beta: AsyncBeta)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBetaWithStreamingResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistantsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncBetaWithStreamingResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads","title":"AsyncThreads","text":"<pre><code>AsyncThreads(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    messages: Iterable[Message] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[Message] | NotGiven</code> <p>A list of messages to   start the thread with.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.create_and_run","title":"create_and_run  <code>async</code>","text":"<pre><code>create_and_run(\n    *,\n    assistant_id: str,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    thread: Thread | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a thread and run it in one request.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Override the default system message of the assistant. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>thread</code> <code>Thread | NotGiven</code> <p>If no thread is provided, an empty thread will be created.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadDeleted\n</code></pre> <p>Delete a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessages\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Retrieves a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRuns\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    thread_id: str,\n    *,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Modifies a thread.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreads.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncThreadsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse","title":"AsyncThreadsWithRawResponse","text":"<pre><code>AsyncThreadsWithRawResponse(threads: AsyncThreads)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = async_to_raw_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithRawResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse","title":"AsyncThreadsWithStreamingResponse","text":"<pre><code>AsyncThreadsWithStreamingResponse(threads: AsyncThreads)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = async_to_streamed_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.AsyncThreadsWithStreamingResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Beta","title":"Beta","text":"<pre><code>Beta(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Beta.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; Assistants\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Beta.threads","title":"threads","text":"<pre><code>threads() -&gt; Threads\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Beta.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; BetaWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Beta.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; BetaWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.BetaWithRawResponse","title":"BetaWithRawResponse","text":"<pre><code>BetaWithRawResponse(beta: Beta)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.BetaWithRawResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.BetaWithRawResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; ThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.BetaWithStreamingResponse","title":"BetaWithStreamingResponse","text":"<pre><code>BetaWithStreamingResponse(beta: Beta)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.BetaWithStreamingResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AssistantsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.BetaWithStreamingResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; ThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads","title":"Threads","text":"<pre><code>Threads(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.create","title":"create","text":"<pre><code>create(\n    *,\n    messages: Iterable[Message] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[Message] | NotGiven</code> <p>A list of messages to   start the thread with.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.create_and_run","title":"create_and_run","text":"<pre><code>create_and_run(\n    *,\n    assistant_id: str,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    thread: Thread | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a thread and run it in one request.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Override the default system message of the assistant. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>thread</code> <code>Thread | NotGiven</code> <p>If no thread is provided, an empty thread will be created.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.delete","title":"delete","text":"<pre><code>delete(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadDeleted\n</code></pre> <p>Delete a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.messages","title":"messages","text":"<pre><code>messages() -&gt; Messages\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Retrieves a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.runs","title":"runs","text":"<pre><code>runs() -&gt; Runs\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.update","title":"update","text":"<pre><code>update(\n    thread_id: str,\n    *,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Modifies a thread.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.Threads.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse","title":"ThreadsWithRawResponse","text":"<pre><code>ThreadsWithRawResponse(threads: Threads)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = to_raw_response_wrapper(create_and_run)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; MessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithRawResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; RunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse","title":"ThreadsWithStreamingResponse","text":"<pre><code>ThreadsWithStreamingResponse(threads: Threads)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = to_streamed_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; MessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/__init__/#src.openai.resources.beta.ThreadsWithStreamingResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; RunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/beta/","title":"Beta","text":""},{"location":"resources/beta/beta/#src.openai.resources.beta.beta","title":"beta","text":""},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBeta","title":"AsyncBeta","text":"<pre><code>AsyncBeta(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBeta.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistants\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBeta.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreads\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBeta.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncBetaWithRawResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBeta.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncBetaWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBetaWithRawResponse","title":"AsyncBetaWithRawResponse","text":"<pre><code>AsyncBetaWithRawResponse(beta: AsyncBeta)\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBetaWithRawResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBetaWithRawResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBetaWithStreamingResponse","title":"AsyncBetaWithStreamingResponse","text":"<pre><code>AsyncBetaWithStreamingResponse(beta: AsyncBeta)\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBetaWithStreamingResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AsyncAssistantsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.AsyncBetaWithStreamingResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; AsyncThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.Beta","title":"Beta","text":"<pre><code>Beta(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.Beta.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; Assistants\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.Beta.threads","title":"threads","text":"<pre><code>threads() -&gt; Threads\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.Beta.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; BetaWithRawResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.Beta.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; BetaWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.BetaWithRawResponse","title":"BetaWithRawResponse","text":"<pre><code>BetaWithRawResponse(beta: Beta)\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.BetaWithRawResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.BetaWithRawResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; ThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.BetaWithStreamingResponse","title":"BetaWithStreamingResponse","text":"<pre><code>BetaWithStreamingResponse(beta: Beta)\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.BetaWithStreamingResponse.assistants","title":"assistants","text":"<pre><code>assistants() -&gt; AssistantsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/beta/#src.openai.resources.beta.beta.BetaWithStreamingResponse.threads","title":"threads","text":"<pre><code>threads() -&gt; ThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/","title":"openai.resources.beta.assistants","text":""},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants","title":"assistants","text":""},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants","title":"Assistants","text":"<pre><code>Assistants(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.create","title":"create","text":"<pre><code>create(\n    *,\n    model: str,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Create an assistant with a model and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant. The maximum length is 512 characters.</p> <code>NOT_GIVEN</code> <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of file IDs   attached to this assistant. There can be a maximum of 20 files attached to the   assistant. Files are ordered by their creation date in ascending order.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>The system instructions that the assistant uses. The maximum length is 32768   characters.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>name</code> <code>Optional[str] | NotGiven</code> <p>The name of the assistant. The maximum length is 256 characters.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Iterable[Tool] | NotGiven</code> <p>A list of tool enabled on the assistant. There can be a maximum of 128 tools per   assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.delete","title":"delete","text":"<pre><code>delete(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantDeleted\n</code></pre> <p>Delete an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.files","title":"files","text":"<pre><code>files() -&gt; Files\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[Assistant]\n</code></pre> <p>Returns a list of assistants.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Retrieves an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.update","title":"update","text":"<pre><code>update(\n    assistant_id: str,\n    *,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: str | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Modifies an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant.</p> <code>NOT_GIVEN</code> <p>The maximum length is 512 characters.</p> <p>file_ids: A list of File IDs       attached to this assistant. There can be a maximum of 20 files attached to the       assistant. Files are ordered by their creation date in ascending order. If a       file was previously attached to the list but does not show up in the list, it       will be deleted from the assistant.</p> <p>instructions: The system instructions that the assistant uses. The maximum length is 32768       characters.</p> <p>metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful       for storing additional information about the object in a structured format. Keys       can be a maximum of 64 characters long and values can be a maxium of 512       characters long.</p> <p>model: ID of the model to use. You can use the       List models API to       see all of your available models, or see our       Model overview for       descriptions of them.</p> <p>name: The name of the assistant. The maximum length is 256 characters.</p> <p>tools: A list of tool enabled on the assistant. There can be a maximum of 128 tools per       assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Assistants.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AssistantsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse","title":"AssistantsWithRawResponse","text":"<pre><code>AssistantsWithRawResponse(assistants: Assistants)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse","title":"AssistantsWithStreamingResponse","text":"<pre><code>AssistantsWithStreamingResponse(assistants: Assistants)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AssistantsWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants","title":"AsyncAssistants","text":"<pre><code>AsyncAssistants(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: str,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Create an assistant with a model and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant. The maximum length is 512 characters.</p> <code>NOT_GIVEN</code> <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of file IDs   attached to this assistant. There can be a maximum of 20 files attached to the   assistant. Files are ordered by their creation date in ascending order.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>The system instructions that the assistant uses. The maximum length is 32768   characters.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>name</code> <code>Optional[str] | NotGiven</code> <p>The name of the assistant. The maximum length is 256 characters.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Iterable[Tool] | NotGiven</code> <p>A list of tool enabled on the assistant. There can be a maximum of 128 tools per   assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantDeleted\n</code></pre> <p>Delete an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.files","title":"files","text":"<pre><code>files() -&gt; AsyncFiles\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Assistant, AsyncCursorPage[Assistant]]\n</code></pre> <p>Returns a list of assistants.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Retrieves an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    assistant_id: str,\n    *,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: str | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Modifies an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant.</p> <code>NOT_GIVEN</code> <p>The maximum length is 512 characters.</p> <p>file_ids: A list of File IDs       attached to this assistant. There can be a maximum of 20 files attached to the       assistant. Files are ordered by their creation date in ascending order. If a       file was previously attached to the list but does not show up in the list, it       will be deleted from the assistant.</p> <p>instructions: The system instructions that the assistant uses. The maximum length is 32768       characters.</p> <p>metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful       for storing additional information about the object in a structured format. Keys       can be a maximum of 64 characters long and values can be a maxium of 512       characters long.</p> <p>model: ID of the model to use. You can use the       List models API to       see all of your available models, or see our       Model overview for       descriptions of them.</p> <p>name: The name of the assistant. The maximum length is 256 characters.</p> <p>tools: A list of tool enabled on the assistant. There can be a maximum of 128 tools per       assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncAssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistants.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncAssistantsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse","title":"AsyncAssistantsWithRawResponse","text":"<pre><code>AsyncAssistantsWithRawResponse(assistants: AsyncAssistants)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse","title":"AsyncAssistantsWithStreamingResponse","text":"<pre><code>AsyncAssistantsWithStreamingResponse(\n    assistants: AsyncAssistants,\n)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncAssistantsWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles","title":"AsyncFiles","text":"<pre><code>AsyncFiles(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    assistant_id: str,\n    *,\n    file_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Create an assistant file by attaching a File to an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>A File ID (with   <code>purpose=\"assistants\"</code>) that the assistant should use. Useful for tools like   <code>retrieval</code> and <code>code_interpreter</code> that can access files.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleteResponse\n</code></pre> <p>Delete an assistant file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles.list","title":"list","text":"<pre><code>list(\n    assistant_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    AssistantFile, AsyncCursorPage[AssistantFile]\n]\n</code></pre> <p>Returns a list of assistant files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Retrieves an AssistantFile.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFiles.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFilesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithRawResponse","title":"AsyncFilesWithRawResponse","text":"<pre><code>AsyncFilesWithRawResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithStreamingResponse","title":"AsyncFilesWithStreamingResponse","text":"<pre><code>AsyncFilesWithStreamingResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.AsyncFilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files","title":"Files","text":"<pre><code>Files(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files.create","title":"create","text":"<pre><code>create(\n    assistant_id: str,\n    *,\n    file_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Create an assistant file by attaching a File to an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>A File ID (with   <code>purpose=\"assistants\"</code>) that the assistant should use. Useful for tools like   <code>retrieval</code> and <code>code_interpreter</code> that can access files.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files.delete","title":"delete","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleteResponse\n</code></pre> <p>Delete an assistant file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files.list","title":"list","text":"<pre><code>list(\n    assistant_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[AssistantFile]\n</code></pre> <p>Returns a list of assistant files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Retrieves an AssistantFile.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.Files.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithRawResponse","title":"FilesWithRawResponse","text":"<pre><code>FilesWithRawResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithStreamingResponse","title":"FilesWithStreamingResponse","text":"<pre><code>FilesWithStreamingResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/__init__/#src.openai.resources.beta.assistants.FilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/","title":"Assistants","text":""},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants","title":"assistants","text":""},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants","title":"Assistants","text":"<pre><code>Assistants(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.create","title":"create","text":"<pre><code>create(\n    *,\n    model: str,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Create an assistant with a model and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant. The maximum length is 512 characters.</p> <code>NOT_GIVEN</code> <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of file IDs   attached to this assistant. There can be a maximum of 20 files attached to the   assistant. Files are ordered by their creation date in ascending order.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>The system instructions that the assistant uses. The maximum length is 32768   characters.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>name</code> <code>Optional[str] | NotGiven</code> <p>The name of the assistant. The maximum length is 256 characters.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Iterable[Tool] | NotGiven</code> <p>A list of tool enabled on the assistant. There can be a maximum of 128 tools per   assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.delete","title":"delete","text":"<pre><code>delete(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantDeleted\n</code></pre> <p>Delete an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.files","title":"files","text":"<pre><code>files() -&gt; Files\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[Assistant]\n</code></pre> <p>Returns a list of assistants.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Retrieves an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.update","title":"update","text":"<pre><code>update(\n    assistant_id: str,\n    *,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: str | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Modifies an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant.</p> <code>NOT_GIVEN</code> <p>The maximum length is 512 characters.</p> <p>file_ids: A list of File IDs       attached to this assistant. There can be a maximum of 20 files attached to the       assistant. Files are ordered by their creation date in ascending order. If a       file was previously attached to the list but does not show up in the list, it       will be deleted from the assistant.</p> <p>instructions: The system instructions that the assistant uses. The maximum length is 32768       characters.</p> <p>metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful       for storing additional information about the object in a structured format. Keys       can be a maximum of 64 characters long and values can be a maxium of 512       characters long.</p> <p>model: ID of the model to use. You can use the       List models API to       see all of your available models, or see our       Model overview for       descriptions of them.</p> <p>name: The name of the assistant. The maximum length is 256 characters.</p> <p>tools: A list of tool enabled on the assistant. There can be a maximum of 128 tools per       assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.Assistants.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AssistantsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse","title":"AssistantsWithRawResponse","text":"<pre><code>AssistantsWithRawResponse(assistants: Assistants)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse","title":"AssistantsWithStreamingResponse","text":"<pre><code>AssistantsWithStreamingResponse(assistants: Assistants)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AssistantsWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants","title":"AsyncAssistants","text":"<pre><code>AsyncAssistants(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: str,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Create an assistant with a model and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>ID of the model to use. You can use the   List models API to   see all of your available models, or see our   Model overview for   descriptions of them.</p> required <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant. The maximum length is 512 characters.</p> <code>NOT_GIVEN</code> <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of file IDs   attached to this assistant. There can be a maximum of 20 files attached to the   assistant. Files are ordered by their creation date in ascending order.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>The system instructions that the assistant uses. The maximum length is 32768   characters.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>name</code> <code>Optional[str] | NotGiven</code> <p>The name of the assistant. The maximum length is 256 characters.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Iterable[Tool] | NotGiven</code> <p>A list of tool enabled on the assistant. There can be a maximum of 128 tools per   assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantDeleted\n</code></pre> <p>Delete an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.files","title":"files","text":"<pre><code>files() -&gt; AsyncFiles\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Assistant, AsyncCursorPage[Assistant]]\n</code></pre> <p>Returns a list of assistants.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    assistant_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Retrieves an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    assistant_id: str,\n    *,\n    description: Optional[str] | NotGiven = NOT_GIVEN,\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: str | NotGiven = NOT_GIVEN,\n    name: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Iterable[Tool] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Assistant\n</code></pre> <p>Modifies an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>Optional[str] | NotGiven</code> <p>The description of the assistant.</p> <code>NOT_GIVEN</code> <p>The maximum length is 512 characters.</p> <p>file_ids: A list of File IDs       attached to this assistant. There can be a maximum of 20 files attached to the       assistant. Files are ordered by their creation date in ascending order. If a       file was previously attached to the list but does not show up in the list, it       will be deleted from the assistant.</p> <p>instructions: The system instructions that the assistant uses. The maximum length is 32768       characters.</p> <p>metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful       for storing additional information about the object in a structured format. Keys       can be a maximum of 64 characters long and values can be a maxium of 512       characters long.</p> <p>model: ID of the model to use. You can use the       List models API to       see all of your available models, or see our       Model overview for       descriptions of them.</p> <p>name: The name of the assistant. The maximum length is 256 characters.</p> <p>tools: A list of tool enabled on the assistant. There can be a maximum of 128 tools per       assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncAssistantsWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistants.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncAssistantsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse","title":"AsyncAssistantsWithRawResponse","text":"<pre><code>AsyncAssistantsWithRawResponse(assistants: AsyncAssistants)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse","title":"AsyncAssistantsWithStreamingResponse","text":"<pre><code>AsyncAssistantsWithStreamingResponse(\n    assistants: AsyncAssistants,\n)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/assistants/assistants/#src.openai.resources.beta.assistants.assistants.AsyncAssistantsWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/files/","title":"Files","text":""},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files","title":"files","text":""},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles","title":"AsyncFiles","text":"<pre><code>AsyncFiles(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    assistant_id: str,\n    *,\n    file_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Create an assistant file by attaching a File to an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>A File ID (with   <code>purpose=\"assistants\"</code>) that the assistant should use. Useful for tools like   <code>retrieval</code> and <code>code_interpreter</code> that can access files.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleteResponse\n</code></pre> <p>Delete an assistant file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles.list","title":"list","text":"<pre><code>list(\n    assistant_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    AssistantFile, AsyncCursorPage[AssistantFile]\n]\n</code></pre> <p>Returns a list of assistant files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Retrieves an AssistantFile.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFiles.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFilesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithRawResponse","title":"AsyncFilesWithRawResponse","text":"<pre><code>AsyncFilesWithRawResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithStreamingResponse","title":"AsyncFilesWithStreamingResponse","text":"<pre><code>AsyncFilesWithStreamingResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.AsyncFilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files","title":"Files","text":"<pre><code>Files(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files.create","title":"create","text":"<pre><code>create(\n    assistant_id: str,\n    *,\n    file_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Create an assistant file by attaching a File to an assistant.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>A File ID (with   <code>purpose=\"assistants\"</code>) that the assistant should use. Useful for tools like   <code>retrieval</code> and <code>code_interpreter</code> that can access files.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files.delete","title":"delete","text":"<pre><code>delete(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FileDeleteResponse\n</code></pre> <p>Delete an assistant file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files.list","title":"list","text":"<pre><code>list(\n    assistant_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[AssistantFile]\n</code></pre> <p>Returns a list of assistant files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    assistant_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AssistantFile\n</code></pre> <p>Retrieves an AssistantFile.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.Files.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithRawResponse","title":"FilesWithRawResponse","text":"<pre><code>FilesWithRawResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithStreamingResponse","title":"FilesWithStreamingResponse","text":"<pre><code>FilesWithStreamingResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/assistants/files/#src.openai.resources.beta.assistants.files.FilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/","title":"openai.resources.beta.threads","text":""},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads","title":"threads","text":""},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages","title":"AsyncMessages","text":"<pre><code>AsyncMessages(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    content: str,\n    role: Literal[\"user\"],\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Create a message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>role</code> <code>Literal['user']</code> <p>The role of the entity that is creating the message. Currently only <code>user</code> is   supported.</p> required <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of File IDs that   the message should use. There can be a maximum of 10 files attached to a   message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can   access and use files.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.files","title":"files","text":"<pre><code>files() -&gt; AsyncFiles\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    ThreadMessage, AsyncCursorPage[ThreadMessage]\n]\n</code></pre> <p>Returns a list of messages for a given thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    message_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Retrieve a message.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    message_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Modifies a message.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncMessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncMessagesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithRawResponse","title":"AsyncMessagesWithRawResponse","text":"<pre><code>AsyncMessagesWithRawResponse(messages: AsyncMessages)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithStreamingResponse","title":"AsyncMessagesWithStreamingResponse","text":"<pre><code>AsyncMessagesWithStreamingResponse(messages: AsyncMessages)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncMessagesWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns","title":"AsyncRuns","text":"<pre><code>AsyncRuns(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.cancel","title":"cancel  <code>async</code>","text":"<pre><code>cancel(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Cancels a run that is <code>in_progress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    assistant_id: str,\n    additional_instructions: (\n        Optional[str] | NotGiven\n    ) = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a run.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>additional_instructions</code> <code>Optional[str] | NotGiven</code> <p>Appends additional instructions at the end of the instructions for the run. This   is useful for modifying the behavior on a per-run basis without overriding other   instructions.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Overrides the   instructions   of the assistant. This is useful for modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Run, AsyncCursorPage[Run]]\n</code></pre> <p>Returns a list of runs belonging to a thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Retrieves a run.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncSteps\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.submit_tool_outputs","title":"submit_tool_outputs  <code>async</code>","text":"<pre><code>submit_tool_outputs(\n    run_id: str,\n    *,\n    thread_id: str,\n    tool_outputs: Iterable[ToolOutput],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>When a run has the <code>status: \"requires_action\"</code> and <code>required_action.type</code> is <code>submit_tool_outputs</code>, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.</p> <p>Parameters:</p> Name Type Description Default <code>tool_outputs</code> <code>Iterable[ToolOutput]</code> <p>A list of tools for which the outputs are being submitted.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    run_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Modifies a run.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncRunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRuns.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncRunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse","title":"AsyncRunsWithRawResponse","text":"<pre><code>AsyncRunsWithRawResponse(runs: AsyncRuns)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = async_to_raw_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithRawResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncStepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse","title":"AsyncRunsWithStreamingResponse","text":"<pre><code>AsyncRunsWithStreamingResponse(runs: AsyncRuns)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = async_to_streamed_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncRunsWithStreamingResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncStepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads","title":"AsyncThreads","text":"<pre><code>AsyncThreads(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    messages: Iterable[Message] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[Message] | NotGiven</code> <p>A list of messages to   start the thread with.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.create_and_run","title":"create_and_run  <code>async</code>","text":"<pre><code>create_and_run(\n    *,\n    assistant_id: str,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    thread: Thread | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a thread and run it in one request.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Override the default system message of the assistant. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>thread</code> <code>Thread | NotGiven</code> <p>If no thread is provided, an empty thread will be created.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadDeleted\n</code></pre> <p>Delete a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessages\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Retrieves a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRuns\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    thread_id: str,\n    *,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Modifies a thread.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreads.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncThreadsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse","title":"AsyncThreadsWithRawResponse","text":"<pre><code>AsyncThreadsWithRawResponse(threads: AsyncThreads)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = async_to_raw_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithRawResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse","title":"AsyncThreadsWithStreamingResponse","text":"<pre><code>AsyncThreadsWithStreamingResponse(threads: AsyncThreads)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = async_to_streamed_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.AsyncThreadsWithStreamingResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages","title":"Messages","text":"<pre><code>Messages(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.create","title":"create","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    content: str,\n    role: Literal[\"user\"],\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Create a message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>role</code> <code>Literal['user']</code> <p>The role of the entity that is creating the message. Currently only <code>user</code> is   supported.</p> required <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of File IDs that   the message should use. There can be a maximum of 10 files attached to a   message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can   access and use files.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.files","title":"files","text":"<pre><code>files() -&gt; Files\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[ThreadMessage]\n</code></pre> <p>Returns a list of messages for a given thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    message_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Retrieve a message.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.update","title":"update","text":"<pre><code>update(\n    message_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Modifies a message.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; MessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Messages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; MessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithRawResponse","title":"MessagesWithRawResponse","text":"<pre><code>MessagesWithRawResponse(messages: Messages)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithStreamingResponse","title":"MessagesWithStreamingResponse","text":"<pre><code>MessagesWithStreamingResponse(messages: Messages)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.MessagesWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs","title":"Runs","text":"<pre><code>Runs(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.cancel","title":"cancel","text":"<pre><code>cancel(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Cancels a run that is <code>in_progress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.create","title":"create","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    assistant_id: str,\n    additional_instructions: (\n        Optional[str] | NotGiven\n    ) = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a run.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>additional_instructions</code> <code>Optional[str] | NotGiven</code> <p>Appends additional instructions at the end of the instructions for the run. This   is useful for modifying the behavior on a per-run basis without overriding other   instructions.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Overrides the   instructions   of the assistant. This is useful for modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[Run]\n</code></pre> <p>Returns a list of runs belonging to a thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Retrieves a run.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.steps","title":"steps","text":"<pre><code>steps() -&gt; Steps\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.submit_tool_outputs","title":"submit_tool_outputs","text":"<pre><code>submit_tool_outputs(\n    run_id: str,\n    *,\n    thread_id: str,\n    tool_outputs: Iterable[ToolOutput],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>When a run has the <code>status: \"requires_action\"</code> and <code>required_action.type</code> is <code>submit_tool_outputs</code>, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.</p> <p>Parameters:</p> Name Type Description Default <code>tool_outputs</code> <code>Iterable[ToolOutput]</code> <p>A list of tools for which the outputs are being submitted.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.update","title":"update","text":"<pre><code>update(\n    run_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Modifies a run.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; RunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Runs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; RunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse","title":"RunsWithRawResponse","text":"<pre><code>RunsWithRawResponse(runs: Runs)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = to_raw_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithRawResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; StepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse","title":"RunsWithStreamingResponse","text":"<pre><code>RunsWithStreamingResponse(runs: Runs)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = to_streamed_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.RunsWithStreamingResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; StepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads","title":"Threads","text":"<pre><code>Threads(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.create","title":"create","text":"<pre><code>create(\n    *,\n    messages: Iterable[Message] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[Message] | NotGiven</code> <p>A list of messages to   start the thread with.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.create_and_run","title":"create_and_run","text":"<pre><code>create_and_run(\n    *,\n    assistant_id: str,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    thread: Thread | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a thread and run it in one request.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Override the default system message of the assistant. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>thread</code> <code>Thread | NotGiven</code> <p>If no thread is provided, an empty thread will be created.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.delete","title":"delete","text":"<pre><code>delete(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadDeleted\n</code></pre> <p>Delete a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.messages","title":"messages","text":"<pre><code>messages() -&gt; Messages\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Retrieves a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.runs","title":"runs","text":"<pre><code>runs() -&gt; Runs\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.update","title":"update","text":"<pre><code>update(\n    thread_id: str,\n    *,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Modifies a thread.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.Threads.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse","title":"ThreadsWithRawResponse","text":"<pre><code>ThreadsWithRawResponse(threads: Threads)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = to_raw_response_wrapper(create_and_run)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; MessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithRawResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; RunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse","title":"ThreadsWithStreamingResponse","text":"<pre><code>ThreadsWithStreamingResponse(threads: Threads)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = to_streamed_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; MessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/__init__/#src.openai.resources.beta.threads.ThreadsWithStreamingResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; RunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/","title":"Threads","text":""},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads","title":"threads","text":""},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads","title":"AsyncThreads","text":"<pre><code>AsyncThreads(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    messages: Iterable[Message] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[Message] | NotGiven</code> <p>A list of messages to   start the thread with.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.create_and_run","title":"create_and_run  <code>async</code>","text":"<pre><code>create_and_run(\n    *,\n    assistant_id: str,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    thread: Thread | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a thread and run it in one request.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Override the default system message of the assistant. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>thread</code> <code>Thread | NotGiven</code> <p>If no thread is provided, an empty thread will be created.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadDeleted\n</code></pre> <p>Delete a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessages\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Retrieves a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRuns\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    thread_id: str,\n    *,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Modifies a thread.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreads.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncThreadsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse","title":"AsyncThreadsWithRawResponse","text":"<pre><code>AsyncThreadsWithRawResponse(threads: AsyncThreads)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = async_to_raw_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithRawResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse","title":"AsyncThreadsWithStreamingResponse","text":"<pre><code>AsyncThreadsWithStreamingResponse(threads: AsyncThreads)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = async_to_streamed_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = async_to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; AsyncMessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.AsyncThreadsWithStreamingResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; AsyncRunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads","title":"Threads","text":"<pre><code>Threads(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.create","title":"create","text":"<pre><code>create(\n    *,\n    messages: Iterable[Message] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[Message] | NotGiven</code> <p>A list of messages to   start the thread with.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.create_and_run","title":"create_and_run","text":"<pre><code>create_and_run(\n    *,\n    assistant_id: str,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    thread: Thread | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a thread and run it in one request.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Override the default system message of the assistant. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>thread</code> <code>Thread | NotGiven</code> <p>If no thread is provided, an empty thread will be created.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.delete","title":"delete","text":"<pre><code>delete(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadDeleted\n</code></pre> <p>Delete a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.messages","title":"messages","text":"<pre><code>messages() -&gt; Messages\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    thread_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Retrieves a thread.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.runs","title":"runs","text":"<pre><code>runs() -&gt; Runs\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.update","title":"update","text":"<pre><code>update(\n    thread_id: str,\n    *,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Thread\n</code></pre> <p>Modifies a thread.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ThreadsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.Threads.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ThreadsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse","title":"ThreadsWithRawResponse","text":"<pre><code>ThreadsWithRawResponse(threads: Threads)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = to_raw_response_wrapper(create_and_run)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_raw_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; MessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithRawResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; RunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse","title":"ThreadsWithStreamingResponse","text":"<pre><code>ThreadsWithStreamingResponse(threads: Threads)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.create_and_run","title":"create_and_run  <code>instance-attribute</code>","text":"<pre><code>create_and_run = to_streamed_response_wrapper(\n    create_and_run\n)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = to_streamed_response_wrapper(delete)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.messages","title":"messages","text":"<pre><code>messages() -&gt; MessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/threads/#src.openai.resources.beta.threads.threads.ThreadsWithStreamingResponse.runs","title":"runs","text":"<pre><code>runs() -&gt; RunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/","title":"openai.resources.beta.threads.messages","text":""},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages","title":"messages","text":""},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFiles","title":"AsyncFiles","text":"<pre><code>AsyncFiles(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFiles.list","title":"list","text":"<pre><code>list(\n    message_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    MessageFile, AsyncCursorPage[MessageFile]\n]\n</code></pre> <p>Returns a list of message files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFiles.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    thread_id: str,\n    message_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; MessageFile\n</code></pre> <p>Retrieves a message file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFiles.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFiles.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFilesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFilesWithRawResponse","title":"AsyncFilesWithRawResponse","text":"<pre><code>AsyncFilesWithRawResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFilesWithStreamingResponse","title":"AsyncFilesWithStreamingResponse","text":"<pre><code>AsyncFilesWithStreamingResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncFilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages","title":"AsyncMessages","text":"<pre><code>AsyncMessages(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    content: str,\n    role: Literal[\"user\"],\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Create a message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>role</code> <code>Literal['user']</code> <p>The role of the entity that is creating the message. Currently only <code>user</code> is   supported.</p> required <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of File IDs that   the message should use. There can be a maximum of 10 files attached to a   message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can   access and use files.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.files","title":"files","text":"<pre><code>files() -&gt; AsyncFiles\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    ThreadMessage, AsyncCursorPage[ThreadMessage]\n]\n</code></pre> <p>Returns a list of messages for a given thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    message_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Retrieve a message.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    message_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Modifies a message.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncMessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncMessagesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithRawResponse","title":"AsyncMessagesWithRawResponse","text":"<pre><code>AsyncMessagesWithRawResponse(messages: AsyncMessages)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithStreamingResponse","title":"AsyncMessagesWithStreamingResponse","text":"<pre><code>AsyncMessagesWithStreamingResponse(messages: AsyncMessages)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.AsyncMessagesWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Files","title":"Files","text":"<pre><code>Files(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Files.list","title":"list","text":"<pre><code>list(\n    message_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[MessageFile]\n</code></pre> <p>Returns a list of message files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Files.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    thread_id: str,\n    message_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; MessageFile\n</code></pre> <p>Retrieves a message file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Files.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Files.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.FilesWithRawResponse","title":"FilesWithRawResponse","text":"<pre><code>FilesWithRawResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.FilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.FilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.FilesWithStreamingResponse","title":"FilesWithStreamingResponse","text":"<pre><code>FilesWithStreamingResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.FilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.FilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages","title":"Messages","text":"<pre><code>Messages(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.create","title":"create","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    content: str,\n    role: Literal[\"user\"],\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Create a message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>role</code> <code>Literal['user']</code> <p>The role of the entity that is creating the message. Currently only <code>user</code> is   supported.</p> required <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of File IDs that   the message should use. There can be a maximum of 10 files attached to a   message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can   access and use files.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.files","title":"files","text":"<pre><code>files() -&gt; Files\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[ThreadMessage]\n</code></pre> <p>Returns a list of messages for a given thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    message_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Retrieve a message.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.update","title":"update","text":"<pre><code>update(\n    message_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Modifies a message.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; MessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.Messages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; MessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithRawResponse","title":"MessagesWithRawResponse","text":"<pre><code>MessagesWithRawResponse(messages: Messages)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithStreamingResponse","title":"MessagesWithStreamingResponse","text":"<pre><code>MessagesWithStreamingResponse(messages: Messages)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/__init__/#src.openai.resources.beta.threads.messages.MessagesWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/files/","title":"Files","text":""},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files","title":"files","text":""},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFiles","title":"AsyncFiles","text":"<pre><code>AsyncFiles(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFiles.list","title":"list","text":"<pre><code>list(\n    message_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    MessageFile, AsyncCursorPage[MessageFile]\n]\n</code></pre> <p>Returns a list of message files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFiles.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    thread_id: str,\n    message_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; MessageFile\n</code></pre> <p>Retrieves a message file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFiles.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFiles.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFilesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFilesWithRawResponse","title":"AsyncFilesWithRawResponse","text":"<pre><code>AsyncFilesWithRawResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFilesWithStreamingResponse","title":"AsyncFilesWithStreamingResponse","text":"<pre><code>AsyncFilesWithStreamingResponse(files: AsyncFiles)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.AsyncFilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.Files","title":"Files","text":"<pre><code>Files(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.Files.list","title":"list","text":"<pre><code>list(\n    message_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[MessageFile]\n</code></pre> <p>Returns a list of message files.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination.</p> <code>NOT_GIVEN</code> <p><code>after</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include after=obj_foo in order to       fetch the next page of the list.</p> <p>before: A cursor for use in pagination. <code>before</code> is an object ID that defines your place       in the list. For instance, if you make a list request and receive 100 objects,       ending with obj_foo, your subsequent call can include before=obj_foo in order to       fetch the previous page of the list.</p> <p>limit: A limit on the number of objects to be returned. Limit can range between 1 and       100, and the default is 20.</p> <p>order: Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending       order and <code>desc</code> for descending order.</p> <p>extra_headers: Send extra headers</p> <p>extra_query: Add additional query parameters to the request</p> <p>extra_body: Add additional JSON properties to the request</p> <p>timeout: Override the client-level default timeout for this request, in seconds</p>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.Files.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    file_id: str,\n    *,\n    thread_id: str,\n    message_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; MessageFile\n</code></pre> <p>Retrieves a message file.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.Files.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.Files.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.FilesWithRawResponse","title":"FilesWithRawResponse","text":"<pre><code>FilesWithRawResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.FilesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.FilesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.FilesWithStreamingResponse","title":"FilesWithStreamingResponse","text":"<pre><code>FilesWithStreamingResponse(files: Files)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.FilesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/files/#src.openai.resources.beta.threads.messages.files.FilesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/","title":"Messages","text":""},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages","title":"messages","text":""},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages","title":"AsyncMessages","text":"<pre><code>AsyncMessages(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    content: str,\n    role: Literal[\"user\"],\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Create a message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>role</code> <code>Literal['user']</code> <p>The role of the entity that is creating the message. Currently only <code>user</code> is   supported.</p> required <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of File IDs that   the message should use. There can be a maximum of 10 files attached to a   message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can   access and use files.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.files","title":"files","text":"<pre><code>files() -&gt; AsyncFiles\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    ThreadMessage, AsyncCursorPage[ThreadMessage]\n]\n</code></pre> <p>Returns a list of messages for a given thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    message_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Retrieve a message.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    message_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Modifies a message.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncMessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncMessagesWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithRawResponse","title":"AsyncMessagesWithRawResponse","text":"<pre><code>AsyncMessagesWithRawResponse(messages: AsyncMessages)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithStreamingResponse","title":"AsyncMessagesWithStreamingResponse","text":"<pre><code>AsyncMessagesWithStreamingResponse(messages: AsyncMessages)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.AsyncMessagesWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; AsyncFilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages","title":"Messages","text":"<pre><code>Messages(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.create","title":"create","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    content: str,\n    role: Literal[\"user\"],\n    file_ids: List[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Create a message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>role</code> <code>Literal['user']</code> <p>The role of the entity that is creating the message. Currently only <code>user</code> is   supported.</p> required <code>file_ids</code> <code>List[str] | NotGiven</code> <p>A list of File IDs that   the message should use. There can be a maximum of 10 files attached to a   message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can   access and use files.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.files","title":"files","text":"<pre><code>files() -&gt; Files\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[ThreadMessage]\n</code></pre> <p>Returns a list of messages for a given thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    message_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Retrieve a message.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.update","title":"update","text":"<pre><code>update(\n    message_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ThreadMessage\n</code></pre> <p>Modifies a message.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; MessagesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.Messages.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; MessagesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithRawResponse","title":"MessagesWithRawResponse","text":"<pre><code>MessagesWithRawResponse(messages: Messages)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithRawResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithStreamingResponse","title":"MessagesWithStreamingResponse","text":"<pre><code>MessagesWithStreamingResponse(messages: Messages)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/messages/messages/#src.openai.resources.beta.threads.messages.messages.MessagesWithStreamingResponse.files","title":"files","text":"<pre><code>files() -&gt; FilesWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/","title":"openai.resources.beta.threads.runs","text":""},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs","title":"runs","text":""},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns","title":"AsyncRuns","text":"<pre><code>AsyncRuns(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.cancel","title":"cancel  <code>async</code>","text":"<pre><code>cancel(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Cancels a run that is <code>in_progress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    assistant_id: str,\n    additional_instructions: (\n        Optional[str] | NotGiven\n    ) = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a run.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>additional_instructions</code> <code>Optional[str] | NotGiven</code> <p>Appends additional instructions at the end of the instructions for the run. This   is useful for modifying the behavior on a per-run basis without overriding other   instructions.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Overrides the   instructions   of the assistant. This is useful for modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Run, AsyncCursorPage[Run]]\n</code></pre> <p>Returns a list of runs belonging to a thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Retrieves a run.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncSteps\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.submit_tool_outputs","title":"submit_tool_outputs  <code>async</code>","text":"<pre><code>submit_tool_outputs(\n    run_id: str,\n    *,\n    thread_id: str,\n    tool_outputs: Iterable[ToolOutput],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>When a run has the <code>status: \"requires_action\"</code> and <code>required_action.type</code> is <code>submit_tool_outputs</code>, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.</p> <p>Parameters:</p> Name Type Description Default <code>tool_outputs</code> <code>Iterable[ToolOutput]</code> <p>A list of tools for which the outputs are being submitted.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    run_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Modifies a run.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncRunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRuns.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncRunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse","title":"AsyncRunsWithRawResponse","text":"<pre><code>AsyncRunsWithRawResponse(runs: AsyncRuns)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = async_to_raw_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithRawResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncStepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse","title":"AsyncRunsWithStreamingResponse","text":"<pre><code>AsyncRunsWithStreamingResponse(runs: AsyncRuns)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = async_to_streamed_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncRunsWithStreamingResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncStepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncSteps","title":"AsyncSteps","text":"<pre><code>AsyncSteps(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncSteps.list","title":"list","text":"<pre><code>list(\n    run_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[RunStep, AsyncCursorPage[RunStep]]\n</code></pre> <p>Returns a list of run steps belonging to a run.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncSteps.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    step_id: str,\n    *,\n    thread_id: str,\n    run_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; RunStep\n</code></pre> <p>Retrieves a run step.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncSteps.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncStepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncSteps.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncStepsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncStepsWithRawResponse","title":"AsyncStepsWithRawResponse","text":"<pre><code>AsyncStepsWithRawResponse(steps: AsyncSteps)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncStepsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncStepsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncStepsWithStreamingResponse","title":"AsyncStepsWithStreamingResponse","text":"<pre><code>AsyncStepsWithStreamingResponse(steps: AsyncSteps)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncStepsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.AsyncStepsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs","title":"Runs","text":"<pre><code>Runs(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.cancel","title":"cancel","text":"<pre><code>cancel(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Cancels a run that is <code>in_progress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.create","title":"create","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    assistant_id: str,\n    additional_instructions: (\n        Optional[str] | NotGiven\n    ) = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a run.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>additional_instructions</code> <code>Optional[str] | NotGiven</code> <p>Appends additional instructions at the end of the instructions for the run. This   is useful for modifying the behavior on a per-run basis without overriding other   instructions.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Overrides the   instructions   of the assistant. This is useful for modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[Run]\n</code></pre> <p>Returns a list of runs belonging to a thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Retrieves a run.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.steps","title":"steps","text":"<pre><code>steps() -&gt; Steps\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.submit_tool_outputs","title":"submit_tool_outputs","text":"<pre><code>submit_tool_outputs(\n    run_id: str,\n    *,\n    thread_id: str,\n    tool_outputs: Iterable[ToolOutput],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>When a run has the <code>status: \"requires_action\"</code> and <code>required_action.type</code> is <code>submit_tool_outputs</code>, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.</p> <p>Parameters:</p> Name Type Description Default <code>tool_outputs</code> <code>Iterable[ToolOutput]</code> <p>A list of tools for which the outputs are being submitted.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.update","title":"update","text":"<pre><code>update(\n    run_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Modifies a run.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; RunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Runs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; RunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse","title":"RunsWithRawResponse","text":"<pre><code>RunsWithRawResponse(runs: Runs)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = to_raw_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithRawResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; StepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse","title":"RunsWithStreamingResponse","text":"<pre><code>RunsWithStreamingResponse(runs: Runs)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = to_streamed_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.RunsWithStreamingResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; StepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Steps","title":"Steps","text":"<pre><code>Steps(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Steps.list","title":"list","text":"<pre><code>list(\n    run_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[RunStep]\n</code></pre> <p>Returns a list of run steps belonging to a run.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Steps.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    step_id: str,\n    *,\n    thread_id: str,\n    run_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; RunStep\n</code></pre> <p>Retrieves a run step.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Steps.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; StepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.Steps.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; StepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.StepsWithRawResponse","title":"StepsWithRawResponse","text":"<pre><code>StepsWithRawResponse(steps: Steps)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.StepsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.StepsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.StepsWithStreamingResponse","title":"StepsWithStreamingResponse","text":"<pre><code>StepsWithStreamingResponse(steps: Steps)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.StepsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/__init__/#src.openai.resources.beta.threads.runs.StepsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/","title":"Runs","text":""},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs","title":"runs","text":""},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns","title":"AsyncRuns","text":"<pre><code>AsyncRuns(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.cancel","title":"cancel  <code>async</code>","text":"<pre><code>cancel(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Cancels a run that is <code>in_progress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    assistant_id: str,\n    additional_instructions: (\n        Optional[str] | NotGiven\n    ) = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a run.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>additional_instructions</code> <code>Optional[str] | NotGiven</code> <p>Appends additional instructions at the end of the instructions for the run. This   is useful for modifying the behavior on a per-run basis without overriding other   instructions.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Overrides the   instructions   of the assistant. This is useful for modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[Run, AsyncCursorPage[Run]]\n</code></pre> <p>Returns a list of runs belonging to a thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Retrieves a run.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncSteps\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.submit_tool_outputs","title":"submit_tool_outputs  <code>async</code>","text":"<pre><code>submit_tool_outputs(\n    run_id: str,\n    *,\n    thread_id: str,\n    tool_outputs: Iterable[ToolOutput],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>When a run has the <code>status: \"requires_action\"</code> and <code>required_action.type</code> is <code>submit_tool_outputs</code>, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.</p> <p>Parameters:</p> Name Type Description Default <code>tool_outputs</code> <code>Iterable[ToolOutput]</code> <p>A list of tools for which the outputs are being submitted.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.update","title":"update  <code>async</code>","text":"<pre><code>update(\n    run_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Modifies a run.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncRunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRuns.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncRunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse","title":"AsyncRunsWithRawResponse","text":"<pre><code>AsyncRunsWithRawResponse(runs: AsyncRuns)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = async_to_raw_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithRawResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncStepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse","title":"AsyncRunsWithStreamingResponse","text":"<pre><code>AsyncRunsWithStreamingResponse(runs: AsyncRuns)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = async_to_streamed_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = async_to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.AsyncRunsWithStreamingResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; AsyncStepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs","title":"Runs","text":"<pre><code>Runs(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.cancel","title":"cancel","text":"<pre><code>cancel(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Cancels a run that is <code>in_progress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.create","title":"create","text":"<pre><code>create(\n    thread_id: str,\n    *,\n    assistant_id: str,\n    additional_instructions: (\n        Optional[str] | NotGiven\n    ) = NOT_GIVEN,\n    instructions: Optional[str] | NotGiven = NOT_GIVEN,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    model: Optional[str] | NotGiven = NOT_GIVEN,\n    tools: Optional[Iterable[Tool]] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Create a run.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the   assistant to use to   execute this run.</p> required <code>additional_instructions</code> <code>Optional[str] | NotGiven</code> <p>Appends additional instructions at the end of the instructions for the run. This   is useful for modifying the behavior on a per-run basis without overriding other   instructions.</p> <code>NOT_GIVEN</code> <code>instructions</code> <code>Optional[str] | NotGiven</code> <p>Overrides the   instructions   of the assistant. This is useful for modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>model</code> <code>Optional[str] | NotGiven</code> <p>The ID of the Model to   be used to execute this run. If a value is provided here, it will override the   model associated with the assistant. If not, the model associated with the   assistant will be used.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Optional[Iterable[Tool]] | NotGiven</code> <p>Override the tools the assistant can use for this run. This is useful for   modifying the behavior on a per-run basis.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.list","title":"list","text":"<pre><code>list(\n    thread_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[Run]\n</code></pre> <p>Returns a list of runs belonging to a thread.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    run_id: str,\n    *,\n    thread_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Retrieves a run.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.steps","title":"steps","text":"<pre><code>steps() -&gt; Steps\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.submit_tool_outputs","title":"submit_tool_outputs","text":"<pre><code>submit_tool_outputs(\n    run_id: str,\n    *,\n    thread_id: str,\n    tool_outputs: Iterable[ToolOutput],\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>When a run has the <code>status: \"requires_action\"</code> and <code>required_action.type</code> is <code>submit_tool_outputs</code>, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.</p> <p>Parameters:</p> Name Type Description Default <code>tool_outputs</code> <code>Iterable[ToolOutput]</code> <p>A list of tools for which the outputs are being submitted.</p> required <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.update","title":"update","text":"<pre><code>update(\n    run_id: str,\n    *,\n    thread_id: str,\n    metadata: Optional[object] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; Run\n</code></pre> <p>Modifies a run.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[object] | NotGiven</code> <p>Set of 16 key-value pairs that can be attached to an object. This can be useful   for storing additional information about the object in a structured format. Keys   can be a maximum of 64 characters long and values can be a maxium of 512   characters long.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; RunsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.Runs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; RunsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse","title":"RunsWithRawResponse","text":"<pre><code>RunsWithRawResponse(runs: Runs)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = to_raw_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_raw_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithRawResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; StepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse","title":"RunsWithStreamingResponse","text":"<pre><code>RunsWithStreamingResponse(runs: Runs)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs = to_streamed_response_wrapper(\n    submit_tool_outputs\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.update","title":"update  <code>instance-attribute</code>","text":"<pre><code>update = to_streamed_response_wrapper(update)\n</code></pre>"},{"location":"resources/beta/threads/runs/runs/#src.openai.resources.beta.threads.runs.runs.RunsWithStreamingResponse.steps","title":"steps","text":"<pre><code>steps() -&gt; StepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/","title":"Steps","text":""},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps","title":"steps","text":""},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncSteps","title":"AsyncSteps","text":"<pre><code>AsyncSteps(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncSteps.list","title":"list","text":"<pre><code>list(\n    run_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[RunStep, AsyncCursorPage[RunStep]]\n</code></pre> <p>Returns a list of run steps belonging to a run.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncSteps.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    step_id: str,\n    *,\n    thread_id: str,\n    run_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; RunStep\n</code></pre> <p>Retrieves a run step.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncSteps.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncStepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncSteps.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncStepsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncStepsWithRawResponse","title":"AsyncStepsWithRawResponse","text":"<pre><code>AsyncStepsWithRawResponse(steps: AsyncSteps)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncStepsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncStepsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncStepsWithStreamingResponse","title":"AsyncStepsWithStreamingResponse","text":"<pre><code>AsyncStepsWithStreamingResponse(steps: AsyncSteps)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncStepsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.AsyncStepsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.Steps","title":"Steps","text":"<pre><code>Steps(client: OpenAI)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.Steps.list","title":"list","text":"<pre><code>list(\n    run_id: str,\n    *,\n    thread_id: str,\n    after: str | NotGiven = NOT_GIVEN,\n    before: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    order: Literal[\"asc\", \"desc\"] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[RunStep]\n</code></pre> <p>Returns a list of run steps belonging to a run.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>after</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include after=obj_foo in order to   fetch the next page of the list.</p> <code>NOT_GIVEN</code> <code>before</code> <code>str | NotGiven</code> <p>A cursor for use in pagination. <code>before</code> is an object ID that defines your place   in the list. For instance, if you make a list request and receive 100 objects,   ending with obj_foo, your subsequent call can include before=obj_foo in order to   fetch the previous page of the list.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>A limit on the number of objects to be returned. Limit can range between 1 and   100, and the default is 20.</p> <code>NOT_GIVEN</code> <code>order</code> <code>Literal['asc', 'desc'] | NotGiven</code> <p>Sort order by the <code>created_at</code> timestamp of the objects. <code>asc</code> for ascending   order and <code>desc</code> for descending order.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.Steps.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    step_id: str,\n    *,\n    thread_id: str,\n    run_id: str,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; RunStep\n</code></pre> <p>Retrieves a run step.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.Steps.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; StepsWithRawResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.Steps.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; StepsWithStreamingResponse\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.StepsWithRawResponse","title":"StepsWithRawResponse","text":"<pre><code>StepsWithRawResponse(steps: Steps)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.StepsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.StepsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.StepsWithStreamingResponse","title":"StepsWithStreamingResponse","text":"<pre><code>StepsWithStreamingResponse(steps: Steps)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.StepsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/beta/threads/runs/steps/#src.openai.resources.beta.threads.runs.steps.StepsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/chat/__init__/","title":"openai.resources.chat","text":""},{"location":"resources/chat/__init__/#src.openai.resources.chat","title":"chat","text":""},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChat","title":"AsyncChat","text":"<pre><code>AsyncChat(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChat.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletions\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChat.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncChatWithRawResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChat.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncChatWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChatWithRawResponse","title":"AsyncChatWithRawResponse","text":"<pre><code>AsyncChatWithRawResponse(chat: AsyncChat)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChatWithRawResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChatWithStreamingResponse","title":"AsyncChatWithStreamingResponse","text":"<pre><code>AsyncChatWithStreamingResponse(chat: AsyncChat)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncChatWithStreamingResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletionsWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletions","title":"AsyncCompletions","text":"<pre><code>AsyncCompletions(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletions.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    messages: Iterable[ChatCompletionMessageParam],\n    model: Union[\n        str,\n        Literal[\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4-vision-preview\",\n            \"gpt-4\",\n            \"gpt-4-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-32k-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo-0301\",\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-16k-0613\",\n        ],\n    ],\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    function_call: FunctionCall | NotGiven = NOT_GIVEN,\n    functions: Iterable[Function] | NotGiven = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[bool] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: ResponseFormat | NotGiven = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str]] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    tool_choice: (\n        ChatCompletionToolChoiceOptionParam | NotGiven\n    ) = NOT_GIVEN,\n    tools: (\n        Iterable[ChatCompletionToolParam] | NotGiven\n    ) = NOT_GIVEN,\n    top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ChatCompletion | AsyncStream[ChatCompletionChunk]\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncCompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletionsWithRawResponse","title":"AsyncCompletionsWithRawResponse","text":"<pre><code>AsyncCompletionsWithRawResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletionsWithStreamingResponse","title":"AsyncCompletionsWithStreamingResponse","text":"<pre><code>AsyncCompletionsWithStreamingResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.AsyncCompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Chat","title":"Chat","text":"<pre><code>Chat(client: OpenAI)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Chat.completions","title":"completions","text":"<pre><code>completions() -&gt; Completions\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Chat.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ChatWithRawResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Chat.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ChatWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.ChatWithRawResponse","title":"ChatWithRawResponse","text":"<pre><code>ChatWithRawResponse(chat: Chat)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.ChatWithRawResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.ChatWithStreamingResponse","title":"ChatWithStreamingResponse","text":"<pre><code>ChatWithStreamingResponse(chat: Chat)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.ChatWithStreamingResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; CompletionsWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Completions","title":"Completions","text":"<pre><code>Completions(client: OpenAI)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Completions.create","title":"create","text":"<pre><code>create(\n    *,\n    messages: Iterable[ChatCompletionMessageParam],\n    model: Union[\n        str,\n        Literal[\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4-vision-preview\",\n            \"gpt-4\",\n            \"gpt-4-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-32k-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo-0301\",\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-16k-0613\",\n        ],\n    ],\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    function_call: FunctionCall | NotGiven = NOT_GIVEN,\n    functions: Iterable[Function] | NotGiven = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[bool] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: ResponseFormat | NotGiven = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str]] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    tool_choice: (\n        ChatCompletionToolChoiceOptionParam | NotGiven\n    ) = NOT_GIVEN,\n    tools: (\n        Iterable[ChatCompletionToolParam] | NotGiven\n    ) = NOT_GIVEN,\n    top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ChatCompletion | Stream[ChatCompletionChunk]\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Completions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.Completions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    CompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.CompletionsWithRawResponse","title":"CompletionsWithRawResponse","text":"<pre><code>CompletionsWithRawResponse(completions: Completions)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.CompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.CompletionsWithStreamingResponse","title":"CompletionsWithStreamingResponse","text":"<pre><code>CompletionsWithStreamingResponse(completions: Completions)\n</code></pre>"},{"location":"resources/chat/__init__/#src.openai.resources.chat.CompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/chat/","title":"Chat","text":""},{"location":"resources/chat/chat/#src.openai.resources.chat.chat","title":"chat","text":""},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChat","title":"AsyncChat","text":"<pre><code>AsyncChat(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChat.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletions\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChat.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncChatWithRawResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChat.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncChatWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChatWithRawResponse","title":"AsyncChatWithRawResponse","text":"<pre><code>AsyncChatWithRawResponse(chat: AsyncChat)\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChatWithRawResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChatWithStreamingResponse","title":"AsyncChatWithStreamingResponse","text":"<pre><code>AsyncChatWithStreamingResponse(chat: AsyncChat)\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.AsyncChatWithStreamingResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; AsyncCompletionsWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.Chat","title":"Chat","text":"<pre><code>Chat(client: OpenAI)\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.Chat.completions","title":"completions","text":"<pre><code>completions() -&gt; Completions\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.Chat.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; ChatWithRawResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.Chat.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; ChatWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.ChatWithRawResponse","title":"ChatWithRawResponse","text":"<pre><code>ChatWithRawResponse(chat: Chat)\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.ChatWithRawResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.ChatWithStreamingResponse","title":"ChatWithStreamingResponse","text":"<pre><code>ChatWithStreamingResponse(chat: Chat)\n</code></pre>"},{"location":"resources/chat/chat/#src.openai.resources.chat.chat.ChatWithStreamingResponse.completions","title":"completions","text":"<pre><code>completions() -&gt; CompletionsWithStreamingResponse\n</code></pre>"},{"location":"resources/chat/completions/","title":"Completions","text":""},{"location":"resources/chat/completions/#src.openai.resources.chat.completions","title":"completions","text":""},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletions","title":"AsyncCompletions","text":"<pre><code>AsyncCompletions(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletions.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    messages: Iterable[ChatCompletionMessageParam],\n    model: Union[\n        str,\n        Literal[\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4-vision-preview\",\n            \"gpt-4\",\n            \"gpt-4-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-32k-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo-0301\",\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-16k-0613\",\n        ],\n    ],\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    function_call: FunctionCall | NotGiven = NOT_GIVEN,\n    functions: Iterable[Function] | NotGiven = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[bool] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: ResponseFormat | NotGiven = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str]] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    tool_choice: (\n        ChatCompletionToolChoiceOptionParam | NotGiven\n    ) = NOT_GIVEN,\n    tools: (\n        Iterable[ChatCompletionToolParam] | NotGiven\n    ) = NOT_GIVEN,\n    top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ChatCompletion | AsyncStream[ChatCompletionChunk]\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncCompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncCompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletionsWithRawResponse","title":"AsyncCompletionsWithRawResponse","text":"<pre><code>AsyncCompletionsWithRawResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletionsWithStreamingResponse","title":"AsyncCompletionsWithStreamingResponse","text":"<pre><code>AsyncCompletionsWithStreamingResponse(\n    completions: AsyncCompletions,\n)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.AsyncCompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.Completions","title":"Completions","text":"<pre><code>Completions(client: OpenAI)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.Completions.create","title":"create","text":"<pre><code>create(\n    *,\n    messages: Iterable[ChatCompletionMessageParam],\n    model: Union[\n        str,\n        Literal[\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4-vision-preview\",\n            \"gpt-4\",\n            \"gpt-4-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-32k-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo-0301\",\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-16k-0613\",\n        ],\n    ],\n    frequency_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    function_call: FunctionCall | NotGiven = NOT_GIVEN,\n    functions: Iterable[Function] | NotGiven = NOT_GIVEN,\n    logit_bias: (\n        Optional[Dict[str, int]] | NotGiven\n    ) = NOT_GIVEN,\n    logprobs: Optional[bool] | NotGiven = NOT_GIVEN,\n    max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n    n: Optional[int] | NotGiven = NOT_GIVEN,\n    presence_penalty: (\n        Optional[float] | NotGiven\n    ) = NOT_GIVEN,\n    response_format: ResponseFormat | NotGiven = NOT_GIVEN,\n    seed: Optional[int] | NotGiven = NOT_GIVEN,\n    stop: (\n        Union[Optional[str], List[str]] | NotGiven\n    ) = NOT_GIVEN,\n    stream: (\n        Optional[Literal[False]] | Literal[True] | NotGiven\n    ) = NOT_GIVEN,\n    temperature: Optional[float] | NotGiven = NOT_GIVEN,\n    tool_choice: (\n        ChatCompletionToolChoiceOptionParam | NotGiven\n    ) = NOT_GIVEN,\n    tools: (\n        Iterable[ChatCompletionToolParam] | NotGiven\n    ) = NOT_GIVEN,\n    top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n    top_p: Optional[float] | NotGiven = NOT_GIVEN,\n    user: str | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; ChatCompletion | Stream[ChatCompletionChunk]\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.Completions.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; CompletionsWithRawResponse\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.Completions.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    CompletionsWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.CompletionsWithRawResponse","title":"CompletionsWithRawResponse","text":"<pre><code>CompletionsWithRawResponse(completions: Completions)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.CompletionsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.CompletionsWithStreamingResponse","title":"CompletionsWithStreamingResponse","text":"<pre><code>CompletionsWithStreamingResponse(completions: Completions)\n</code></pre>"},{"location":"resources/chat/completions/#src.openai.resources.chat.completions.CompletionsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/","title":"openai.resources.fine_tuning","text":""},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning","title":"fine_tuning","text":""},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuning","title":"AsyncFineTuning","text":"<pre><code>AsyncFineTuning(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuning.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobs\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuning.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFineTuningWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuning.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFineTuningWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuningWithRawResponse","title":"AsyncFineTuningWithRawResponse","text":"<pre><code>AsyncFineTuningWithRawResponse(\n    fine_tuning: AsyncFineTuning,\n)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuningWithRawResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuningWithStreamingResponse","title":"AsyncFineTuningWithStreamingResponse","text":"<pre><code>AsyncFineTuningWithStreamingResponse(\n    fine_tuning: AsyncFineTuning,\n)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncFineTuningWithStreamingResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs","title":"AsyncJobs","text":"<pre><code>AsyncJobs(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.cancel","title":"cancel  <code>async</code>","text":"<pre><code>cancel(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Immediately cancel a fine-tune job.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"babbage-002\", \"davinci-002\", \"gpt-3.5-turbo\"\n        ],\n    ],\n    training_file: str,\n    hyperparameters: Hyperparameters | NotGiven = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    validation_file: Optional[str] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Creates a fine-tuning job which begins the process of creating a new model from a given dataset.</p> <p>Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[str, Literal['babbage-002', 'davinci-002', 'gpt-3.5-turbo']]</code> <p>The name of the model to fine-tune. You can select one of the   supported models.</p> required <code>training_file</code> <code>str</code> <p>The ID of an uploaded file that contains training data.</p> <p>See upload file   for how to upload a file.</p> <p>Your dataset must be formatted as a JSONL file. Additionally, you must upload   your file with the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> required <code>hyperparameters</code> <code>Hyperparameters | NotGiven</code> <p>The hyperparameters used for the fine-tuning job.</p> <code>NOT_GIVEN</code> <code>suffix</code> <code>Optional[str] | NotGiven</code> <p>A string of up to 18 characters that will be added to your fine-tuned model   name.</p> <p>For example, a <code>suffix</code> of \"custom-model-name\" would produce a model name like   <code>ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel</code>.</p> <code>NOT_GIVEN</code> <code>validation_file</code> <code>Optional[str] | NotGiven</code> <p>The ID of an uploaded file that contains validation data.</p> <p>If you provide this file, the data is used to generate validation metrics   periodically during fine-tuning. These metrics can be viewed in the fine-tuning   results file. The same data should not be present in both train and validation   files.</p> <p>Your dataset must be formatted as a JSONL file. You must upload your file with   the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    FineTuningJob, AsyncCursorPage[FineTuningJob]\n]\n</code></pre> <p>List your organization's fine-tuning jobs</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last job from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of fine-tuning jobs to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.list_events","title":"list_events","text":"<pre><code>list_events(\n    fine_tuning_job_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    FineTuningJobEvent, AsyncCursorPage[FineTuningJobEvent]\n]\n</code></pre> <p>Get status updates for a fine-tuning job.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last event from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of events to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Get info about a fine-tuning job.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncJobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncJobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithRawResponse","title":"AsyncJobsWithRawResponse","text":"<pre><code>AsyncJobsWithRawResponse(jobs: AsyncJobs)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithRawResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = async_to_raw_response_wrapper(list_events)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithStreamingResponse","title":"AsyncJobsWithStreamingResponse","text":"<pre><code>AsyncJobsWithStreamingResponse(jobs: AsyncJobs)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithStreamingResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = async_to_streamed_response_wrapper(\n    list_events\n)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.AsyncJobsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuning","title":"FineTuning","text":"<pre><code>FineTuning(client: OpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuning.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; Jobs\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuning.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FineTuningWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuning.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    FineTuningWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuningWithRawResponse","title":"FineTuningWithRawResponse","text":"<pre><code>FineTuningWithRawResponse(fine_tuning: FineTuning)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuningWithRawResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; JobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuningWithStreamingResponse","title":"FineTuningWithStreamingResponse","text":"<pre><code>FineTuningWithStreamingResponse(fine_tuning: FineTuning)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.FineTuningWithStreamingResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; JobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs","title":"Jobs","text":"<pre><code>Jobs(client: OpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.cancel","title":"cancel","text":"<pre><code>cancel(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Immediately cancel a fine-tune job.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.create","title":"create","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"babbage-002\", \"davinci-002\", \"gpt-3.5-turbo\"\n        ],\n    ],\n    training_file: str,\n    hyperparameters: Hyperparameters | NotGiven = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    validation_file: Optional[str] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Creates a fine-tuning job which begins the process of creating a new model from a given dataset.</p> <p>Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[str, Literal['babbage-002', 'davinci-002', 'gpt-3.5-turbo']]</code> <p>The name of the model to fine-tune. You can select one of the   supported models.</p> required <code>training_file</code> <code>str</code> <p>The ID of an uploaded file that contains training data.</p> <p>See upload file   for how to upload a file.</p> <p>Your dataset must be formatted as a JSONL file. Additionally, you must upload   your file with the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> required <code>hyperparameters</code> <code>Hyperparameters | NotGiven</code> <p>The hyperparameters used for the fine-tuning job.</p> <code>NOT_GIVEN</code> <code>suffix</code> <code>Optional[str] | NotGiven</code> <p>A string of up to 18 characters that will be added to your fine-tuned model   name.</p> <p>For example, a <code>suffix</code> of \"custom-model-name\" would produce a model name like   <code>ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel</code>.</p> <code>NOT_GIVEN</code> <code>validation_file</code> <code>Optional[str] | NotGiven</code> <p>The ID of an uploaded file that contains validation data.</p> <p>If you provide this file, the data is used to generate validation metrics   periodically during fine-tuning. These metrics can be viewed in the fine-tuning   results file. The same data should not be present in both train and validation   files.</p> <p>Your dataset must be formatted as a JSONL file. You must upload your file with   the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[FineTuningJob]\n</code></pre> <p>List your organization's fine-tuning jobs</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last job from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of fine-tuning jobs to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.list_events","title":"list_events","text":"<pre><code>list_events(\n    fine_tuning_job_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[FineTuningJobEvent]\n</code></pre> <p>Get status updates for a fine-tuning job.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last event from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of events to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Get info about a fine-tuning job.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; JobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.Jobs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; JobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithRawResponse","title":"JobsWithRawResponse","text":"<pre><code>JobsWithRawResponse(jobs: Jobs)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithRawResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = to_raw_response_wrapper(list_events)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithStreamingResponse","title":"JobsWithStreamingResponse","text":"<pre><code>JobsWithStreamingResponse(jobs: Jobs)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithStreamingResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = to_streamed_response_wrapper(list_events)\n</code></pre>"},{"location":"resources/fine_tuning/__init__/#src.openai.resources.fine_tuning.JobsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/","title":"Fine tuning","text":""},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning","title":"fine_tuning","text":""},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuning","title":"AsyncFineTuning","text":"<pre><code>AsyncFineTuning(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuning.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobs\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuning.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncFineTuningWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuning.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    AsyncFineTuningWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuningWithRawResponse","title":"AsyncFineTuningWithRawResponse","text":"<pre><code>AsyncFineTuningWithRawResponse(\n    fine_tuning: AsyncFineTuning,\n)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuningWithRawResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuningWithStreamingResponse","title":"AsyncFineTuningWithStreamingResponse","text":"<pre><code>AsyncFineTuningWithStreamingResponse(\n    fine_tuning: AsyncFineTuning,\n)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.AsyncFineTuningWithStreamingResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; AsyncJobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuning","title":"FineTuning","text":"<pre><code>FineTuning(client: OpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuning.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; Jobs\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuning.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; FineTuningWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuning.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; (\n    FineTuningWithStreamingResponse\n)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuningWithRawResponse","title":"FineTuningWithRawResponse","text":"<pre><code>FineTuningWithRawResponse(fine_tuning: FineTuning)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuningWithRawResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; JobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuningWithStreamingResponse","title":"FineTuningWithStreamingResponse","text":"<pre><code>FineTuningWithStreamingResponse(fine_tuning: FineTuning)\n</code></pre>"},{"location":"resources/fine_tuning/fine_tuning/#src.openai.resources.fine_tuning.fine_tuning.FineTuningWithStreamingResponse.jobs","title":"jobs","text":"<pre><code>jobs() -&gt; JobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/jobs/","title":"Jobs","text":""},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs","title":"jobs","text":""},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs","title":"AsyncJobs","text":"<pre><code>AsyncJobs(client: AsyncOpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.cancel","title":"cancel  <code>async</code>","text":"<pre><code>cancel(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Immediately cancel a fine-tune job.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"babbage-002\", \"davinci-002\", \"gpt-3.5-turbo\"\n        ],\n    ],\n    training_file: str,\n    hyperparameters: Hyperparameters | NotGiven = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    validation_file: Optional[str] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Creates a fine-tuning job which begins the process of creating a new model from a given dataset.</p> <p>Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[str, Literal['babbage-002', 'davinci-002', 'gpt-3.5-turbo']]</code> <p>The name of the model to fine-tune. You can select one of the   supported models.</p> required <code>training_file</code> <code>str</code> <p>The ID of an uploaded file that contains training data.</p> <p>See upload file   for how to upload a file.</p> <p>Your dataset must be formatted as a JSONL file. Additionally, you must upload   your file with the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> required <code>hyperparameters</code> <code>Hyperparameters | NotGiven</code> <p>The hyperparameters used for the fine-tuning job.</p> <code>NOT_GIVEN</code> <code>suffix</code> <code>Optional[str] | NotGiven</code> <p>A string of up to 18 characters that will be added to your fine-tuned model   name.</p> <p>For example, a <code>suffix</code> of \"custom-model-name\" would produce a model name like   <code>ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel</code>.</p> <code>NOT_GIVEN</code> <code>validation_file</code> <code>Optional[str] | NotGiven</code> <p>The ID of an uploaded file that contains validation data.</p> <p>If you provide this file, the data is used to generate validation metrics   periodically during fine-tuning. These metrics can be viewed in the fine-tuning   results file. The same data should not be present in both train and validation   files.</p> <p>Your dataset must be formatted as a JSONL file. You must upload your file with   the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    FineTuningJob, AsyncCursorPage[FineTuningJob]\n]\n</code></pre> <p>List your organization's fine-tuning jobs</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last job from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of fine-tuning jobs to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.list_events","title":"list_events","text":"<pre><code>list_events(\n    fine_tuning_job_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; AsyncPaginator[\n    FineTuningJobEvent, AsyncCursorPage[FineTuningJobEvent]\n]\n</code></pre> <p>Get status updates for a fine-tuning job.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last event from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of events to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Get info about a fine-tuning job.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; AsyncJobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; AsyncJobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithRawResponse","title":"AsyncJobsWithRawResponse","text":"<pre><code>AsyncJobsWithRawResponse(jobs: AsyncJobs)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithRawResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = async_to_raw_response_wrapper(list_events)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithStreamingResponse","title":"AsyncJobsWithStreamingResponse","text":"<pre><code>AsyncJobsWithStreamingResponse(jobs: AsyncJobs)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = async_to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = async_to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = async_to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithStreamingResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = async_to_streamed_response_wrapper(\n    list_events\n)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.AsyncJobsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = async_to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs","title":"Jobs","text":"<pre><code>Jobs(client: OpenAI)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.cancel","title":"cancel","text":"<pre><code>cancel(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Immediately cancel a fine-tune job.</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.create","title":"create","text":"<pre><code>create(\n    *,\n    model: Union[\n        str,\n        Literal[\n            \"babbage-002\", \"davinci-002\", \"gpt-3.5-turbo\"\n        ],\n    ],\n    training_file: str,\n    hyperparameters: Hyperparameters | NotGiven = NOT_GIVEN,\n    suffix: Optional[str] | NotGiven = NOT_GIVEN,\n    validation_file: Optional[str] | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Creates a fine-tuning job which begins the process of creating a new model from a given dataset.</p> <p>Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[str, Literal['babbage-002', 'davinci-002', 'gpt-3.5-turbo']]</code> <p>The name of the model to fine-tune. You can select one of the   supported models.</p> required <code>training_file</code> <code>str</code> <p>The ID of an uploaded file that contains training data.</p> <p>See upload file   for how to upload a file.</p> <p>Your dataset must be formatted as a JSONL file. Additionally, you must upload   your file with the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> required <code>hyperparameters</code> <code>Hyperparameters | NotGiven</code> <p>The hyperparameters used for the fine-tuning job.</p> <code>NOT_GIVEN</code> <code>suffix</code> <code>Optional[str] | NotGiven</code> <p>A string of up to 18 characters that will be added to your fine-tuned model   name.</p> <p>For example, a <code>suffix</code> of \"custom-model-name\" would produce a model name like   <code>ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel</code>.</p> <code>NOT_GIVEN</code> <code>validation_file</code> <code>Optional[str] | NotGiven</code> <p>The ID of an uploaded file that contains validation data.</p> <p>If you provide this file, the data is used to generate validation metrics   periodically during fine-tuning. These metrics can be viewed in the fine-tuning   results file. The same data should not be present in both train and validation   files.</p> <p>Your dataset must be formatted as a JSONL file. You must upload your file with   the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide   for more details.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.list","title":"list","text":"<pre><code>list(\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[FineTuningJob]\n</code></pre> <p>List your organization's fine-tuning jobs</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last job from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of fine-tuning jobs to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.list_events","title":"list_events","text":"<pre><code>list_events(\n    fine_tuning_job_id: str,\n    *,\n    after: str | NotGiven = NOT_GIVEN,\n    limit: int | NotGiven = NOT_GIVEN,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; SyncCursorPage[FineTuningJobEvent]\n</code></pre> <p>Get status updates for a fine-tuning job.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | NotGiven</code> <p>Identifier for the last event from the previous pagination request.</p> <code>NOT_GIVEN</code> <code>limit</code> <code>int | NotGiven</code> <p>Number of events to retrieve.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    fine_tuning_job_id: str,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | Timeout | None | NotGiven = NOT_GIVEN\n) -&gt; FineTuningJob\n</code></pre> <p>Get info about a fine-tuning job.</p> <p>Learn more about fine-tuning</p> <p>Parameters:</p> Name Type Description Default <code>extra_headers</code> <code>Headers | None</code> <p>Send extra headers</p> <code>None</code> <code>extra_query</code> <code>Query | None</code> <p>Add additional query parameters to the request</p> <code>None</code> <code>extra_body</code> <code>Body | None</code> <p>Add additional JSON properties to the request</p> <code>None</code> <code>timeout</code> <code>float | Timeout | None | NotGiven</code> <p>Override the client-level default timeout for this request, in seconds</p> <code>NOT_GIVEN</code>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.with_raw_response","title":"with_raw_response","text":"<pre><code>with_raw_response() -&gt; JobsWithRawResponse\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.Jobs.with_streaming_response","title":"with_streaming_response","text":"<pre><code>with_streaming_response() -&gt; JobsWithStreamingResponse\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithRawResponse","title":"JobsWithRawResponse","text":"<pre><code>JobsWithRawResponse(jobs: Jobs)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithRawResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_raw_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithRawResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_raw_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithRawResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_raw_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithRawResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = to_raw_response_wrapper(list_events)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithRawResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_raw_response_wrapper(retrieve)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithStreamingResponse","title":"JobsWithStreamingResponse","text":"<pre><code>JobsWithStreamingResponse(jobs: Jobs)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithStreamingResponse.cancel","title":"cancel  <code>instance-attribute</code>","text":"<pre><code>cancel = to_streamed_response_wrapper(cancel)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithStreamingResponse.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create = to_streamed_response_wrapper(create)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithStreamingResponse.list","title":"list  <code>instance-attribute</code>","text":"<pre><code>list = to_streamed_response_wrapper(list)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithStreamingResponse.list_events","title":"list_events  <code>instance-attribute</code>","text":"<pre><code>list_events = to_streamed_response_wrapper(list_events)\n</code></pre>"},{"location":"resources/fine_tuning/jobs/#src.openai.resources.fine_tuning.jobs.JobsWithStreamingResponse.retrieve","title":"retrieve  <code>instance-attribute</code>","text":"<pre><code>retrieve = to_streamed_response_wrapper(retrieve)\n</code></pre>"},{"location":"types/","title":"openai.types","text":""},{"location":"types/#src.openai.types","title":"types","text":""},{"location":"types/#src.openai.types.completion_choice.CompletionChoice","title":"CompletionChoice","text":""},{"location":"types/#src.openai.types.completion_choice.CompletionChoice.finish_reason","title":"finish_reason  <code>instance-attribute</code>","text":"<pre><code>finish_reason: Literal['stop', 'length', 'content_filter']\n</code></pre> <p>The reason the model stopped generating tokens.</p> <p>This will be <code>stop</code> if the model hit a natural stop point or a provided stop sequence, <code>length</code> if the maximum number of tokens specified in the request was reached, or <code>content_filter</code> if content was omitted due to a flag from our content filters.</p>"},{"location":"types/#src.openai.types.completion_choice.CompletionChoice.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre>"},{"location":"types/#src.openai.types.completion_choice.CompletionChoice.logprobs","title":"logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logprobs: Optional[Logprobs] = None\n</code></pre>"},{"location":"types/#src.openai.types.completion_choice.CompletionChoice.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"types/#src.openai.types.completion_create_params.CompletionCreateParams","title":"CompletionCreateParams  <code>module-attribute</code>","text":"<pre><code>CompletionCreateParams = Union[\n    CompletionCreateParamsNonStreaming,\n    CompletionCreateParamsStreaming,\n]\n</code></pre>"},{"location":"types/#src.openai.types.completion_usage.CompletionUsage","title":"CompletionUsage","text":""},{"location":"types/#src.openai.types.completion_usage.CompletionUsage.completion_tokens","title":"completion_tokens  <code>instance-attribute</code>","text":"<pre><code>completion_tokens: int\n</code></pre> <p>Number of tokens in the generated completion.</p>"},{"location":"types/#src.openai.types.completion_usage.CompletionUsage.prompt_tokens","title":"prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>prompt_tokens: int\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"types/#src.openai.types.completion_usage.CompletionUsage.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Total number of tokens used in the request (prompt + completion).</p>"},{"location":"types/#src.openai.types.completion.Completion","title":"Completion","text":""},{"location":"types/#src.openai.types.completion.Completion.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: List[CompletionChoice]\n</code></pre> <p>The list of completion choices the model generated for the input prompt.</p>"},{"location":"types/#src.openai.types.completion.Completion.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre> <p>The Unix timestamp (in seconds) of when the completion was created.</p>"},{"location":"types/#src.openai.types.completion.Completion.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>A unique identifier for the completion.</p>"},{"location":"types/#src.openai.types.completion.Completion.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model used for completion.</p>"},{"location":"types/#src.openai.types.completion.Completion.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['text_completion']\n</code></pre> <p>The object type, which is always \"text_completion\"</p>"},{"location":"types/#src.openai.types.completion.Completion.system_fingerprint","title":"system_fingerprint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>system_fingerprint: Optional[str] = None\n</code></pre> <p>This fingerprint represents the backend configuration that the model runs with.</p> <p>Can be used in conjunction with the <code>seed</code> request parameter to understand when backend changes have been made that might impact determinism.</p>"},{"location":"types/#src.openai.types.completion.Completion.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Optional[CompletionUsage] = None\n</code></pre> <p>Usage statistics for the completion request.</p>"},{"location":"types/#src.openai.types.create_embedding_response.CreateEmbeddingResponse","title":"CreateEmbeddingResponse","text":""},{"location":"types/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[Embedding]\n</code></pre> <p>The list of embeddings generated by the model.</p>"},{"location":"types/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The name of the model used to generate the embedding.</p>"},{"location":"types/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['list']\n</code></pre> <p>The object type, which is always \"list\".</p>"},{"location":"types/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.usage","title":"usage  <code>instance-attribute</code>","text":"<pre><code>usage: Usage\n</code></pre> <p>The usage information for the request.</p>"},{"location":"types/#src.openai.types.embedding_create_params.EmbeddingCreateParams","title":"EmbeddingCreateParams","text":""},{"location":"types/#src.openai.types.embedding_create_params.EmbeddingCreateParams.dimensions","title":"dimensions  <code>instance-attribute</code>","text":"<pre><code>dimensions: int\n</code></pre> <p>The number of dimensions the resulting output embeddings should have.</p> <p>Only supported in <code>text-embedding-3</code> and later models.</p>"},{"location":"types/#src.openai.types.embedding_create_params.EmbeddingCreateParams.encoding_format","title":"encoding_format  <code>instance-attribute</code>","text":"<pre><code>encoding_format: Literal['float', 'base64']\n</code></pre> <p>The format to return the embeddings in.</p> <p>Can be either <code>float</code> or <code>base64</code>.</p>"},{"location":"types/#src.openai.types.embedding_create_params.EmbeddingCreateParams.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: Required[\n    Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n    ]\n]\n</code></pre> <p>Input text to embed, encoded as a string or array of tokens.</p> <p>To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for <code>text-embedding-ada-002</code>), cannot be an empty string, and any array must be 2048 dimensions or less. Example Python code for counting tokens.</p>"},{"location":"types/#src.openai.types.embedding_create_params.EmbeddingCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[\n    Union[\n        str,\n        Literal[\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-small\",\n            \"text-embedding-3-large\",\n        ],\n    ]\n]\n</code></pre> <p>ID of the model to use.</p> <p>You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.</p>"},{"location":"types/#src.openai.types.embedding_create_params.EmbeddingCreateParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/#src.openai.types.embedding.Embedding","title":"Embedding","text":""},{"location":"types/#src.openai.types.embedding.Embedding.embedding","title":"embedding  <code>instance-attribute</code>","text":"<pre><code>embedding: List[float]\n</code></pre> <p>The embedding vector, which is a list of floats.</p> <p>The length of vector depends on the model as listed in the embedding guide.</p>"},{"location":"types/#src.openai.types.embedding.Embedding.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>The index of the embedding in the list of embeddings.</p>"},{"location":"types/#src.openai.types.embedding.Embedding.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['embedding']\n</code></pre> <p>The object type, which is always \"embedding\".</p>"},{"location":"types/#src.openai.types.file_content.FileContent","title":"FileContent  <code>module-attribute</code>","text":"<pre><code>FileContent = str\n</code></pre>"},{"location":"types/#src.openai.types.file_create_params.FileCreateParams","title":"FileCreateParams","text":""},{"location":"types/#src.openai.types.file_create_params.FileCreateParams.file","title":"file  <code>instance-attribute</code>","text":"<pre><code>file: Required[FileTypes]\n</code></pre> <p>The File object (not file name) to be uploaded.</p>"},{"location":"types/#src.openai.types.file_create_params.FileCreateParams.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: Required[Literal['fine-tune', 'assistants']]\n</code></pre> <p>The intended purpose of the uploaded file.</p> <p>Use \"fine-tune\" for Fine-tuning and \"assistants\" for Assistants and Messages. This allows us to validate the format of the uploaded file is correct for fine-tuning.</p>"},{"location":"types/#src.openai.types.file_deleted.FileDeleted","title":"FileDeleted","text":""},{"location":"types/#src.openai.types.file_deleted.FileDeleted.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/#src.openai.types.file_deleted.FileDeleted.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/#src.openai.types.file_deleted.FileDeleted.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['file']\n</code></pre>"},{"location":"types/#src.openai.types.file_list_params.FileListParams","title":"FileListParams","text":""},{"location":"types/#src.openai.types.file_list_params.FileListParams.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: str\n</code></pre> <p>Only return files with the given purpose.</p>"},{"location":"types/#src.openai.types.file_object.FileObject","title":"FileObject","text":""},{"location":"types/#src.openai.types.file_object.FileObject.bytes","title":"bytes  <code>instance-attribute</code>","text":"<pre><code>bytes: int\n</code></pre> <p>The size of the file, in bytes.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the file was created.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.filename","title":"filename  <code>instance-attribute</code>","text":"<pre><code>filename: str\n</code></pre> <p>The name of the file.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The file identifier, which can be referenced in the API endpoints.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['file']\n</code></pre> <p>The object type, which is always <code>file</code>.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: Literal[\n    \"fine-tune\",\n    \"fine-tune-results\",\n    \"assistants\",\n    \"assistants_output\",\n]\n</code></pre> <p>The intended purpose of the file.</p> <p>Supported values are <code>fine-tune</code>, <code>fine-tune-results</code>, <code>assistants</code>, and <code>assistants_output</code>.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Literal['uploaded', 'processed', 'error']\n</code></pre> <p>Deprecated.</p> <p>The current status of the file, which can be either <code>uploaded</code>, <code>processed</code>, or <code>error</code>.</p>"},{"location":"types/#src.openai.types.file_object.FileObject.status_details","title":"status_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_details: Optional[str] = None\n</code></pre> <p>Deprecated.</p> <p>For details on why a fine-tuning training file failed validation, see the <code>error</code> field on <code>fine_tuning.job</code>.</p>"},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams","title":"ImageCreateVariationParams","text":""},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: Required[FileTypes]\n</code></pre> <p>The image to use as the basis for the variation(s).</p> <p>Must be a valid PNG file, less than 4MB, and square.</p>"},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[str, Literal['dall-e-2'], None]\n</code></pre> <p>The model to use for image generation.</p> <p>Only <code>dall-e-2</code> is supported at this time.</p>"},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>The number of images to generate.</p> <p>Must be between 1 and 10. For <code>dall-e-3</code>, only <code>n=1</code> is supported.</p>"},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Optional[Literal['url', 'b64_json']]\n</code></pre> <p>The format in which the generated images are returned.</p> <p>Must be one of <code>url</code> or <code>b64_json</code>.</p>"},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: Optional[Literal['256x256', '512x512', '1024x1024']]\n</code></pre> <p>The size of the generated images.</p> <p>Must be one of <code>256x256</code>, <code>512x512</code>, or <code>1024x1024</code>.</p>"},{"location":"types/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams","title":"ImageEditParams","text":""},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: Required[FileTypes]\n</code></pre> <p>The image to edit.</p> <p>Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.mask","title":"mask  <code>instance-attribute</code>","text":"<pre><code>mask: FileTypes\n</code></pre> <p>An additional image whose fully transparent areas (e.g.</p> <p>where alpha is zero) indicate where <code>image</code> should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as <code>image</code>.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[str, Literal['dall-e-2'], None]\n</code></pre> <p>The model to use for image generation.</p> <p>Only <code>dall-e-2</code> is supported at this time.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>The number of images to generate. Must be between 1 and 10.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: Required[str]\n</code></pre> <p>A text description of the desired image(s).</p> <p>The maximum length is 1000 characters.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Optional[Literal['url', 'b64_json']]\n</code></pre> <p>The format in which the generated images are returned.</p> <p>Must be one of <code>url</code> or <code>b64_json</code>.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: Optional[Literal['256x256', '512x512', '1024x1024']]\n</code></pre> <p>The size of the generated images.</p> <p>Must be one of <code>256x256</code>, <code>512x512</code>, or <code>1024x1024</code>.</p>"},{"location":"types/#src.openai.types.image_edit_params.ImageEditParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams","title":"ImageGenerateParams","text":""},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[str, Literal['dall-e-2', 'dall-e-3'], None]\n</code></pre> <p>The model to use for image generation.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>The number of images to generate.</p> <p>Must be between 1 and 10. For <code>dall-e-3</code>, only <code>n=1</code> is supported.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: Required[str]\n</code></pre> <p>A text description of the desired image(s).</p> <p>The maximum length is 1000 characters for <code>dall-e-2</code> and 4000 characters for <code>dall-e-3</code>.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.quality","title":"quality  <code>instance-attribute</code>","text":"<pre><code>quality: Literal['standard', 'hd']\n</code></pre> <p>The quality of the image that will be generated.</p> <p><code>hd</code> creates images with finer details and greater consistency across the image. This param is only supported for <code>dall-e-3</code>.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Optional[Literal['url', 'b64_json']]\n</code></pre> <p>The format in which the generated images are returned.</p> <p>Must be one of <code>url</code> or <code>b64_json</code>.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: Optional[\n    Literal[\n        \"256x256\",\n        \"512x512\",\n        \"1024x1024\",\n        \"1792x1024\",\n        \"1024x1792\",\n    ]\n]\n</code></pre> <p>The size of the generated images.</p> <p>Must be one of <code>256x256</code>, <code>512x512</code>, or <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or <code>1024x1792</code> for <code>dall-e-3</code> models.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.style","title":"style  <code>instance-attribute</code>","text":"<pre><code>style: Optional[Literal['vivid', 'natural']]\n</code></pre> <p>The style of the generated images.</p> <p>Must be one of <code>vivid</code> or <code>natural</code>. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for <code>dall-e-3</code>.</p>"},{"location":"types/#src.openai.types.image_generate_params.ImageGenerateParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/#src.openai.types.image.Image","title":"Image","text":""},{"location":"types/#src.openai.types.image.Image.b64_json","title":"b64_json  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>b64_json: Optional[str] = None\n</code></pre> <p>The base64-encoded JSON of the generated image, if <code>response_format</code> is <code>b64_json</code>.</p>"},{"location":"types/#src.openai.types.image.Image.revised_prompt","title":"revised_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>revised_prompt: Optional[str] = None\n</code></pre> <p>The prompt that was used to generate the image, if there was any revision to the prompt.</p>"},{"location":"types/#src.openai.types.image.Image.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: Optional[str] = None\n</code></pre> <p>The URL of the generated image, if <code>response_format</code> is <code>url</code> (default).</p>"},{"location":"types/#src.openai.types.images_response.ImagesResponse","title":"ImagesResponse","text":""},{"location":"types/#src.openai.types.images_response.ImagesResponse.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre>"},{"location":"types/#src.openai.types.images_response.ImagesResponse.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[Image]\n</code></pre>"},{"location":"types/#src.openai.types.model_deleted.ModelDeleted","title":"ModelDeleted","text":""},{"location":"types/#src.openai.types.model_deleted.ModelDeleted.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/#src.openai.types.model_deleted.ModelDeleted.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/#src.openai.types.model_deleted.ModelDeleted.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: str\n</code></pre>"},{"location":"types/#src.openai.types.model.Model","title":"Model","text":""},{"location":"types/#src.openai.types.model.Model.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre> <p>The Unix timestamp (in seconds) when the model was created.</p>"},{"location":"types/#src.openai.types.model.Model.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The model identifier, which can be referenced in the API endpoints.</p>"},{"location":"types/#src.openai.types.model.Model.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['model']\n</code></pre> <p>The object type, which is always \"model\".</p>"},{"location":"types/#src.openai.types.model.Model.owned_by","title":"owned_by  <code>instance-attribute</code>","text":"<pre><code>owned_by: str\n</code></pre> <p>The organization that owns the model.</p>"},{"location":"types/#src.openai.types.moderation_create_params.ModerationCreateParams","title":"ModerationCreateParams","text":""},{"location":"types/#src.openai.types.moderation_create_params.ModerationCreateParams.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: Required[Union[str, List[str]]]\n</code></pre> <p>The input text to classify</p>"},{"location":"types/#src.openai.types.moderation_create_params.ModerationCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[\n    str,\n    Literal[\n        \"text-moderation-latest\", \"text-moderation-stable\"\n    ],\n]\n</code></pre> <p>Two content moderations models are available: <code>text-moderation-stable</code> and <code>text-moderation-latest</code>.</p> <p>The default is <code>text-moderation-latest</code> which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use <code>text-moderation-stable</code>, we will provide advanced notice before updating the model. Accuracy of <code>text-moderation-stable</code> may be slightly lower than for <code>text-moderation-latest</code>.</p>"},{"location":"types/#src.openai.types.moderation_create_response.ModerationCreateResponse","title":"ModerationCreateResponse","text":""},{"location":"types/#src.openai.types.moderation_create_response.ModerationCreateResponse.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The unique identifier for the moderation request.</p>"},{"location":"types/#src.openai.types.moderation_create_response.ModerationCreateResponse.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model used to generate the moderation results.</p>"},{"location":"types/#src.openai.types.moderation_create_response.ModerationCreateResponse.results","title":"results  <code>instance-attribute</code>","text":"<pre><code>results: List[Moderation]\n</code></pre> <p>A list of moderation objects.</p>"},{"location":"types/#src.openai.types.moderation.Moderation","title":"Moderation","text":""},{"location":"types/#src.openai.types.moderation.Moderation.categories","title":"categories  <code>instance-attribute</code>","text":"<pre><code>categories: Categories\n</code></pre> <p>A list of the categories, and whether they are flagged or not.</p>"},{"location":"types/#src.openai.types.moderation.Moderation.category_scores","title":"category_scores  <code>instance-attribute</code>","text":"<pre><code>category_scores: CategoryScores\n</code></pre> <p>A list of the categories along with their scores as predicted by model.</p>"},{"location":"types/#src.openai.types.moderation.Moderation.flagged","title":"flagged  <code>instance-attribute</code>","text":"<pre><code>flagged: bool\n</code></pre> <p>Whether the content violates OpenAI's usage policies.</p>"},{"location":"types/#src.openai.types.shared.FunctionDefinition","title":"FunctionDefinition","text":""},{"location":"types/#src.openai.types.shared.FunctionDefinition.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre> <p>A description of what the function does, used by the model to choose when and how to call the function.</p>"},{"location":"types/#src.openai.types.shared.FunctionDefinition.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function to be called.</p> <p>Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.</p>"},{"location":"types/#src.openai.types.shared.FunctionDefinition.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Optional[FunctionParameters] = None\n</code></pre> <p>The parameters the functions accepts, described as a JSON Schema object.</p> <p>See the guide for examples, and the JSON Schema reference for documentation about the format.</p> <p>Omitting <code>parameters</code> defines a function with an empty parameter list.</p>"},{"location":"types/#src.openai.types.shared.FunctionParameters","title":"FunctionParameters  <code>module-attribute</code>","text":"<pre><code>FunctionParameters = Dict[str, object]\n</code></pre>"},{"location":"types/completion/","title":"Completion","text":""},{"location":"types/completion/#src.openai.types.completion","title":"completion","text":""},{"location":"types/completion/#src.openai.types.completion.Completion","title":"Completion","text":""},{"location":"types/completion/#src.openai.types.completion.Completion.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: List[CompletionChoice]\n</code></pre> <p>The list of completion choices the model generated for the input prompt.</p>"},{"location":"types/completion/#src.openai.types.completion.Completion.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre> <p>The Unix timestamp (in seconds) of when the completion was created.</p>"},{"location":"types/completion/#src.openai.types.completion.Completion.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>A unique identifier for the completion.</p>"},{"location":"types/completion/#src.openai.types.completion.Completion.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model used for completion.</p>"},{"location":"types/completion/#src.openai.types.completion.Completion.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['text_completion']\n</code></pre> <p>The object type, which is always \"text_completion\"</p>"},{"location":"types/completion/#src.openai.types.completion.Completion.system_fingerprint","title":"system_fingerprint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>system_fingerprint: Optional[str] = None\n</code></pre> <p>This fingerprint represents the backend configuration that the model runs with.</p> <p>Can be used in conjunction with the <code>seed</code> request parameter to understand when backend changes have been made that might impact determinism.</p>"},{"location":"types/completion/#src.openai.types.completion.Completion.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Optional[CompletionUsage] = None\n</code></pre> <p>Usage statistics for the completion request.</p>"},{"location":"types/completion_choice/","title":"Completion choice","text":""},{"location":"types/completion_choice/#src.openai.types.completion_choice","title":"completion_choice","text":""},{"location":"types/completion_choice/#src.openai.types.completion_choice.CompletionChoice","title":"CompletionChoice","text":""},{"location":"types/completion_choice/#src.openai.types.completion_choice.CompletionChoice.finish_reason","title":"finish_reason  <code>instance-attribute</code>","text":"<pre><code>finish_reason: Literal['stop', 'length', 'content_filter']\n</code></pre> <p>The reason the model stopped generating tokens.</p> <p>This will be <code>stop</code> if the model hit a natural stop point or a provided stop sequence, <code>length</code> if the maximum number of tokens specified in the request was reached, or <code>content_filter</code> if content was omitted due to a flag from our content filters.</p>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.CompletionChoice.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.CompletionChoice.logprobs","title":"logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logprobs: Optional[Logprobs] = None\n</code></pre>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.CompletionChoice.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.Logprobs","title":"Logprobs","text":""},{"location":"types/completion_choice/#src.openai.types.completion_choice.Logprobs.text_offset","title":"text_offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_offset: Optional[List[int]] = None\n</code></pre>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.Logprobs.token_logprobs","title":"token_logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>token_logprobs: Optional[List[float]] = None\n</code></pre>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.Logprobs.tokens","title":"tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tokens: Optional[List[str]] = None\n</code></pre>"},{"location":"types/completion_choice/#src.openai.types.completion_choice.Logprobs.top_logprobs","title":"top_logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_logprobs: Optional[List[Dict[str, float]]] = None\n</code></pre>"},{"location":"types/completion_create_params/","title":"Completion create params","text":""},{"location":"types/completion_create_params/#src.openai.types.completion_create_params","title":"completion_create_params","text":""},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParams","title":"CompletionCreateParams  <code>module-attribute</code>","text":"<pre><code>CompletionCreateParams = Union[\n    CompletionCreateParamsNonStreaming,\n    CompletionCreateParamsStreaming,\n]\n</code></pre>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase","title":"CompletionCreateParamsBase","text":""},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.best_of","title":"best_of  <code>instance-attribute</code>","text":"<pre><code>best_of: Optional[int]\n</code></pre> <p>Generates <code>best_of</code> completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.</p> <p>When used with <code>n</code>, <code>best_of</code> controls the number of candidate completions and <code>n</code> specifies how many to return \u2013 <code>best_of</code> must be greater than <code>n</code>.</p> <p>Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for <code>max_tokens</code> and <code>stop</code>.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.echo","title":"echo  <code>instance-attribute</code>","text":"<pre><code>echo: Optional[bool]\n</code></pre> <p>Echo back the prompt in addition to the completion</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.frequency_penalty","title":"frequency_penalty  <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: Optional[float]\n</code></pre> <p>Number between -2.0 and 2.0.</p> <p>Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <p>See more information about frequency and presence penalties.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.logit_bias","title":"logit_bias  <code>instance-attribute</code>","text":"<pre><code>logit_bias: Optional[Dict[str, int]]\n</code></pre> <p>Modify the likelihood of specified tokens appearing in the completion.</p> <p>Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.</p> <p>As an example, you can pass <code>{\"50256\": -100}</code> to prevent the &lt;|endoftext|&gt; token from being generated.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.logprobs","title":"logprobs  <code>instance-attribute</code>","text":"<pre><code>logprobs: Optional[int]\n</code></pre> <p>Include the log probabilities on the <code>logprobs</code> most likely output tokens, as well the chosen tokens. For example, if <code>logprobs</code> is 5, the API will return a list of the 5 most likely tokens. The API will always return the <code>logprob</code> of the sampled token, so there may be up to <code>logprobs+1</code> elements in the response.</p> <p>The maximum value for <code>logprobs</code> is 5.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens: Optional[int]\n</code></pre> <p>The maximum number of tokens that can be generated in the completion.</p> <p>The token count of your prompt plus <code>max_tokens</code> cannot exceed the model's context length. Example Python code for counting tokens.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[\n    Union[\n        str,\n        Literal[\n            \"gpt-3.5-turbo-instruct\",\n            \"davinci-002\",\n            \"babbage-002\",\n        ],\n    ]\n]\n</code></pre> <p>ID of the model to use.</p> <p>You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>How many completions to generate for each prompt.</p> <p>Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for <code>max_tokens</code> and <code>stop</code>.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.presence_penalty","title":"presence_penalty  <code>instance-attribute</code>","text":"<pre><code>presence_penalty: Optional[float]\n</code></pre> <p>Number between -2.0 and 2.0.</p> <p>Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <p>See more information about frequency and presence penalties.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: Required[\n    Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n        None,\n    ]\n]\n</code></pre> <p>The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.</p> <p>Note that &lt;|endoftext|&gt; is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed: Optional[int]\n</code></pre> <p>If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same <code>seed</code> and parameters should return the same result.</p> <p>Determinism is not guaranteed, and you should refer to the <code>system_fingerprint</code> response parameter to monitor changes in the backend.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.stop","title":"stop  <code>instance-attribute</code>","text":"<pre><code>stop: Union[Optional[str], List[str], None]\n</code></pre> <p>Up to 4 sequences where the API will stop generating further tokens.</p> <p>The returned text will not contain the stop sequence.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.suffix","title":"suffix  <code>instance-attribute</code>","text":"<pre><code>suffix: Optional[str]\n</code></pre> <p>The suffix that comes after a completion of inserted text.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature: Optional[float]\n</code></pre> <p>What sampling temperature to use, between 0 and 2.</p> <p>Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <p>We generally recommend altering this or <code>top_p</code> but not both.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.top_p","title":"top_p  <code>instance-attribute</code>","text":"<pre><code>top_p: Optional[float]\n</code></pre> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.</p> <p>We generally recommend altering this or <code>temperature</code> but not both.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsBase.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsNonStreaming","title":"CompletionCreateParamsNonStreaming","text":""},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsNonStreaming.stream","title":"stream  <code>instance-attribute</code>","text":"<pre><code>stream: Optional[Literal[False]]\n</code></pre> <p>Whether to stream back partial progress.</p> <p>If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a <code>data: [DONE]</code> message. Example Python code.</p>"},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsStreaming","title":"CompletionCreateParamsStreaming","text":""},{"location":"types/completion_create_params/#src.openai.types.completion_create_params.CompletionCreateParamsStreaming.stream","title":"stream  <code>instance-attribute</code>","text":"<pre><code>stream: Required[Literal[True]]\n</code></pre> <p>Whether to stream back partial progress.</p> <p>If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a <code>data: [DONE]</code> message. Example Python code.</p>"},{"location":"types/completion_usage/","title":"Completion usage","text":""},{"location":"types/completion_usage/#src.openai.types.completion_usage","title":"completion_usage","text":""},{"location":"types/completion_usage/#src.openai.types.completion_usage.CompletionUsage","title":"CompletionUsage","text":""},{"location":"types/completion_usage/#src.openai.types.completion_usage.CompletionUsage.completion_tokens","title":"completion_tokens  <code>instance-attribute</code>","text":"<pre><code>completion_tokens: int\n</code></pre> <p>Number of tokens in the generated completion.</p>"},{"location":"types/completion_usage/#src.openai.types.completion_usage.CompletionUsage.prompt_tokens","title":"prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>prompt_tokens: int\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"types/completion_usage/#src.openai.types.completion_usage.CompletionUsage.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Total number of tokens used in the request (prompt + completion).</p>"},{"location":"types/create_embedding_response/","title":"Create embedding response","text":""},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response","title":"create_embedding_response","text":""},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.CreateEmbeddingResponse","title":"CreateEmbeddingResponse","text":""},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[Embedding]\n</code></pre> <p>The list of embeddings generated by the model.</p>"},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The name of the model used to generate the embedding.</p>"},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['list']\n</code></pre> <p>The object type, which is always \"list\".</p>"},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.CreateEmbeddingResponse.usage","title":"usage  <code>instance-attribute</code>","text":"<pre><code>usage: Usage\n</code></pre> <p>The usage information for the request.</p>"},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.Usage","title":"Usage","text":""},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.Usage.prompt_tokens","title":"prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>prompt_tokens: int\n</code></pre> <p>The number of tokens used by the prompt.</p>"},{"location":"types/create_embedding_response/#src.openai.types.create_embedding_response.Usage.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>The total number of tokens used by the request.</p>"},{"location":"types/embedding/","title":"Embedding","text":""},{"location":"types/embedding/#src.openai.types.embedding","title":"embedding","text":""},{"location":"types/embedding/#src.openai.types.embedding.Embedding","title":"Embedding","text":""},{"location":"types/embedding/#src.openai.types.embedding.Embedding.embedding","title":"embedding  <code>instance-attribute</code>","text":"<pre><code>embedding: List[float]\n</code></pre> <p>The embedding vector, which is a list of floats.</p> <p>The length of vector depends on the model as listed in the embedding guide.</p>"},{"location":"types/embedding/#src.openai.types.embedding.Embedding.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>The index of the embedding in the list of embeddings.</p>"},{"location":"types/embedding/#src.openai.types.embedding.Embedding.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['embedding']\n</code></pre> <p>The object type, which is always \"embedding\".</p>"},{"location":"types/embedding_create_params/","title":"Embedding create params","text":""},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params","title":"embedding_create_params","text":""},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params.EmbeddingCreateParams","title":"EmbeddingCreateParams","text":""},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params.EmbeddingCreateParams.dimensions","title":"dimensions  <code>instance-attribute</code>","text":"<pre><code>dimensions: int\n</code></pre> <p>The number of dimensions the resulting output embeddings should have.</p> <p>Only supported in <code>text-embedding-3</code> and later models.</p>"},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params.EmbeddingCreateParams.encoding_format","title":"encoding_format  <code>instance-attribute</code>","text":"<pre><code>encoding_format: Literal['float', 'base64']\n</code></pre> <p>The format to return the embeddings in.</p> <p>Can be either <code>float</code> or <code>base64</code>.</p>"},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params.EmbeddingCreateParams.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: Required[\n    Union[\n        str,\n        List[str],\n        Iterable[int],\n        Iterable[Iterable[int]],\n    ]\n]\n</code></pre> <p>Input text to embed, encoded as a string or array of tokens.</p> <p>To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for <code>text-embedding-ada-002</code>), cannot be an empty string, and any array must be 2048 dimensions or less. Example Python code for counting tokens.</p>"},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params.EmbeddingCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[\n    Union[\n        str,\n        Literal[\n            \"text-embedding-ada-002\",\n            \"text-embedding-3-small\",\n            \"text-embedding-3-large\",\n        ],\n    ]\n]\n</code></pre> <p>ID of the model to use.</p> <p>You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.</p>"},{"location":"types/embedding_create_params/#src.openai.types.embedding_create_params.EmbeddingCreateParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/file_content/","title":"File content","text":""},{"location":"types/file_content/#src.openai.types.file_content","title":"file_content","text":""},{"location":"types/file_content/#src.openai.types.file_content.FileContent","title":"FileContent  <code>module-attribute</code>","text":"<pre><code>FileContent = str\n</code></pre>"},{"location":"types/file_create_params/","title":"File create params","text":""},{"location":"types/file_create_params/#src.openai.types.file_create_params","title":"file_create_params","text":""},{"location":"types/file_create_params/#src.openai.types.file_create_params.FileCreateParams","title":"FileCreateParams","text":""},{"location":"types/file_create_params/#src.openai.types.file_create_params.FileCreateParams.file","title":"file  <code>instance-attribute</code>","text":"<pre><code>file: Required[FileTypes]\n</code></pre> <p>The File object (not file name) to be uploaded.</p>"},{"location":"types/file_create_params/#src.openai.types.file_create_params.FileCreateParams.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: Required[Literal['fine-tune', 'assistants']]\n</code></pre> <p>The intended purpose of the uploaded file.</p> <p>Use \"fine-tune\" for Fine-tuning and \"assistants\" for Assistants and Messages. This allows us to validate the format of the uploaded file is correct for fine-tuning.</p>"},{"location":"types/file_deleted/","title":"File deleted","text":""},{"location":"types/file_deleted/#src.openai.types.file_deleted","title":"file_deleted","text":""},{"location":"types/file_deleted/#src.openai.types.file_deleted.FileDeleted","title":"FileDeleted","text":""},{"location":"types/file_deleted/#src.openai.types.file_deleted.FileDeleted.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/file_deleted/#src.openai.types.file_deleted.FileDeleted.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/file_deleted/#src.openai.types.file_deleted.FileDeleted.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['file']\n</code></pre>"},{"location":"types/file_list_params/","title":"File list params","text":""},{"location":"types/file_list_params/#src.openai.types.file_list_params","title":"file_list_params","text":""},{"location":"types/file_list_params/#src.openai.types.file_list_params.FileListParams","title":"FileListParams","text":""},{"location":"types/file_list_params/#src.openai.types.file_list_params.FileListParams.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: str\n</code></pre> <p>Only return files with the given purpose.</p>"},{"location":"types/file_object/","title":"File object","text":""},{"location":"types/file_object/#src.openai.types.file_object","title":"file_object","text":""},{"location":"types/file_object/#src.openai.types.file_object.FileObject","title":"FileObject","text":""},{"location":"types/file_object/#src.openai.types.file_object.FileObject.bytes","title":"bytes  <code>instance-attribute</code>","text":"<pre><code>bytes: int\n</code></pre> <p>The size of the file, in bytes.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the file was created.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.filename","title":"filename  <code>instance-attribute</code>","text":"<pre><code>filename: str\n</code></pre> <p>The name of the file.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The file identifier, which can be referenced in the API endpoints.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['file']\n</code></pre> <p>The object type, which is always <code>file</code>.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: Literal[\n    \"fine-tune\",\n    \"fine-tune-results\",\n    \"assistants\",\n    \"assistants_output\",\n]\n</code></pre> <p>The intended purpose of the file.</p> <p>Supported values are <code>fine-tune</code>, <code>fine-tune-results</code>, <code>assistants</code>, and <code>assistants_output</code>.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Literal['uploaded', 'processed', 'error']\n</code></pre> <p>Deprecated.</p> <p>The current status of the file, which can be either <code>uploaded</code>, <code>processed</code>, or <code>error</code>.</p>"},{"location":"types/file_object/#src.openai.types.file_object.FileObject.status_details","title":"status_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status_details: Optional[str] = None\n</code></pre> <p>Deprecated.</p> <p>For details on why a fine-tuning training file failed validation, see the <code>error</code> field on <code>fine_tuning.job</code>.</p>"},{"location":"types/image/","title":"Image","text":""},{"location":"types/image/#src.openai.types.image","title":"image","text":""},{"location":"types/image/#src.openai.types.image.Image","title":"Image","text":""},{"location":"types/image/#src.openai.types.image.Image.b64_json","title":"b64_json  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>b64_json: Optional[str] = None\n</code></pre> <p>The base64-encoded JSON of the generated image, if <code>response_format</code> is <code>b64_json</code>.</p>"},{"location":"types/image/#src.openai.types.image.Image.revised_prompt","title":"revised_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>revised_prompt: Optional[str] = None\n</code></pre> <p>The prompt that was used to generate the image, if there was any revision to the prompt.</p>"},{"location":"types/image/#src.openai.types.image.Image.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: Optional[str] = None\n</code></pre> <p>The URL of the generated image, if <code>response_format</code> is <code>url</code> (default).</p>"},{"location":"types/image_create_variation_params/","title":"Image create variation params","text":""},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params","title":"image_create_variation_params","text":""},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams","title":"ImageCreateVariationParams","text":""},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: Required[FileTypes]\n</code></pre> <p>The image to use as the basis for the variation(s).</p> <p>Must be a valid PNG file, less than 4MB, and square.</p>"},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[str, Literal['dall-e-2'], None]\n</code></pre> <p>The model to use for image generation.</p> <p>Only <code>dall-e-2</code> is supported at this time.</p>"},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>The number of images to generate.</p> <p>Must be between 1 and 10. For <code>dall-e-3</code>, only <code>n=1</code> is supported.</p>"},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Optional[Literal['url', 'b64_json']]\n</code></pre> <p>The format in which the generated images are returned.</p> <p>Must be one of <code>url</code> or <code>b64_json</code>.</p>"},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: Optional[Literal['256x256', '512x512', '1024x1024']]\n</code></pre> <p>The size of the generated images.</p> <p>Must be one of <code>256x256</code>, <code>512x512</code>, or <code>1024x1024</code>.</p>"},{"location":"types/image_create_variation_params/#src.openai.types.image_create_variation_params.ImageCreateVariationParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/image_edit_params/","title":"Image edit params","text":""},{"location":"types/image_edit_params/#src.openai.types.image_edit_params","title":"image_edit_params","text":""},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams","title":"ImageEditParams","text":""},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: Required[FileTypes]\n</code></pre> <p>The image to edit.</p> <p>Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.mask","title":"mask  <code>instance-attribute</code>","text":"<pre><code>mask: FileTypes\n</code></pre> <p>An additional image whose fully transparent areas (e.g.</p> <p>where alpha is zero) indicate where <code>image</code> should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as <code>image</code>.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[str, Literal['dall-e-2'], None]\n</code></pre> <p>The model to use for image generation.</p> <p>Only <code>dall-e-2</code> is supported at this time.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>The number of images to generate. Must be between 1 and 10.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: Required[str]\n</code></pre> <p>A text description of the desired image(s).</p> <p>The maximum length is 1000 characters.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Optional[Literal['url', 'b64_json']]\n</code></pre> <p>The format in which the generated images are returned.</p> <p>Must be one of <code>url</code> or <code>b64_json</code>.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: Optional[Literal['256x256', '512x512', '1024x1024']]\n</code></pre> <p>The size of the generated images.</p> <p>Must be one of <code>256x256</code>, <code>512x512</code>, or <code>1024x1024</code>.</p>"},{"location":"types/image_edit_params/#src.openai.types.image_edit_params.ImageEditParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/image_generate_params/","title":"Image generate params","text":""},{"location":"types/image_generate_params/#src.openai.types.image_generate_params","title":"image_generate_params","text":""},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams","title":"ImageGenerateParams","text":""},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[str, Literal['dall-e-2', 'dall-e-3'], None]\n</code></pre> <p>The model to use for image generation.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>The number of images to generate.</p> <p>Must be between 1 and 10. For <code>dall-e-3</code>, only <code>n=1</code> is supported.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: Required[str]\n</code></pre> <p>A text description of the desired image(s).</p> <p>The maximum length is 1000 characters for <code>dall-e-2</code> and 4000 characters for <code>dall-e-3</code>.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.quality","title":"quality  <code>instance-attribute</code>","text":"<pre><code>quality: Literal['standard', 'hd']\n</code></pre> <p>The quality of the image that will be generated.</p> <p><code>hd</code> creates images with finer details and greater consistency across the image. This param is only supported for <code>dall-e-3</code>.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Optional[Literal['url', 'b64_json']]\n</code></pre> <p>The format in which the generated images are returned.</p> <p>Must be one of <code>url</code> or <code>b64_json</code>.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: Optional[\n    Literal[\n        \"256x256\",\n        \"512x512\",\n        \"1024x1024\",\n        \"1792x1024\",\n        \"1024x1792\",\n    ]\n]\n</code></pre> <p>The size of the generated images.</p> <p>Must be one of <code>256x256</code>, <code>512x512</code>, or <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or <code>1024x1792</code> for <code>dall-e-3</code> models.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.style","title":"style  <code>instance-attribute</code>","text":"<pre><code>style: Optional[Literal['vivid', 'natural']]\n</code></pre> <p>The style of the generated images.</p> <p>Must be one of <code>vivid</code> or <code>natural</code>. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for <code>dall-e-3</code>.</p>"},{"location":"types/image_generate_params/#src.openai.types.image_generate_params.ImageGenerateParams.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/images_response/","title":"Images response","text":""},{"location":"types/images_response/#src.openai.types.images_response","title":"images_response","text":""},{"location":"types/images_response/#src.openai.types.images_response.ImagesResponse","title":"ImagesResponse","text":""},{"location":"types/images_response/#src.openai.types.images_response.ImagesResponse.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre>"},{"location":"types/images_response/#src.openai.types.images_response.ImagesResponse.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: List[Image]\n</code></pre>"},{"location":"types/model/","title":"Model","text":""},{"location":"types/model/#src.openai.types.model","title":"model","text":""},{"location":"types/model/#src.openai.types.model.Model","title":"Model","text":""},{"location":"types/model/#src.openai.types.model.Model.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre> <p>The Unix timestamp (in seconds) when the model was created.</p>"},{"location":"types/model/#src.openai.types.model.Model.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The model identifier, which can be referenced in the API endpoints.</p>"},{"location":"types/model/#src.openai.types.model.Model.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['model']\n</code></pre> <p>The object type, which is always \"model\".</p>"},{"location":"types/model/#src.openai.types.model.Model.owned_by","title":"owned_by  <code>instance-attribute</code>","text":"<pre><code>owned_by: str\n</code></pre> <p>The organization that owns the model.</p>"},{"location":"types/model_deleted/","title":"Model deleted","text":""},{"location":"types/model_deleted/#src.openai.types.model_deleted","title":"model_deleted","text":""},{"location":"types/model_deleted/#src.openai.types.model_deleted.ModelDeleted","title":"ModelDeleted","text":""},{"location":"types/model_deleted/#src.openai.types.model_deleted.ModelDeleted.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/model_deleted/#src.openai.types.model_deleted.ModelDeleted.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/model_deleted/#src.openai.types.model_deleted.ModelDeleted.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: str\n</code></pre>"},{"location":"types/moderation/","title":"Moderation","text":""},{"location":"types/moderation/#src.openai.types.moderation","title":"moderation","text":""},{"location":"types/moderation/#src.openai.types.moderation.Categories","title":"Categories","text":""},{"location":"types/moderation/#src.openai.types.moderation.Categories.harassment","title":"harassment  <code>instance-attribute</code>","text":"<pre><code>harassment: bool\n</code></pre> <p>Content that expresses, incites, or promotes harassing language towards any target.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.harassment_threatening","title":"harassment_threatening  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>harassment_threatening: bool = Field(\n    alias=\"harassment/threatening\"\n)\n</code></pre> <p>Harassment content that also includes violence or serious harm towards any target.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.hate","title":"hate  <code>instance-attribute</code>","text":"<pre><code>hate: bool\n</code></pre> <p>Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harassment.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.hate_threatening","title":"hate_threatening  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hate_threatening: bool = Field(alias='hate/threatening')\n</code></pre> <p>Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.self_harm","title":"self_harm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>self_harm: bool = Field(alias='self-harm')\n</code></pre> <p>Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.self_harm_instructions","title":"self_harm_instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>self_harm_instructions: bool = Field(\n    alias=\"self-harm/instructions\"\n)\n</code></pre> <p>Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.self_harm_intent","title":"self_harm_intent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>self_harm_intent: bool = Field(alias='self-harm/intent')\n</code></pre> <p>Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.sexual","title":"sexual  <code>instance-attribute</code>","text":"<pre><code>sexual: bool\n</code></pre> <p>Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.sexual_minors","title":"sexual_minors  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sexual_minors: bool = Field(alias='sexual/minors')\n</code></pre> <p>Sexual content that includes an individual who is under 18 years old.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.violence","title":"violence  <code>instance-attribute</code>","text":"<pre><code>violence: bool\n</code></pre> <p>Content that depicts death, violence, or physical injury.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Categories.violence_graphic","title":"violence_graphic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>violence_graphic: bool = Field(alias='violence/graphic')\n</code></pre> <p>Content that depicts death, violence, or physical injury in graphic detail.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores","title":"CategoryScores","text":""},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.harassment","title":"harassment  <code>instance-attribute</code>","text":"<pre><code>harassment: float\n</code></pre> <p>The score for the category 'harassment'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.harassment_threatening","title":"harassment_threatening  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>harassment_threatening: float = Field(\n    alias=\"harassment/threatening\"\n)\n</code></pre> <p>The score for the category 'harassment/threatening'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.hate","title":"hate  <code>instance-attribute</code>","text":"<pre><code>hate: float\n</code></pre> <p>The score for the category 'hate'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.hate_threatening","title":"hate_threatening  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hate_threatening: float = Field(alias='hate/threatening')\n</code></pre> <p>The score for the category 'hate/threatening'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.self_harm","title":"self_harm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>self_harm: float = Field(alias='self-harm')\n</code></pre> <p>The score for the category 'self-harm'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.self_harm_instructions","title":"self_harm_instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>self_harm_instructions: float = Field(\n    alias=\"self-harm/instructions\"\n)\n</code></pre> <p>The score for the category 'self-harm/instructions'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.self_harm_intent","title":"self_harm_intent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>self_harm_intent: float = Field(alias='self-harm/intent')\n</code></pre> <p>The score for the category 'self-harm/intent'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.sexual","title":"sexual  <code>instance-attribute</code>","text":"<pre><code>sexual: float\n</code></pre> <p>The score for the category 'sexual'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.sexual_minors","title":"sexual_minors  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sexual_minors: float = Field(alias='sexual/minors')\n</code></pre> <p>The score for the category 'sexual/minors'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.violence","title":"violence  <code>instance-attribute</code>","text":"<pre><code>violence: float\n</code></pre> <p>The score for the category 'violence'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.CategoryScores.violence_graphic","title":"violence_graphic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>violence_graphic: float = Field(alias='violence/graphic')\n</code></pre> <p>The score for the category 'violence/graphic'.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Moderation","title":"Moderation","text":""},{"location":"types/moderation/#src.openai.types.moderation.Moderation.categories","title":"categories  <code>instance-attribute</code>","text":"<pre><code>categories: Categories\n</code></pre> <p>A list of the categories, and whether they are flagged or not.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Moderation.category_scores","title":"category_scores  <code>instance-attribute</code>","text":"<pre><code>category_scores: CategoryScores\n</code></pre> <p>A list of the categories along with their scores as predicted by model.</p>"},{"location":"types/moderation/#src.openai.types.moderation.Moderation.flagged","title":"flagged  <code>instance-attribute</code>","text":"<pre><code>flagged: bool\n</code></pre> <p>Whether the content violates OpenAI's usage policies.</p>"},{"location":"types/moderation_create_params/","title":"Moderation create params","text":""},{"location":"types/moderation_create_params/#src.openai.types.moderation_create_params","title":"moderation_create_params","text":""},{"location":"types/moderation_create_params/#src.openai.types.moderation_create_params.ModerationCreateParams","title":"ModerationCreateParams","text":""},{"location":"types/moderation_create_params/#src.openai.types.moderation_create_params.ModerationCreateParams.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: Required[Union[str, List[str]]]\n</code></pre> <p>The input text to classify</p>"},{"location":"types/moderation_create_params/#src.openai.types.moderation_create_params.ModerationCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Union[\n    str,\n    Literal[\n        \"text-moderation-latest\", \"text-moderation-stable\"\n    ],\n]\n</code></pre> <p>Two content moderations models are available: <code>text-moderation-stable</code> and <code>text-moderation-latest</code>.</p> <p>The default is <code>text-moderation-latest</code> which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use <code>text-moderation-stable</code>, we will provide advanced notice before updating the model. Accuracy of <code>text-moderation-stable</code> may be slightly lower than for <code>text-moderation-latest</code>.</p>"},{"location":"types/moderation_create_response/","title":"Moderation create response","text":""},{"location":"types/moderation_create_response/#src.openai.types.moderation_create_response","title":"moderation_create_response","text":""},{"location":"types/moderation_create_response/#src.openai.types.moderation_create_response.ModerationCreateResponse","title":"ModerationCreateResponse","text":""},{"location":"types/moderation_create_response/#src.openai.types.moderation_create_response.ModerationCreateResponse.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The unique identifier for the moderation request.</p>"},{"location":"types/moderation_create_response/#src.openai.types.moderation_create_response.ModerationCreateResponse.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model used to generate the moderation results.</p>"},{"location":"types/moderation_create_response/#src.openai.types.moderation_create_response.ModerationCreateResponse.results","title":"results  <code>instance-attribute</code>","text":"<pre><code>results: List[Moderation]\n</code></pre> <p>A list of moderation objects.</p>"},{"location":"types/audio/__init__/","title":"openai.types.audio","text":""},{"location":"types/audio/__init__/#src.openai.types.audio","title":"audio","text":""},{"location":"types/audio/speech_create_params/","title":"Speech create params","text":""},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params","title":"speech_create_params","text":""},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params.SpeechCreateParams","title":"SpeechCreateParams","text":""},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params.SpeechCreateParams.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: Required[str]\n</code></pre> <p>The text to generate audio for. The maximum length is 4096 characters.</p>"},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params.SpeechCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[Union[str, Literal['tts-1', 'tts-1-hd']]]\n</code></pre> <p>One of the available TTS models: <code>tts-1</code> or <code>tts-1-hd</code></p>"},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params.SpeechCreateParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Literal['mp3', 'opus', 'aac', 'flac']\n</code></pre> <p>The format to audio in. Supported formats are <code>mp3</code>, <code>opus</code>, <code>aac</code>, and <code>flac</code>.</p>"},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params.SpeechCreateParams.speed","title":"speed  <code>instance-attribute</code>","text":"<pre><code>speed: float\n</code></pre> <p>The speed of the generated audio.</p> <p>Select a value from <code>0.25</code> to <code>4.0</code>. <code>1.0</code> is the default.</p>"},{"location":"types/audio/speech_create_params/#src.openai.types.audio.speech_create_params.SpeechCreateParams.voice","title":"voice  <code>instance-attribute</code>","text":"<pre><code>voice: Required[\n    Literal[\n        \"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"\n    ]\n]\n</code></pre> <p>The voice to use when generating the audio.</p> <p>Supported voices are <code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, and <code>shimmer</code>. Previews of the voices are available in the Text to speech guide.</p>"},{"location":"types/audio/transcription/","title":"Transcription","text":""},{"location":"types/audio/transcription/#src.openai.types.audio.transcription","title":"transcription","text":""},{"location":"types/audio/transcription/#src.openai.types.audio.transcription.Transcription","title":"Transcription","text":""},{"location":"types/audio/transcription/#src.openai.types.audio.transcription.Transcription.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"types/audio/transcription_create_params/","title":"Transcription create params","text":""},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params","title":"transcription_create_params","text":""},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams","title":"TranscriptionCreateParams","text":""},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.file","title":"file  <code>instance-attribute</code>","text":"<pre><code>file: Required[FileTypes]\n</code></pre> <p>The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p>"},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.language","title":"language  <code>instance-attribute</code>","text":"<pre><code>language: str\n</code></pre> <p>The language of the input audio.</p> <p>Supplying the input language in ISO-639-1 format will improve accuracy and latency.</p>"},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[Union[str, Literal['whisper-1']]]\n</code></pre> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p>"},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: str\n</code></pre> <p>An optional text to guide the model's style or continue a previous audio segment.</p> <p>The prompt should match the audio language.</p>"},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: Literal[\n    \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n]\n</code></pre> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>, <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p>"},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature: float\n</code></pre> <p>The sampling temperature, between 0 and 1.</p> <p>Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.</p>"},{"location":"types/audio/transcription_create_params/#src.openai.types.audio.transcription_create_params.TranscriptionCreateParams.timestamp_granularities","title":"timestamp_granularities  <code>instance-attribute</code>","text":"<pre><code>timestamp_granularities: List[Literal['word', 'segment']]\n</code></pre> <p>The timestamp granularities to populate for this transcription.</p> <p>Any of these options: <code>word</code>, or <code>segment</code>. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.</p>"},{"location":"types/audio/translation/","title":"Translation","text":""},{"location":"types/audio/translation/#src.openai.types.audio.translation","title":"translation","text":""},{"location":"types/audio/translation/#src.openai.types.audio.translation.Translation","title":"Translation","text":""},{"location":"types/audio/translation/#src.openai.types.audio.translation.Translation.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"types/audio/translation_create_params/","title":"Translation create params","text":""},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params","title":"translation_create_params","text":""},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params.TranslationCreateParams","title":"TranslationCreateParams","text":""},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params.TranslationCreateParams.file","title":"file  <code>instance-attribute</code>","text":"<pre><code>file: Required[FileTypes]\n</code></pre> <p>The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p>"},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params.TranslationCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[Union[str, Literal['whisper-1']]]\n</code></pre> <p>ID of the model to use. Only <code>whisper-1</code> is currently available.</p>"},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params.TranslationCreateParams.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: str\n</code></pre> <p>An optional text to guide the model's style or continue a previous audio segment.</p> <p>The prompt should be in English.</p>"},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params.TranslationCreateParams.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: str\n</code></pre> <p>The format of the transcript output, in one of these options: <code>json</code>, <code>text</code>, <code>srt</code>, <code>verbose_json</code>, or <code>vtt</code>.</p>"},{"location":"types/audio/translation_create_params/#src.openai.types.audio.translation_create_params.TranslationCreateParams.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature: float\n</code></pre> <p>The sampling temperature, between 0 and 1.</p> <p>Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.</p>"},{"location":"types/beta/__init__/","title":"openai.types.beta","text":""},{"location":"types/beta/__init__/#src.openai.types.beta","title":"beta","text":""},{"location":"types/beta/assistant/","title":"Assistant","text":""},{"location":"types/beta/assistant/#src.openai.types.beta.assistant","title":"assistant","text":""},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    ToolCodeInterpreter, ToolRetrieval, ToolFunction\n]\n</code></pre>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant","title":"Assistant","text":""},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the assistant was created.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre> <p>The description of the assistant. The maximum length is 512 characters.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of file IDs attached to this assistant. There can be a maximum of 20 files attached to the assistant. Files are ordered by their creation date in ascending order.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier, which can be referenced in API endpoints.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: Optional[str] = None\n</code></pre> <p>The system instructions that the assistant uses.</p> <p>The maximum length is 32768 characters.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object] = None\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>ID of the model to use.</p> <p>You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: Optional[str] = None\n</code></pre> <p>The name of the assistant. The maximum length is 256 characters.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['assistant']\n</code></pre> <p>The object type, which is always <code>assistant</code>.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.Assistant.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: List[Tool]\n</code></pre> <p>A list of tool enabled on the assistant.</p> <p>There can be a maximum of 128 tools per assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolCodeInterpreter","title":"ToolCodeInterpreter","text":""},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolCodeInterpreter.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['code_interpreter']\n</code></pre> <p>The type of tool being defined: <code>code_interpreter</code></p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolFunction","title":"ToolFunction","text":""},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolFunction.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: FunctionDefinition\n</code></pre>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolFunction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['function']\n</code></pre> <p>The type of tool being defined: <code>function</code></p>"},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolRetrieval","title":"ToolRetrieval","text":""},{"location":"types/beta/assistant/#src.openai.types.beta.assistant.ToolRetrieval.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['retrieval']\n</code></pre> <p>The type of tool being defined: <code>retrieval</code></p>"},{"location":"types/beta/assistant_create_params/","title":"Assistant create params","text":""},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params","title":"assistant_create_params","text":""},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    ToolAssistantToolsCode,\n    ToolAssistantToolsRetrieval,\n    ToolAssistantToolsFunction,\n]\n</code></pre>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams","title":"AssistantCreateParams","text":""},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: Optional[str]\n</code></pre> <p>The description of the assistant. The maximum length is 512 characters.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of file IDs attached to this assistant. There can be a maximum of 20 files attached to the assistant. Files are ordered by their creation date in ascending order.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: Optional[str]\n</code></pre> <p>The system instructions that the assistant uses.</p> <p>The maximum length is 32768 characters.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[str]\n</code></pre> <p>ID of the model to use.</p> <p>You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Optional[str]\n</code></pre> <p>The name of the assistant. The maximum length is 256 characters.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.AssistantCreateParams.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: Iterable[Tool]\n</code></pre> <p>A list of tool enabled on the assistant.</p> <p>There can be a maximum of 128 tools per assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsCode","title":"ToolAssistantToolsCode","text":""},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsCode.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['code_interpreter']]\n</code></pre> <p>The type of tool being defined: <code>code_interpreter</code></p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsFunction","title":"ToolAssistantToolsFunction","text":""},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsFunction.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[FunctionDefinition]\n</code></pre>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsFunction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of tool being defined: <code>function</code></p>"},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsRetrieval","title":"ToolAssistantToolsRetrieval","text":""},{"location":"types/beta/assistant_create_params/#src.openai.types.beta.assistant_create_params.ToolAssistantToolsRetrieval.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['retrieval']]\n</code></pre> <p>The type of tool being defined: <code>retrieval</code></p>"},{"location":"types/beta/assistant_deleted/","title":"Assistant deleted","text":""},{"location":"types/beta/assistant_deleted/#src.openai.types.beta.assistant_deleted","title":"assistant_deleted","text":""},{"location":"types/beta/assistant_deleted/#src.openai.types.beta.assistant_deleted.AssistantDeleted","title":"AssistantDeleted","text":""},{"location":"types/beta/assistant_deleted/#src.openai.types.beta.assistant_deleted.AssistantDeleted.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/beta/assistant_deleted/#src.openai.types.beta.assistant_deleted.AssistantDeleted.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/beta/assistant_deleted/#src.openai.types.beta.assistant_deleted.AssistantDeleted.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['assistant.deleted']\n</code></pre>"},{"location":"types/beta/assistant_list_params/","title":"Assistant list params","text":""},{"location":"types/beta/assistant_list_params/#src.openai.types.beta.assistant_list_params","title":"assistant_list_params","text":""},{"location":"types/beta/assistant_list_params/#src.openai.types.beta.assistant_list_params.AssistantListParams","title":"AssistantListParams","text":""},{"location":"types/beta/assistant_list_params/#src.openai.types.beta.assistant_list_params.AssistantListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>after</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.</p>"},{"location":"types/beta/assistant_list_params/#src.openai.types.beta.assistant_list_params.AssistantListParams.before","title":"before  <code>instance-attribute</code>","text":"<pre><code>before: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>before</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.</p>"},{"location":"types/beta/assistant_list_params/#src.openai.types.beta.assistant_list_params.AssistantListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>A limit on the number of objects to be returned.</p> <p>Limit can range between 1 and 100, and the default is 20.</p>"},{"location":"types/beta/assistant_list_params/#src.openai.types.beta.assistant_list_params.AssistantListParams.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order: Literal['asc', 'desc']\n</code></pre> <p>Sort order by the <code>created_at</code> timestamp of the objects.</p> <p><code>asc</code> for ascending order and <code>desc</code> for descending order.</p>"},{"location":"types/beta/assistant_update_params/","title":"Assistant update params","text":""},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params","title":"assistant_update_params","text":""},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    ToolAssistantToolsCode,\n    ToolAssistantToolsRetrieval,\n    ToolAssistantToolsFunction,\n]\n</code></pre>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams","title":"AssistantUpdateParams","text":""},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: Optional[str]\n</code></pre> <p>The description of the assistant. The maximum length is 512 characters.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of File IDs attached to this assistant. There can be a maximum of 20 files attached to the assistant. Files are ordered by their creation date in ascending order. If a file was previously attached to the list but does not show up in the list, it will be deleted from the assistant.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: Optional[str]\n</code></pre> <p>The system instructions that the assistant uses.</p> <p>The maximum length is 32768 characters.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>ID of the model to use.</p> <p>You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Optional[str]\n</code></pre> <p>The name of the assistant. The maximum length is 256 characters.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.AssistantUpdateParams.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: Iterable[Tool]\n</code></pre> <p>A list of tool enabled on the assistant.</p> <p>There can be a maximum of 128 tools per assistant. Tools can be of types <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsCode","title":"ToolAssistantToolsCode","text":""},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsCode.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['code_interpreter']]\n</code></pre> <p>The type of tool being defined: <code>code_interpreter</code></p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsFunction","title":"ToolAssistantToolsFunction","text":""},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsFunction.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[FunctionDefinition]\n</code></pre>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsFunction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of tool being defined: <code>function</code></p>"},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsRetrieval","title":"ToolAssistantToolsRetrieval","text":""},{"location":"types/beta/assistant_update_params/#src.openai.types.beta.assistant_update_params.ToolAssistantToolsRetrieval.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['retrieval']]\n</code></pre> <p>The type of tool being defined: <code>retrieval</code></p>"},{"location":"types/beta/thread/","title":"Thread","text":""},{"location":"types/beta/thread/#src.openai.types.beta.thread","title":"thread","text":""},{"location":"types/beta/thread/#src.openai.types.beta.thread.Thread","title":"Thread","text":""},{"location":"types/beta/thread/#src.openai.types.beta.thread.Thread.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the thread was created.</p>"},{"location":"types/beta/thread/#src.openai.types.beta.thread.Thread.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier, which can be referenced in API endpoints.</p>"},{"location":"types/beta/thread/#src.openai.types.beta.thread.Thread.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object] = None\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/thread/#src.openai.types.beta.thread.Thread.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['thread']\n</code></pre> <p>The object type, which is always <code>thread</code>.</p>"},{"location":"types/beta/thread_create_and_run_params/","title":"Thread create and run params","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params","title":"thread_create_and_run_params","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    ToolAssistantToolsCode,\n    ToolAssistantToolsRetrieval,\n    ToolAssistantToolsFunction,\n]\n</code></pre>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.Thread","title":"Thread","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.Thread.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: Iterable[ThreadMessage]\n</code></pre> <p>A list of messages to start the thread with.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.Thread.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams","title":"ThreadCreateAndRunParams","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams.assistant_id","title":"assistant_id  <code>instance-attribute</code>","text":"<pre><code>assistant_id: Required[str]\n</code></pre> <p>The ID of the assistant to use to execute this run.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: Optional[str]\n</code></pre> <p>Override the default system message of the assistant.</p> <p>This is useful for modifying the behavior on a per-run basis.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Optional[str]\n</code></pre> <p>The ID of the Model to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams.thread","title":"thread  <code>instance-attribute</code>","text":"<pre><code>thread: Thread\n</code></pre> <p>If no thread is provided, an empty thread will be created.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadCreateAndRunParams.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: Optional[Iterable[Tool]]\n</code></pre> <p>Override the tools the assistant can use for this run.</p> <p>This is useful for modifying the behavior on a per-run basis.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadMessage","title":"ThreadMessage","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The content of the message.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadMessage.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of File IDs that the message should use. There can be a maximum of 10 files attached to a message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can access and use files.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadMessage.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ThreadMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the entity that is creating the message.</p> <p>Currently only <code>user</code> is supported.</p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsCode","title":"ToolAssistantToolsCode","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsCode.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['code_interpreter']]\n</code></pre> <p>The type of tool being defined: <code>code_interpreter</code></p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsFunction","title":"ToolAssistantToolsFunction","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsFunction.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[FunctionDefinition]\n</code></pre>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsFunction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of tool being defined: <code>function</code></p>"},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsRetrieval","title":"ToolAssistantToolsRetrieval","text":""},{"location":"types/beta/thread_create_and_run_params/#src.openai.types.beta.thread_create_and_run_params.ToolAssistantToolsRetrieval.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['retrieval']]\n</code></pre> <p>The type of tool being defined: <code>retrieval</code></p>"},{"location":"types/beta/thread_create_params/","title":"Thread create params","text":""},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params","title":"thread_create_params","text":""},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.Message","title":"Message","text":""},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.Message.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The content of the message.</p>"},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.Message.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of File IDs that the message should use. There can be a maximum of 10 files attached to a message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can access and use files.</p>"},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.Message.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.Message.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the entity that is creating the message.</p> <p>Currently only <code>user</code> is supported.</p>"},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.ThreadCreateParams","title":"ThreadCreateParams","text":""},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.ThreadCreateParams.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: Iterable[Message]\n</code></pre> <p>A list of messages to start the thread with.</p>"},{"location":"types/beta/thread_create_params/#src.openai.types.beta.thread_create_params.ThreadCreateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/thread_deleted/","title":"Thread deleted","text":""},{"location":"types/beta/thread_deleted/#src.openai.types.beta.thread_deleted","title":"thread_deleted","text":""},{"location":"types/beta/thread_deleted/#src.openai.types.beta.thread_deleted.ThreadDeleted","title":"ThreadDeleted","text":""},{"location":"types/beta/thread_deleted/#src.openai.types.beta.thread_deleted.ThreadDeleted.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/beta/thread_deleted/#src.openai.types.beta.thread_deleted.ThreadDeleted.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/beta/thread_deleted/#src.openai.types.beta.thread_deleted.ThreadDeleted.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['thread.deleted']\n</code></pre>"},{"location":"types/beta/thread_update_params/","title":"Thread update params","text":""},{"location":"types/beta/thread_update_params/#src.openai.types.beta.thread_update_params","title":"thread_update_params","text":""},{"location":"types/beta/thread_update_params/#src.openai.types.beta.thread_update_params.ThreadUpdateParams","title":"ThreadUpdateParams","text":""},{"location":"types/beta/thread_update_params/#src.openai.types.beta.thread_update_params.ThreadUpdateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/assistants/__init__/","title":"openai.types.beta.assistants","text":""},{"location":"types/beta/assistants/__init__/#src.openai.types.beta.assistants","title":"assistants","text":""},{"location":"types/beta/assistants/assistant_file/","title":"Assistant file","text":""},{"location":"types/beta/assistants/assistant_file/#src.openai.types.beta.assistants.assistant_file","title":"assistant_file","text":""},{"location":"types/beta/assistants/assistant_file/#src.openai.types.beta.assistants.assistant_file.AssistantFile","title":"AssistantFile","text":""},{"location":"types/beta/assistants/assistant_file/#src.openai.types.beta.assistants.assistant_file.AssistantFile.assistant_id","title":"assistant_id  <code>instance-attribute</code>","text":"<pre><code>assistant_id: str\n</code></pre> <p>The assistant ID that the file is attached to.</p>"},{"location":"types/beta/assistants/assistant_file/#src.openai.types.beta.assistants.assistant_file.AssistantFile.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the assistant file was created.</p>"},{"location":"types/beta/assistants/assistant_file/#src.openai.types.beta.assistants.assistant_file.AssistantFile.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier, which can be referenced in API endpoints.</p>"},{"location":"types/beta/assistants/assistant_file/#src.openai.types.beta.assistants.assistant_file.AssistantFile.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['assistant.file']\n</code></pre> <p>The object type, which is always <code>assistant.file</code>.</p>"},{"location":"types/beta/assistants/file_create_params/","title":"File create params","text":""},{"location":"types/beta/assistants/file_create_params/#src.openai.types.beta.assistants.file_create_params","title":"file_create_params","text":""},{"location":"types/beta/assistants/file_create_params/#src.openai.types.beta.assistants.file_create_params.FileCreateParams","title":"FileCreateParams","text":""},{"location":"types/beta/assistants/file_create_params/#src.openai.types.beta.assistants.file_create_params.FileCreateParams.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: Required[str]\n</code></pre> <p>A File ID (with <code>purpose=\"assistants\"</code>) that the assistant should use. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can access files.</p>"},{"location":"types/beta/assistants/file_delete_response/","title":"File delete response","text":""},{"location":"types/beta/assistants/file_delete_response/#src.openai.types.beta.assistants.file_delete_response","title":"file_delete_response","text":""},{"location":"types/beta/assistants/file_delete_response/#src.openai.types.beta.assistants.file_delete_response.FileDeleteResponse","title":"FileDeleteResponse","text":""},{"location":"types/beta/assistants/file_delete_response/#src.openai.types.beta.assistants.file_delete_response.FileDeleteResponse.deleted","title":"deleted  <code>instance-attribute</code>","text":"<pre><code>deleted: bool\n</code></pre>"},{"location":"types/beta/assistants/file_delete_response/#src.openai.types.beta.assistants.file_delete_response.FileDeleteResponse.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/beta/assistants/file_delete_response/#src.openai.types.beta.assistants.file_delete_response.FileDeleteResponse.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['assistant.file.deleted']\n</code></pre>"},{"location":"types/beta/assistants/file_list_params/","title":"File list params","text":""},{"location":"types/beta/assistants/file_list_params/#src.openai.types.beta.assistants.file_list_params","title":"file_list_params","text":""},{"location":"types/beta/assistants/file_list_params/#src.openai.types.beta.assistants.file_list_params.FileListParams","title":"FileListParams","text":""},{"location":"types/beta/assistants/file_list_params/#src.openai.types.beta.assistants.file_list_params.FileListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>after</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.</p>"},{"location":"types/beta/assistants/file_list_params/#src.openai.types.beta.assistants.file_list_params.FileListParams.before","title":"before  <code>instance-attribute</code>","text":"<pre><code>before: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>before</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.</p>"},{"location":"types/beta/assistants/file_list_params/#src.openai.types.beta.assistants.file_list_params.FileListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>A limit on the number of objects to be returned.</p> <p>Limit can range between 1 and 100, and the default is 20.</p>"},{"location":"types/beta/assistants/file_list_params/#src.openai.types.beta.assistants.file_list_params.FileListParams.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order: Literal['asc', 'desc']\n</code></pre> <p>Sort order by the <code>created_at</code> timestamp of the objects.</p> <p><code>asc</code> for ascending order and <code>desc</code> for descending order.</p>"},{"location":"types/beta/chat/__init__/","title":"openai.types.beta.chat","text":""},{"location":"types/beta/chat/__init__/#src.openai.types.beta.chat","title":"chat","text":""},{"location":"types/beta/threads/__init__/","title":"openai.types.beta.threads","text":""},{"location":"types/beta/threads/__init__/#src.openai.types.beta.threads","title":"threads","text":""},{"location":"types/beta/threads/message_content_image_file/","title":"Message content image file","text":""},{"location":"types/beta/threads/message_content_image_file/#src.openai.types.beta.threads.message_content_image_file","title":"message_content_image_file","text":""},{"location":"types/beta/threads/message_content_image_file/#src.openai.types.beta.threads.message_content_image_file.ImageFile","title":"ImageFile","text":""},{"location":"types/beta/threads/message_content_image_file/#src.openai.types.beta.threads.message_content_image_file.ImageFile.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>The File ID of the image in the message content.</p>"},{"location":"types/beta/threads/message_content_image_file/#src.openai.types.beta.threads.message_content_image_file.MessageContentImageFile","title":"MessageContentImageFile","text":""},{"location":"types/beta/threads/message_content_image_file/#src.openai.types.beta.threads.message_content_image_file.MessageContentImageFile.image_file","title":"image_file  <code>instance-attribute</code>","text":"<pre><code>image_file: ImageFile\n</code></pre>"},{"location":"types/beta/threads/message_content_image_file/#src.openai.types.beta.threads.message_content_image_file.MessageContentImageFile.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['image_file']\n</code></pre> <p>Always <code>image_file</code>.</p>"},{"location":"types/beta/threads/message_content_text/","title":"Message content text","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text","title":"message_content_text","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotation","title":"TextAnnotation  <code>module-attribute</code>","text":"<pre><code>TextAnnotation = Union[\n    TextAnnotationFileCitation, TextAnnotationFilePath\n]\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.MessageContentText","title":"MessageContentText","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.MessageContentText.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: Text\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.MessageContentText.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['text']\n</code></pre> <p>Always <code>text</code>.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.Text","title":"Text","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.Text.annotations","title":"annotations  <code>instance-attribute</code>","text":"<pre><code>annotations: List[TextAnnotation]\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.Text.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: str\n</code></pre> <p>The data that makes up the text.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitation","title":"TextAnnotationFileCitation","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitation.end_index","title":"end_index  <code>instance-attribute</code>","text":"<pre><code>end_index: int\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitation.file_citation","title":"file_citation  <code>instance-attribute</code>","text":"<pre><code>file_citation: TextAnnotationFileCitationFileCitation\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitation.start_index","title":"start_index  <code>instance-attribute</code>","text":"<pre><code>start_index: int\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitation.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text in the message content that needs to be replaced.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitation.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['file_citation']\n</code></pre> <p>Always <code>file_citation</code>.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitationFileCitation","title":"TextAnnotationFileCitationFileCitation","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitationFileCitation.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>The ID of the specific File the citation is from.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFileCitationFileCitation.quote","title":"quote  <code>instance-attribute</code>","text":"<pre><code>quote: str\n</code></pre> <p>The specific quote in the file.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePath","title":"TextAnnotationFilePath","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePath.end_index","title":"end_index  <code>instance-attribute</code>","text":"<pre><code>end_index: int\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePath.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path: TextAnnotationFilePathFilePath\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePath.start_index","title":"start_index  <code>instance-attribute</code>","text":"<pre><code>start_index: int\n</code></pre>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePath.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text in the message content that needs to be replaced.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePath.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['file_path']\n</code></pre> <p>Always <code>file_path</code>.</p>"},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePathFilePath","title":"TextAnnotationFilePathFilePath","text":""},{"location":"types/beta/threads/message_content_text/#src.openai.types.beta.threads.message_content_text.TextAnnotationFilePathFilePath.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>The ID of the file that was generated.</p>"},{"location":"types/beta/threads/message_create_params/","title":"Message create params","text":""},{"location":"types/beta/threads/message_create_params/#src.openai.types.beta.threads.message_create_params","title":"message_create_params","text":""},{"location":"types/beta/threads/message_create_params/#src.openai.types.beta.threads.message_create_params.MessageCreateParams","title":"MessageCreateParams","text":""},{"location":"types/beta/threads/message_create_params/#src.openai.types.beta.threads.message_create_params.MessageCreateParams.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The content of the message.</p>"},{"location":"types/beta/threads/message_create_params/#src.openai.types.beta.threads.message_create_params.MessageCreateParams.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of File IDs that the message should use. There can be a maximum of 10 files attached to a message. Useful for tools like <code>retrieval</code> and <code>code_interpreter</code> that can access and use files.</p>"},{"location":"types/beta/threads/message_create_params/#src.openai.types.beta.threads.message_create_params.MessageCreateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/message_create_params/#src.openai.types.beta.threads.message_create_params.MessageCreateParams.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the entity that is creating the message.</p> <p>Currently only <code>user</code> is supported.</p>"},{"location":"types/beta/threads/message_list_params/","title":"Message list params","text":""},{"location":"types/beta/threads/message_list_params/#src.openai.types.beta.threads.message_list_params","title":"message_list_params","text":""},{"location":"types/beta/threads/message_list_params/#src.openai.types.beta.threads.message_list_params.MessageListParams","title":"MessageListParams","text":""},{"location":"types/beta/threads/message_list_params/#src.openai.types.beta.threads.message_list_params.MessageListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>after</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.</p>"},{"location":"types/beta/threads/message_list_params/#src.openai.types.beta.threads.message_list_params.MessageListParams.before","title":"before  <code>instance-attribute</code>","text":"<pre><code>before: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>before</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.</p>"},{"location":"types/beta/threads/message_list_params/#src.openai.types.beta.threads.message_list_params.MessageListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>A limit on the number of objects to be returned.</p> <p>Limit can range between 1 and 100, and the default is 20.</p>"},{"location":"types/beta/threads/message_list_params/#src.openai.types.beta.threads.message_list_params.MessageListParams.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order: Literal['asc', 'desc']\n</code></pre> <p>Sort order by the <code>created_at</code> timestamp of the objects.</p> <p><code>asc</code> for ascending order and <code>desc</code> for descending order.</p>"},{"location":"types/beta/threads/message_update_params/","title":"Message update params","text":""},{"location":"types/beta/threads/message_update_params/#src.openai.types.beta.threads.message_update_params","title":"message_update_params","text":""},{"location":"types/beta/threads/message_update_params/#src.openai.types.beta.threads.message_update_params.MessageUpdateParams","title":"MessageUpdateParams","text":""},{"location":"types/beta/threads/message_update_params/#src.openai.types.beta.threads.message_update_params.MessageUpdateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/message_update_params/#src.openai.types.beta.threads.message_update_params.MessageUpdateParams.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: Required[str]\n</code></pre>"},{"location":"types/beta/threads/required_action_function_tool_call/","title":"Required action function tool call","text":""},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call","title":"required_action_function_tool_call","text":""},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.Function","title":"Function","text":""},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The arguments that the model expects you to pass to the function.</p>"},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function.</p>"},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.RequiredActionFunctionToolCall","title":"RequiredActionFunctionToolCall","text":""},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.RequiredActionFunctionToolCall.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Function\n</code></pre> <p>The function definition.</p>"},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.RequiredActionFunctionToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call.</p> <p>This ID must be referenced when you submit the tool outputs in using the Submit tool outputs to run endpoint.</p>"},{"location":"types/beta/threads/required_action_function_tool_call/#src.openai.types.beta.threads.required_action_function_tool_call.RequiredActionFunctionToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['function']\n</code></pre> <p>The type of tool call the output is required for.</p> <p>For now, this is always <code>function</code>.</p>"},{"location":"types/beta/threads/run/","title":"Run","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run","title":"run","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    ToolAssistantToolsCode,\n    ToolAssistantToolsRetrieval,\n    ToolAssistantToolsFunction,\n]\n</code></pre>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.LastError","title":"LastError","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.LastError.code","title":"code  <code>instance-attribute</code>","text":"<pre><code>code: Literal['server_error', 'rate_limit_exceeded']\n</code></pre> <p>One of <code>server_error</code> or <code>rate_limit_exceeded</code>.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.LastError.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>A human-readable description of the error.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.RequiredAction","title":"RequiredAction","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.RequiredAction.submit_tool_outputs","title":"submit_tool_outputs  <code>instance-attribute</code>","text":"<pre><code>submit_tool_outputs: RequiredActionSubmitToolOutputs\n</code></pre> <p>Details on the tool outputs needed for this run to continue.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.RequiredAction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['submit_tool_outputs']\n</code></pre> <p>For now, this is always <code>submit_tool_outputs</code>.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.RequiredActionSubmitToolOutputs","title":"RequiredActionSubmitToolOutputs","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.RequiredActionSubmitToolOutputs.tool_calls","title":"tool_calls  <code>instance-attribute</code>","text":"<pre><code>tool_calls: List[RequiredActionFunctionToolCall]\n</code></pre> <p>A list of the relevant tool calls.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run","title":"Run","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.assistant_id","title":"assistant_id  <code>instance-attribute</code>","text":"<pre><code>assistant_id: str\n</code></pre> <p>The ID of the assistant used for execution of this run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.cancelled_at","title":"cancelled_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cancelled_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run was cancelled.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.completed_at","title":"completed_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>completed_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run was completed.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the run was created.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.expires_at","title":"expires_at  <code>instance-attribute</code>","text":"<pre><code>expires_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the run will expire.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.failed_at","title":"failed_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>failed_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run failed.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>The list of File IDs the assistant used for this run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier, which can be referenced in API endpoints.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: str\n</code></pre> <p>The instructions that the assistant used for this run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.last_error","title":"last_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>last_error: Optional[LastError] = None\n</code></pre> <p>The last error associated with this run. Will be <code>null</code> if there are no errors.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object] = None\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model that the assistant used for this run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['thread.run']\n</code></pre> <p>The object type, which is always <code>thread.run</code>.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.required_action","title":"required_action  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>required_action: Optional[RequiredAction] = None\n</code></pre> <p>Details on the action required to continue the run.</p> <p>Will be <code>null</code> if no action is required.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.started_at","title":"started_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>started_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run was started.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Literal[\n    \"queued\",\n    \"in_progress\",\n    \"requires_action\",\n    \"cancelling\",\n    \"cancelled\",\n    \"failed\",\n    \"completed\",\n    \"expired\",\n]\n</code></pre> <p>The status of the run, which can be either <code>queued</code>, <code>in_progress</code>, <code>requires_action</code>, <code>cancelling</code>, <code>cancelled</code>, <code>failed</code>, <code>completed</code>, or <code>expired</code>.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: str\n</code></pre> <p>The ID of the thread that was executed on as a part of this run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: List[Tool]\n</code></pre> <p>The list of tools that the assistant used for this run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Run.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Optional[Usage] = None\n</code></pre> <p>Usage statistics related to the run.</p> <p>This value will be <code>null</code> if the run is not in a terminal state (i.e. <code>in_progress</code>, <code>queued</code>, etc.).</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsCode","title":"ToolAssistantToolsCode","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsCode.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['code_interpreter']\n</code></pre> <p>The type of tool being defined: <code>code_interpreter</code></p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsFunction","title":"ToolAssistantToolsFunction","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsFunction.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: FunctionDefinition\n</code></pre>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsFunction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['function']\n</code></pre> <p>The type of tool being defined: <code>function</code></p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsRetrieval","title":"ToolAssistantToolsRetrieval","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.ToolAssistantToolsRetrieval.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['retrieval']\n</code></pre> <p>The type of tool being defined: <code>retrieval</code></p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Usage","title":"Usage","text":""},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Usage.completion_tokens","title":"completion_tokens  <code>instance-attribute</code>","text":"<pre><code>completion_tokens: int\n</code></pre> <p>Number of completion tokens used over the course of the run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Usage.prompt_tokens","title":"prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>prompt_tokens: int\n</code></pre> <p>Number of prompt tokens used over the course of the run.</p>"},{"location":"types/beta/threads/run/#src.openai.types.beta.threads.run.Usage.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Total number of tokens used (prompt + completion).</p>"},{"location":"types/beta/threads/run_create_params/","title":"Run create params","text":""},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params","title":"run_create_params","text":""},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    ToolAssistantToolsCode,\n    ToolAssistantToolsRetrieval,\n    ToolAssistantToolsFunction,\n]\n</code></pre>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams","title":"RunCreateParams","text":""},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams.additional_instructions","title":"additional_instructions  <code>instance-attribute</code>","text":"<pre><code>additional_instructions: Optional[str]\n</code></pre> <p>Appends additional instructions at the end of the instructions for the run.</p> <p>This is useful for modifying the behavior on a per-run basis without overriding other instructions.</p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams.assistant_id","title":"assistant_id  <code>instance-attribute</code>","text":"<pre><code>assistant_id: Required[str]\n</code></pre> <p>The ID of the assistant to use to execute this run.</p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: Optional[str]\n</code></pre> <p>Overrides the instructions of the assistant. This is useful for modifying the behavior on a per-run basis.</p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Optional[str]\n</code></pre> <p>The ID of the Model to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.</p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.RunCreateParams.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: Optional[Iterable[Tool]]\n</code></pre> <p>Override the tools the assistant can use for this run.</p> <p>This is useful for modifying the behavior on a per-run basis.</p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsCode","title":"ToolAssistantToolsCode","text":""},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsCode.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['code_interpreter']]\n</code></pre> <p>The type of tool being defined: <code>code_interpreter</code></p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsFunction","title":"ToolAssistantToolsFunction","text":""},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsFunction.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[FunctionDefinition]\n</code></pre>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsFunction.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of tool being defined: <code>function</code></p>"},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsRetrieval","title":"ToolAssistantToolsRetrieval","text":""},{"location":"types/beta/threads/run_create_params/#src.openai.types.beta.threads.run_create_params.ToolAssistantToolsRetrieval.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['retrieval']]\n</code></pre> <p>The type of tool being defined: <code>retrieval</code></p>"},{"location":"types/beta/threads/run_list_params/","title":"Run list params","text":""},{"location":"types/beta/threads/run_list_params/#src.openai.types.beta.threads.run_list_params","title":"run_list_params","text":""},{"location":"types/beta/threads/run_list_params/#src.openai.types.beta.threads.run_list_params.RunListParams","title":"RunListParams","text":""},{"location":"types/beta/threads/run_list_params/#src.openai.types.beta.threads.run_list_params.RunListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>after</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.</p>"},{"location":"types/beta/threads/run_list_params/#src.openai.types.beta.threads.run_list_params.RunListParams.before","title":"before  <code>instance-attribute</code>","text":"<pre><code>before: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>before</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.</p>"},{"location":"types/beta/threads/run_list_params/#src.openai.types.beta.threads.run_list_params.RunListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>A limit on the number of objects to be returned.</p> <p>Limit can range between 1 and 100, and the default is 20.</p>"},{"location":"types/beta/threads/run_list_params/#src.openai.types.beta.threads.run_list_params.RunListParams.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order: Literal['asc', 'desc']\n</code></pre> <p>Sort order by the <code>created_at</code> timestamp of the objects.</p> <p><code>asc</code> for ascending order and <code>desc</code> for descending order.</p>"},{"location":"types/beta/threads/run_submit_tool_outputs_params/","title":"Run submit tool outputs params","text":""},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params","title":"run_submit_tool_outputs_params","text":""},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params.RunSubmitToolOutputsParams","title":"RunSubmitToolOutputsParams","text":""},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params.RunSubmitToolOutputsParams.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: Required[str]\n</code></pre>"},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params.RunSubmitToolOutputsParams.tool_outputs","title":"tool_outputs  <code>instance-attribute</code>","text":"<pre><code>tool_outputs: Required[Iterable[ToolOutput]]\n</code></pre> <p>A list of tools for which the outputs are being submitted.</p>"},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params.ToolOutput","title":"ToolOutput","text":""},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params.ToolOutput.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: str\n</code></pre> <p>The output of the tool call to be submitted to continue the run.</p>"},{"location":"types/beta/threads/run_submit_tool_outputs_params/#src.openai.types.beta.threads.run_submit_tool_outputs_params.ToolOutput.tool_call_id","title":"tool_call_id  <code>instance-attribute</code>","text":"<pre><code>tool_call_id: str\n</code></pre> <p>The ID of the tool call in the <code>required_action</code> object within the run object the output is being submitted for.</p>"},{"location":"types/beta/threads/run_update_params/","title":"Run update params","text":""},{"location":"types/beta/threads/run_update_params/#src.openai.types.beta.threads.run_update_params","title":"run_update_params","text":""},{"location":"types/beta/threads/run_update_params/#src.openai.types.beta.threads.run_update_params.RunUpdateParams","title":"RunUpdateParams","text":""},{"location":"types/beta/threads/run_update_params/#src.openai.types.beta.threads.run_update_params.RunUpdateParams.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object]\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/run_update_params/#src.openai.types.beta.threads.run_update_params.RunUpdateParams.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: Required[str]\n</code></pre>"},{"location":"types/beta/threads/thread_message/","title":"Thread message","text":""},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message","title":"thread_message","text":""},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.Content","title":"Content  <code>module-attribute</code>","text":"<pre><code>Content = Union[MessageContentImageFile, MessageContentText]\n</code></pre>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage","title":"ThreadMessage","text":""},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.assistant_id","title":"assistant_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>assistant_id: Optional[str] = None\n</code></pre> <p>If applicable, the ID of the assistant that authored this message.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: List[Content]\n</code></pre> <p>The content of the message in array of text and/or images.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the message was created.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.file_ids","title":"file_ids  <code>instance-attribute</code>","text":"<pre><code>file_ids: List[str]\n</code></pre> <p>A list of file IDs that the assistant should use. Useful for tools like retrieval and code_interpreter that can access files. A maximum of 10 files can be attached to a message.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier, which can be referenced in API endpoints.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object] = None\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['thread.message']\n</code></pre> <p>The object type, which is always <code>thread.message</code>.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Literal['user', 'assistant']\n</code></pre> <p>The entity that produced the message. One of <code>user</code> or <code>assistant</code>.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.run_id","title":"run_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_id: Optional[str] = None\n</code></pre> <p>If applicable, the ID of the run associated with the authoring of this message.</p>"},{"location":"types/beta/threads/thread_message/#src.openai.types.beta.threads.thread_message.ThreadMessage.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: str\n</code></pre> <p>The thread ID that this message belongs to.</p>"},{"location":"types/beta/threads/messages/__init__/","title":"openai.types.beta.threads.messages","text":""},{"location":"types/beta/threads/messages/__init__/#src.openai.types.beta.threads.messages","title":"messages","text":""},{"location":"types/beta/threads/messages/file_list_params/","title":"File list params","text":""},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params","title":"file_list_params","text":""},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params.FileListParams","title":"FileListParams","text":""},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params.FileListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>after</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.</p>"},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params.FileListParams.before","title":"before  <code>instance-attribute</code>","text":"<pre><code>before: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>before</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.</p>"},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params.FileListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>A limit on the number of objects to be returned.</p> <p>Limit can range between 1 and 100, and the default is 20.</p>"},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params.FileListParams.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order: Literal['asc', 'desc']\n</code></pre> <p>Sort order by the <code>created_at</code> timestamp of the objects.</p> <p><code>asc</code> for ascending order and <code>desc</code> for descending order.</p>"},{"location":"types/beta/threads/messages/file_list_params/#src.openai.types.beta.threads.messages.file_list_params.FileListParams.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: Required[str]\n</code></pre>"},{"location":"types/beta/threads/messages/message_file/","title":"Message file","text":""},{"location":"types/beta/threads/messages/message_file/#src.openai.types.beta.threads.messages.message_file","title":"message_file","text":""},{"location":"types/beta/threads/messages/message_file/#src.openai.types.beta.threads.messages.message_file.MessageFile","title":"MessageFile","text":""},{"location":"types/beta/threads/messages/message_file/#src.openai.types.beta.threads.messages.message_file.MessageFile.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the message file was created.</p>"},{"location":"types/beta/threads/messages/message_file/#src.openai.types.beta.threads.messages.message_file.MessageFile.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier, which can be referenced in API endpoints.</p>"},{"location":"types/beta/threads/messages/message_file/#src.openai.types.beta.threads.messages.message_file.MessageFile.message_id","title":"message_id  <code>instance-attribute</code>","text":"<pre><code>message_id: str\n</code></pre> <p>The ID of the message that the File is attached to.</p>"},{"location":"types/beta/threads/messages/message_file/#src.openai.types.beta.threads.messages.message_file.MessageFile.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['thread.message.file']\n</code></pre> <p>The object type, which is always <code>thread.message.file</code>.</p>"},{"location":"types/beta/threads/runs/__init__/","title":"openai.types.beta.threads.runs","text":""},{"location":"types/beta/threads/runs/__init__/#src.openai.types.beta.threads.runs","title":"runs","text":""},{"location":"types/beta/threads/runs/code_tool_call/","title":"Code tool call","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call","title":"code_tool_call","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutput","title":"CodeInterpreterOutput  <code>module-attribute</code>","text":"<pre><code>CodeInterpreterOutput = Union[\n    CodeInterpreterOutputLogs, CodeInterpreterOutputImage\n]\n</code></pre>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreter","title":"CodeInterpreter","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreter.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str\n</code></pre> <p>The input to the Code Interpreter tool call.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreter.outputs","title":"outputs  <code>instance-attribute</code>","text":"<pre><code>outputs: List[CodeInterpreterOutput]\n</code></pre> <p>The outputs from the Code Interpreter tool call.</p> <p>Code Interpreter can output one or more items, including text (<code>logs</code>) or images (<code>image</code>). Each of these are represented by a different object type.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputImage","title":"CodeInterpreterOutputImage","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputImage.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: CodeInterpreterOutputImageImage\n</code></pre>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputImage.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['image']\n</code></pre> <p>Always <code>image</code>.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputImageImage","title":"CodeInterpreterOutputImageImage","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputImageImage.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>The file ID of the image.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputLogs","title":"CodeInterpreterOutputLogs","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputLogs.logs","title":"logs  <code>instance-attribute</code>","text":"<pre><code>logs: str\n</code></pre> <p>The text output from the Code Interpreter tool call.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeInterpreterOutputLogs.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['logs']\n</code></pre> <p>Always <code>logs</code>.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeToolCall","title":"CodeToolCall","text":""},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeToolCall.code_interpreter","title":"code_interpreter  <code>instance-attribute</code>","text":"<pre><code>code_interpreter: CodeInterpreter\n</code></pre> <p>The Code Interpreter tool call definition.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"types/beta/threads/runs/code_tool_call/#src.openai.types.beta.threads.runs.code_tool_call.CodeToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['code_interpreter']\n</code></pre> <p>The type of tool call.</p> <p>This is always going to be <code>code_interpreter</code> for this type of tool call.</p>"},{"location":"types/beta/threads/runs/function_tool_call/","title":"Function tool call","text":""},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call","title":"function_tool_call","text":""},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.Function","title":"Function","text":""},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The arguments passed to the function.</p>"},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function.</p>"},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.Function.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: Optional[str] = None\n</code></pre> <p>The output of the function.</p> <p>This will be <code>null</code> if the outputs have not been submitted yet.</p>"},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.FunctionToolCall","title":"FunctionToolCall","text":""},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.FunctionToolCall.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Function\n</code></pre> <p>The definition of the function that was called.</p>"},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.FunctionToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call object.</p>"},{"location":"types/beta/threads/runs/function_tool_call/#src.openai.types.beta.threads.runs.function_tool_call.FunctionToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['function']\n</code></pre> <p>The type of tool call.</p> <p>This is always going to be <code>function</code> for this type of tool call.</p>"},{"location":"types/beta/threads/runs/message_creation_step_details/","title":"Message creation step details","text":""},{"location":"types/beta/threads/runs/message_creation_step_details/#src.openai.types.beta.threads.runs.message_creation_step_details","title":"message_creation_step_details","text":""},{"location":"types/beta/threads/runs/message_creation_step_details/#src.openai.types.beta.threads.runs.message_creation_step_details.MessageCreation","title":"MessageCreation","text":""},{"location":"types/beta/threads/runs/message_creation_step_details/#src.openai.types.beta.threads.runs.message_creation_step_details.MessageCreation.message_id","title":"message_id  <code>instance-attribute</code>","text":"<pre><code>message_id: str\n</code></pre> <p>The ID of the message that was created by this run step.</p>"},{"location":"types/beta/threads/runs/message_creation_step_details/#src.openai.types.beta.threads.runs.message_creation_step_details.MessageCreationStepDetails","title":"MessageCreationStepDetails","text":""},{"location":"types/beta/threads/runs/message_creation_step_details/#src.openai.types.beta.threads.runs.message_creation_step_details.MessageCreationStepDetails.message_creation","title":"message_creation  <code>instance-attribute</code>","text":"<pre><code>message_creation: MessageCreation\n</code></pre>"},{"location":"types/beta/threads/runs/message_creation_step_details/#src.openai.types.beta.threads.runs.message_creation_step_details.MessageCreationStepDetails.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['message_creation']\n</code></pre> <p>Always <code>message_creation</code>.</p>"},{"location":"types/beta/threads/runs/retrieval_tool_call/","title":"Retrieval tool call","text":""},{"location":"types/beta/threads/runs/retrieval_tool_call/#src.openai.types.beta.threads.runs.retrieval_tool_call","title":"retrieval_tool_call","text":""},{"location":"types/beta/threads/runs/retrieval_tool_call/#src.openai.types.beta.threads.runs.retrieval_tool_call.RetrievalToolCall","title":"RetrievalToolCall","text":""},{"location":"types/beta/threads/runs/retrieval_tool_call/#src.openai.types.beta.threads.runs.retrieval_tool_call.RetrievalToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call object.</p>"},{"location":"types/beta/threads/runs/retrieval_tool_call/#src.openai.types.beta.threads.runs.retrieval_tool_call.RetrievalToolCall.retrieval","title":"retrieval  <code>instance-attribute</code>","text":"<pre><code>retrieval: object\n</code></pre> <p>For now, this is always going to be an empty object.</p>"},{"location":"types/beta/threads/runs/retrieval_tool_call/#src.openai.types.beta.threads.runs.retrieval_tool_call.RetrievalToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['retrieval']\n</code></pre> <p>The type of tool call.</p> <p>This is always going to be <code>retrieval</code> for this type of tool call.</p>"},{"location":"types/beta/threads/runs/run_step/","title":"Run step","text":""},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step","title":"run_step","text":""},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.StepDetails","title":"StepDetails  <code>module-attribute</code>","text":"<pre><code>StepDetails = Union[\n    MessageCreationStepDetails, ToolCallsStepDetails\n]\n</code></pre>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.LastError","title":"LastError","text":""},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.LastError.code","title":"code  <code>instance-attribute</code>","text":"<pre><code>code: Literal['server_error', 'rate_limit_exceeded']\n</code></pre> <p>One of <code>server_error</code> or <code>rate_limit_exceeded</code>.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.LastError.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>A human-readable description of the error.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep","title":"RunStep","text":""},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.assistant_id","title":"assistant_id  <code>instance-attribute</code>","text":"<pre><code>assistant_id: str\n</code></pre> <p>The ID of the assistant associated with the run step.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.cancelled_at","title":"cancelled_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cancelled_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run step was cancelled.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.completed_at","title":"completed_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>completed_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run step completed.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the run step was created.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.expired_at","title":"expired_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>expired_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run step expired.</p> <p>A step is considered expired if the parent run is expired.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.failed_at","title":"failed_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>failed_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the run step failed.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The identifier of the run step, which can be referenced in API endpoints.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.last_error","title":"last_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>last_error: Optional[LastError] = None\n</code></pre> <p>The last error associated with this run step.</p> <p>Will be <code>null</code> if there are no errors.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[object] = None\n</code></pre> <p>Set of 16 key-value pairs that can be attached to an object.</p> <p>This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['thread.run.step']\n</code></pre> <p>The object type, which is always <code>thread.run.step</code>.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.run_id","title":"run_id  <code>instance-attribute</code>","text":"<pre><code>run_id: str\n</code></pre> <p>The ID of the run that this run step is a part of.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Literal[\n    \"in_progress\",\n    \"cancelled\",\n    \"failed\",\n    \"completed\",\n    \"expired\",\n]\n</code></pre> <p>The status of the run step, which can be either <code>in_progress</code>, <code>cancelled</code>, <code>failed</code>, <code>completed</code>, or <code>expired</code>.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.step_details","title":"step_details  <code>instance-attribute</code>","text":"<pre><code>step_details: StepDetails\n</code></pre> <p>The details of the run step.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: str\n</code></pre> <p>The ID of the thread that was run.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['message_creation', 'tool_calls']\n</code></pre> <p>The type of run step, which can be either <code>message_creation</code> or <code>tool_calls</code>.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.RunStep.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Optional[Usage] = None\n</code></pre> <p>Usage statistics related to the run step.</p> <p>This value will be <code>null</code> while the run step's status is <code>in_progress</code>.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.Usage","title":"Usage","text":""},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.Usage.completion_tokens","title":"completion_tokens  <code>instance-attribute</code>","text":"<pre><code>completion_tokens: int\n</code></pre> <p>Number of completion tokens used over the course of the run step.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.Usage.prompt_tokens","title":"prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>prompt_tokens: int\n</code></pre> <p>Number of prompt tokens used over the course of the run step.</p>"},{"location":"types/beta/threads/runs/run_step/#src.openai.types.beta.threads.runs.run_step.Usage.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Total number of tokens used (prompt + completion).</p>"},{"location":"types/beta/threads/runs/step_list_params/","title":"Step list params","text":""},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params","title":"step_list_params","text":""},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params.StepListParams","title":"StepListParams","text":""},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params.StepListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>after</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.</p>"},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params.StepListParams.before","title":"before  <code>instance-attribute</code>","text":"<pre><code>before: str\n</code></pre> <p>A cursor for use in pagination.</p> <p><code>before</code> is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.</p>"},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params.StepListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>A limit on the number of objects to be returned.</p> <p>Limit can range between 1 and 100, and the default is 20.</p>"},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params.StepListParams.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order: Literal['asc', 'desc']\n</code></pre> <p>Sort order by the <code>created_at</code> timestamp of the objects.</p> <p><code>asc</code> for ascending order and <code>desc</code> for descending order.</p>"},{"location":"types/beta/threads/runs/step_list_params/#src.openai.types.beta.threads.runs.step_list_params.StepListParams.thread_id","title":"thread_id  <code>instance-attribute</code>","text":"<pre><code>thread_id: Required[str]\n</code></pre>"},{"location":"types/beta/threads/runs/tool_calls_step_details/","title":"Tool calls step details","text":""},{"location":"types/beta/threads/runs/tool_calls_step_details/#src.openai.types.beta.threads.runs.tool_calls_step_details","title":"tool_calls_step_details","text":""},{"location":"types/beta/threads/runs/tool_calls_step_details/#src.openai.types.beta.threads.runs.tool_calls_step_details.ToolCall","title":"ToolCall  <code>module-attribute</code>","text":"<pre><code>ToolCall = Union[\n    CodeToolCall, RetrievalToolCall, FunctionToolCall\n]\n</code></pre>"},{"location":"types/beta/threads/runs/tool_calls_step_details/#src.openai.types.beta.threads.runs.tool_calls_step_details.ToolCallsStepDetails","title":"ToolCallsStepDetails","text":""},{"location":"types/beta/threads/runs/tool_calls_step_details/#src.openai.types.beta.threads.runs.tool_calls_step_details.ToolCallsStepDetails.tool_calls","title":"tool_calls  <code>instance-attribute</code>","text":"<pre><code>tool_calls: List[ToolCall]\n</code></pre> <p>An array of tool calls the run step was involved in.</p> <p>These can be associated with one of three types of tools: <code>code_interpreter</code>, <code>retrieval</code>, or <code>function</code>.</p>"},{"location":"types/beta/threads/runs/tool_calls_step_details/#src.openai.types.beta.threads.runs.tool_calls_step_details.ToolCallsStepDetails.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['tool_calls']\n</code></pre> <p>Always <code>tool_calls</code>.</p>"},{"location":"types/chat/__init__/","title":"openai.types.chat","text":""},{"location":"types/chat/__init__/#src.openai.types.chat","title":"chat","text":""},{"location":"types/chat/chat_completion/","title":"Chat completion","text":""},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion","title":"chat_completion","text":""},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion","title":"ChatCompletion","text":""},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: List[Choice]\n</code></pre> <p>A list of chat completion choices.</p> <p>Can be more than one if <code>n</code> is greater than 1.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre> <p>The Unix timestamp (in seconds) of when the chat completion was created.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>A unique identifier for the chat completion.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model used for the chat completion.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['chat.completion']\n</code></pre> <p>The object type, which is always <code>chat.completion</code>.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.system_fingerprint","title":"system_fingerprint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>system_fingerprint: Optional[str] = None\n</code></pre> <p>This fingerprint represents the backend configuration that the model runs with.</p> <p>Can be used in conjunction with the <code>seed</code> request parameter to understand when backend changes have been made that might impact determinism.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChatCompletion.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Optional[CompletionUsage] = None\n</code></pre> <p>Usage statistics for the completion request.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.Choice","title":"Choice","text":""},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.Choice.finish_reason","title":"finish_reason  <code>instance-attribute</code>","text":"<pre><code>finish_reason: Literal[\n    \"stop\",\n    \"length\",\n    \"tool_calls\",\n    \"content_filter\",\n    \"function_call\",\n]\n</code></pre> <p>The reason the model stopped generating tokens.</p> <p>This will be <code>stop</code> if the model hit a natural stop point or a provided stop sequence, <code>length</code> if the maximum number of tokens specified in the request was reached, <code>content_filter</code> if content was omitted due to a flag from our content filters, <code>tool_calls</code> if the model called a tool, or <code>function_call</code> (deprecated) if the model called a function.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.Choice.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>The index of the choice in the list of choices.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.Choice.logprobs","title":"logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logprobs: Optional[ChoiceLogprobs] = None\n</code></pre> <p>Log probability information for the choice.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.Choice.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: ChatCompletionMessage\n</code></pre> <p>A chat completion message generated by the model.</p>"},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChoiceLogprobs","title":"ChoiceLogprobs","text":""},{"location":"types/chat/chat_completion/#src.openai.types.chat.chat_completion.ChoiceLogprobs.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: Optional[List[ChatCompletionTokenLogprob]] = None\n</code></pre> <p>A list of message content tokens with log probability information.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/","title":"Chat completion assistant message param","text":""},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param","title":"chat_completion_assistant_message_param","text":""},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam","title":"ChatCompletionAssistantMessageParam","text":""},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Optional[str]\n</code></pre> <p>The contents of the assistant message.</p> <p>Required unless <code>tool_calls</code> or <code>function_call</code> is specified.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam.function_call","title":"function_call  <code>instance-attribute</code>","text":"<pre><code>function_call: FunctionCall\n</code></pre> <p>Deprecated and replaced by <code>tool_calls</code>.</p> <p>The name and arguments of a function that should be called, as generated by the model.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>An optional name for the participant.</p> <p>Provides the model information to differentiate between participants of the same role.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['assistant']]\n</code></pre> <p>The role of the messages author, in this case <code>assistant</code>.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam.tool_calls","title":"tool_calls  <code>instance-attribute</code>","text":"<pre><code>tool_calls: Iterable[ChatCompletionMessageToolCallParam]\n</code></pre> <p>The tool calls generated by the model, such as function calls.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.FunctionCall","title":"FunctionCall","text":""},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.FunctionCall.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Required[str]\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"types/chat/chat_completion_assistant_message_param/#src.openai.types.chat.chat_completion_assistant_message_param.FunctionCall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_chunk/","title":"Chat completion chunk","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk","title":"chat_completion_chunk","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk","title":"ChatCompletionChunk","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: List[Choice]\n</code></pre> <p>A list of chat completion choices.</p> <p>Can be more than one if <code>n</code> is greater than 1.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: int\n</code></pre> <p>The Unix timestamp (in seconds) of when the chat completion was created.</p> <p>Each chunk has the same timestamp.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>A unique identifier for the chat completion. Each chunk has the same ID.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The model to generate the completion.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['chat.completion.chunk']\n</code></pre> <p>The object type, which is always <code>chat.completion.chunk</code>.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChatCompletionChunk.system_fingerprint","title":"system_fingerprint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>system_fingerprint: Optional[str] = None\n</code></pre> <p>This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the <code>seed</code> request parameter to understand when backend changes have been made that might impact determinism.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.Choice","title":"Choice","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.Choice.delta","title":"delta  <code>instance-attribute</code>","text":"<pre><code>delta: ChoiceDelta\n</code></pre> <p>A chat completion delta generated by streamed model responses.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.Choice.finish_reason","title":"finish_reason  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>finish_reason: Optional[\n    Literal[\n        \"stop\",\n        \"length\",\n        \"tool_calls\",\n        \"content_filter\",\n        \"function_call\",\n    ]\n] = None\n</code></pre> <p>The reason the model stopped generating tokens.</p> <p>This will be <code>stop</code> if the model hit a natural stop point or a provided stop sequence, <code>length</code> if the maximum number of tokens specified in the request was reached, <code>content_filter</code> if content was omitted due to a flag from our content filters, <code>tool_calls</code> if the model called a tool, or <code>function_call</code> (deprecated) if the model called a function.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.Choice.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>The index of the choice in the list of choices.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.Choice.logprobs","title":"logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logprobs: Optional[ChoiceLogprobs] = None\n</code></pre> <p>Log probability information for the choice.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDelta","title":"ChoiceDelta","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDelta.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: Optional[str] = None\n</code></pre> <p>The contents of the chunk message.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDelta.function_call","title":"function_call  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>function_call: Optional[ChoiceDeltaFunctionCall] = None\n</code></pre> <p>Deprecated and replaced by <code>tool_calls</code>.</p> <p>The name and arguments of a function that should be called, as generated by the model.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDelta.role","title":"role  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>role: Optional[\n    Literal[\"system\", \"user\", \"assistant\", \"tool\"]\n] = None\n</code></pre> <p>The role of the author of this message.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDelta.tool_calls","title":"tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_calls: Optional[List[ChoiceDeltaToolCall]] = None\n</code></pre>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaFunctionCall","title":"ChoiceDeltaFunctionCall","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaFunctionCall.arguments","title":"arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arguments: Optional[str] = None\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaFunctionCall.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: Optional[str] = None\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCall","title":"ChoiceDeltaToolCall","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCall.function","title":"function  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>function: Optional[ChoiceDeltaToolCallFunction] = None\n</code></pre>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCall.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Optional[str] = None\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCall.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCall.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Optional[Literal['function']] = None\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCallFunction","title":"ChoiceDeltaToolCallFunction","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCallFunction.arguments","title":"arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arguments: Optional[str] = None\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCallFunction.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: Optional[str] = None\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceLogprobs","title":"ChoiceLogprobs","text":""},{"location":"types/chat/chat_completion_chunk/#src.openai.types.chat.chat_completion_chunk.ChoiceLogprobs.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: Optional[List[ChatCompletionTokenLogprob]] = None\n</code></pre> <p>A list of message content tokens with log probability information.</p>"},{"location":"types/chat/chat_completion_content_part_image_param/","title":"Chat completion content part image param","text":""},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param","title":"chat_completion_content_part_image_param","text":""},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param.ChatCompletionContentPartImageParam","title":"ChatCompletionContentPartImageParam","text":""},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param.ChatCompletionContentPartImageParam.image_url","title":"image_url  <code>instance-attribute</code>","text":"<pre><code>image_url: Required[ImageURL]\n</code></pre>"},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param.ChatCompletionContentPartImageParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['image_url']]\n</code></pre> <p>The type of the content part.</p>"},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param.ImageURL","title":"ImageURL","text":""},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param.ImageURL.detail","title":"detail  <code>instance-attribute</code>","text":"<pre><code>detail: Literal['auto', 'low', 'high']\n</code></pre> <p>Specifies the detail level of the image.</p> <p>Learn more in the Vision guide.</p>"},{"location":"types/chat/chat_completion_content_part_image_param/#src.openai.types.chat.chat_completion_content_part_image_param.ImageURL.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: Required[str]\n</code></pre> <p>Either a URL of the image or the base64 encoded image data.</p>"},{"location":"types/chat/chat_completion_content_part_param/","title":"Chat completion content part param","text":""},{"location":"types/chat/chat_completion_content_part_param/#src.openai.types.chat.chat_completion_content_part_param","title":"chat_completion_content_part_param","text":""},{"location":"types/chat/chat_completion_content_part_param/#src.openai.types.chat.chat_completion_content_part_param.ChatCompletionContentPartParam","title":"ChatCompletionContentPartParam  <code>module-attribute</code>","text":"<pre><code>ChatCompletionContentPartParam = Union[\n    ChatCompletionContentPartTextParam,\n    ChatCompletionContentPartImageParam,\n]\n</code></pre>"},{"location":"types/chat/chat_completion_content_part_text_param/","title":"Chat completion content part text param","text":""},{"location":"types/chat/chat_completion_content_part_text_param/#src.openai.types.chat.chat_completion_content_part_text_param","title":"chat_completion_content_part_text_param","text":""},{"location":"types/chat/chat_completion_content_part_text_param/#src.openai.types.chat.chat_completion_content_part_text_param.ChatCompletionContentPartTextParam","title":"ChatCompletionContentPartTextParam","text":""},{"location":"types/chat/chat_completion_content_part_text_param/#src.openai.types.chat.chat_completion_content_part_text_param.ChatCompletionContentPartTextParam.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: Required[str]\n</code></pre> <p>The text content.</p>"},{"location":"types/chat/chat_completion_content_part_text_param/#src.openai.types.chat.chat_completion_content_part_text_param.ChatCompletionContentPartTextParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['text']]\n</code></pre> <p>The type of the content part.</p>"},{"location":"types/chat/chat_completion_function_call_option_param/","title":"Chat completion function call option param","text":""},{"location":"types/chat/chat_completion_function_call_option_param/#src.openai.types.chat.chat_completion_function_call_option_param","title":"chat_completion_function_call_option_param","text":""},{"location":"types/chat/chat_completion_function_call_option_param/#src.openai.types.chat.chat_completion_function_call_option_param.ChatCompletionFunctionCallOptionParam","title":"ChatCompletionFunctionCallOptionParam","text":""},{"location":"types/chat/chat_completion_function_call_option_param/#src.openai.types.chat.chat_completion_function_call_option_param.ChatCompletionFunctionCallOptionParam.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_function_message_param/","title":"Chat completion function message param","text":""},{"location":"types/chat/chat_completion_function_message_param/#src.openai.types.chat.chat_completion_function_message_param","title":"chat_completion_function_message_param","text":""},{"location":"types/chat/chat_completion_function_message_param/#src.openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam","title":"ChatCompletionFunctionMessageParam","text":""},{"location":"types/chat/chat_completion_function_message_param/#src.openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[Optional[str]]\n</code></pre> <p>The contents of the function message.</p>"},{"location":"types/chat/chat_completion_function_message_param/#src.openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_function_message_param/#src.openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['function']]\n</code></pre> <p>The role of the messages author, in this case <code>function</code>.</p>"},{"location":"types/chat/chat_completion_message/","title":"Chat completion message","text":""},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message","title":"chat_completion_message","text":""},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.ChatCompletionMessage","title":"ChatCompletionMessage","text":""},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.ChatCompletionMessage.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: Optional[str] = None\n</code></pre> <p>The contents of the message.</p>"},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.ChatCompletionMessage.function_call","title":"function_call  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>function_call: Optional[FunctionCall] = None\n</code></pre> <p>Deprecated and replaced by <code>tool_calls</code>.</p> <p>The name and arguments of a function that should be called, as generated by the model.</p>"},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.ChatCompletionMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Literal['assistant']\n</code></pre> <p>The role of the author of this message.</p>"},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.ChatCompletionMessage.tool_calls","title":"tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_calls: Optional[\n    List[ChatCompletionMessageToolCall]\n] = None\n</code></pre> <p>The tool calls generated by the model, such as function calls.</p>"},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.FunctionCall","title":"FunctionCall","text":""},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.FunctionCall.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"types/chat/chat_completion_message/#src.openai.types.chat.chat_completion_message.FunctionCall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_message_param/","title":"Chat completion message param","text":""},{"location":"types/chat/chat_completion_message_param/#src.openai.types.chat.chat_completion_message_param","title":"chat_completion_message_param","text":""},{"location":"types/chat/chat_completion_message_param/#src.openai.types.chat.chat_completion_message_param.ChatCompletionMessageParam","title":"ChatCompletionMessageParam  <code>module-attribute</code>","text":"<pre><code>ChatCompletionMessageParam = Union[\n    ChatCompletionSystemMessageParam,\n    ChatCompletionUserMessageParam,\n    ChatCompletionAssistantMessageParam,\n    ChatCompletionToolMessageParam,\n    ChatCompletionFunctionMessageParam,\n]\n</code></pre>"},{"location":"types/chat/chat_completion_message_tool_call/","title":"Chat completion message tool call","text":""},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call","title":"chat_completion_message_tool_call","text":""},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.ChatCompletionMessageToolCall","title":"ChatCompletionMessageToolCall","text":""},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.ChatCompletionMessageToolCall.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Function\n</code></pre> <p>The function that the model called.</p>"},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.ChatCompletionMessageToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.ChatCompletionMessageToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['function']\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.Function","title":"Function","text":""},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"types/chat/chat_completion_message_tool_call/#src.openai.types.chat.chat_completion_message_tool_call.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_message_tool_call_param/","title":"Chat completion message tool call param","text":""},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param","title":"chat_completion_message_tool_call_param","text":""},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.ChatCompletionMessageToolCallParam","title":"ChatCompletionMessageToolCallParam","text":""},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.ChatCompletionMessageToolCallParam.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[Function]\n</code></pre> <p>The function that the model called.</p>"},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.ChatCompletionMessageToolCallParam.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: Required[str]\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.ChatCompletionMessageToolCallParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.Function","title":"Function","text":""},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Required[str]\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"types/chat/chat_completion_message_tool_call_param/#src.openai.types.chat.chat_completion_message_tool_call_param.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_named_tool_choice_param/","title":"Chat completion named tool choice param","text":""},{"location":"types/chat/chat_completion_named_tool_choice_param/#src.openai.types.chat.chat_completion_named_tool_choice_param","title":"chat_completion_named_tool_choice_param","text":""},{"location":"types/chat/chat_completion_named_tool_choice_param/#src.openai.types.chat.chat_completion_named_tool_choice_param.ChatCompletionNamedToolChoiceParam","title":"ChatCompletionNamedToolChoiceParam","text":""},{"location":"types/chat/chat_completion_named_tool_choice_param/#src.openai.types.chat.chat_completion_named_tool_choice_param.ChatCompletionNamedToolChoiceParam.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[Function]\n</code></pre>"},{"location":"types/chat/chat_completion_named_tool_choice_param/#src.openai.types.chat.chat_completion_named_tool_choice_param.ChatCompletionNamedToolChoiceParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"types/chat/chat_completion_named_tool_choice_param/#src.openai.types.chat.chat_completion_named_tool_choice_param.Function","title":"Function","text":""},{"location":"types/chat/chat_completion_named_tool_choice_param/#src.openai.types.chat.chat_completion_named_tool_choice_param.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"types/chat/chat_completion_role/","title":"Chat completion role","text":""},{"location":"types/chat/chat_completion_role/#src.openai.types.chat.chat_completion_role","title":"chat_completion_role","text":""},{"location":"types/chat/chat_completion_role/#src.openai.types.chat.chat_completion_role.ChatCompletionRole","title":"ChatCompletionRole  <code>module-attribute</code>","text":"<pre><code>ChatCompletionRole = Literal[\n    \"system\", \"user\", \"assistant\", \"tool\", \"function\"\n]\n</code></pre>"},{"location":"types/chat/chat_completion_system_message_param/","title":"Chat completion system message param","text":""},{"location":"types/chat/chat_completion_system_message_param/#src.openai.types.chat.chat_completion_system_message_param","title":"chat_completion_system_message_param","text":""},{"location":"types/chat/chat_completion_system_message_param/#src.openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam","title":"ChatCompletionSystemMessageParam","text":""},{"location":"types/chat/chat_completion_system_message_param/#src.openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the system message.</p>"},{"location":"types/chat/chat_completion_system_message_param/#src.openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>An optional name for the participant.</p> <p>Provides the model information to differentiate between participants of the same role.</p>"},{"location":"types/chat/chat_completion_system_message_param/#src.openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['system']]\n</code></pre> <p>The role of the messages author, in this case <code>system</code>.</p>"},{"location":"types/chat/chat_completion_token_logprob/","title":"Chat completion token logprob","text":""},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob","title":"chat_completion_token_logprob","text":""},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.ChatCompletionTokenLogprob","title":"ChatCompletionTokenLogprob","text":""},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.ChatCompletionTokenLogprob.bytes","title":"bytes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bytes: Optional[List[int]] = None\n</code></pre> <p>A list of integers representing the UTF-8 bytes representation of the token.</p> <p>Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be <code>null</code> if there is no bytes representation for the token.</p>"},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.ChatCompletionTokenLogprob.logprob","title":"logprob  <code>instance-attribute</code>","text":"<pre><code>logprob: float\n</code></pre> <p>The log probability of this token.</p>"},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.ChatCompletionTokenLogprob.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: str\n</code></pre> <p>The token.</p>"},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.ChatCompletionTokenLogprob.top_logprobs","title":"top_logprobs  <code>instance-attribute</code>","text":"<pre><code>top_logprobs: List[TopLogprob]\n</code></pre> <p>List of the most likely tokens and their log probability, at this token position.</p> <p>In rare cases, there may be fewer than the number of requested <code>top_logprobs</code> returned.</p>"},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.TopLogprob","title":"TopLogprob","text":""},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.TopLogprob.bytes","title":"bytes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bytes: Optional[List[int]] = None\n</code></pre> <p>A list of integers representing the UTF-8 bytes representation of the token.</p> <p>Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be <code>null</code> if there is no bytes representation for the token.</p>"},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.TopLogprob.logprob","title":"logprob  <code>instance-attribute</code>","text":"<pre><code>logprob: float\n</code></pre> <p>The log probability of this token.</p>"},{"location":"types/chat/chat_completion_token_logprob/#src.openai.types.chat.chat_completion_token_logprob.TopLogprob.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: str\n</code></pre> <p>The token.</p>"},{"location":"types/chat/chat_completion_tool_choice_option_param/","title":"Chat completion tool choice option param","text":""},{"location":"types/chat/chat_completion_tool_choice_option_param/#src.openai.types.chat.chat_completion_tool_choice_option_param","title":"chat_completion_tool_choice_option_param","text":""},{"location":"types/chat/chat_completion_tool_choice_option_param/#src.openai.types.chat.chat_completion_tool_choice_option_param.ChatCompletionToolChoiceOptionParam","title":"ChatCompletionToolChoiceOptionParam  <code>module-attribute</code>","text":"<pre><code>ChatCompletionToolChoiceOptionParam = Union[\n    Literal[\"none\", \"auto\"],\n    ChatCompletionNamedToolChoiceParam,\n]\n</code></pre>"},{"location":"types/chat/chat_completion_tool_message_param/","title":"Chat completion tool message param","text":""},{"location":"types/chat/chat_completion_tool_message_param/#src.openai.types.chat.chat_completion_tool_message_param","title":"chat_completion_tool_message_param","text":""},{"location":"types/chat/chat_completion_tool_message_param/#src.openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam","title":"ChatCompletionToolMessageParam","text":""},{"location":"types/chat/chat_completion_tool_message_param/#src.openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the tool message.</p>"},{"location":"types/chat/chat_completion_tool_message_param/#src.openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['tool']]\n</code></pre> <p>The role of the messages author, in this case <code>tool</code>.</p>"},{"location":"types/chat/chat_completion_tool_message_param/#src.openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam.tool_call_id","title":"tool_call_id  <code>instance-attribute</code>","text":"<pre><code>tool_call_id: Required[str]\n</code></pre> <p>Tool call that this message is responding to.</p>"},{"location":"types/chat/chat_completion_tool_param/","title":"Chat completion tool param","text":""},{"location":"types/chat/chat_completion_tool_param/#src.openai.types.chat.chat_completion_tool_param","title":"chat_completion_tool_param","text":""},{"location":"types/chat/chat_completion_tool_param/#src.openai.types.chat.chat_completion_tool_param.ChatCompletionToolParam","title":"ChatCompletionToolParam","text":""},{"location":"types/chat/chat_completion_tool_param/#src.openai.types.chat.chat_completion_tool_param.ChatCompletionToolParam.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[FunctionDefinition]\n</code></pre>"},{"location":"types/chat/chat_completion_tool_param/#src.openai.types.chat.chat_completion_tool_param.ChatCompletionToolParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"types/chat/chat_completion_user_message_param/","title":"Chat completion user message param","text":""},{"location":"types/chat/chat_completion_user_message_param/#src.openai.types.chat.chat_completion_user_message_param","title":"chat_completion_user_message_param","text":""},{"location":"types/chat/chat_completion_user_message_param/#src.openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam","title":"ChatCompletionUserMessageParam","text":""},{"location":"types/chat/chat_completion_user_message_param/#src.openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[\n    Union[str, Iterable[ChatCompletionContentPartParam]]\n]\n</code></pre> <p>The contents of the user message.</p>"},{"location":"types/chat/chat_completion_user_message_param/#src.openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>An optional name for the participant.</p> <p>Provides the model information to differentiate between participants of the same role.</p>"},{"location":"types/chat/chat_completion_user_message_param/#src.openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the messages author, in this case <code>user</code>.</p>"},{"location":"types/chat/completion_create_params/","title":"Completion create params","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params","title":"completion_create_params","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParams","title":"CompletionCreateParams  <code>module-attribute</code>","text":"<pre><code>CompletionCreateParams = Union[\n    CompletionCreateParamsNonStreaming,\n    CompletionCreateParamsStreaming,\n]\n</code></pre>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.FunctionCall","title":"FunctionCall  <code>module-attribute</code>","text":"<pre><code>FunctionCall = Union[\n    Literal[\"none\", \"auto\"],\n    ChatCompletionFunctionCallOptionParam,\n]\n</code></pre>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase","title":"CompletionCreateParamsBase","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.frequency_penalty","title":"frequency_penalty  <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: Optional[float]\n</code></pre> <p>Number between -2.0 and 2.0.</p> <p>Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <p>See more information about frequency and presence penalties.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.function_call","title":"function_call  <code>instance-attribute</code>","text":"<pre><code>function_call: FunctionCall\n</code></pre> <p>Deprecated in favor of <code>tool_choice</code>.</p> <p>Controls which (if any) function is called by the model. <code>none</code> means the model will not call a function and instead generates a message. <code>auto</code> means the model can pick between generating a message or calling a function. Specifying a particular function via <code>{\"name\": \"my_function\"}</code> forces the model to call that function.</p> <p><code>none</code> is the default when no functions are present. <code>auto</code> is the default if functions are present.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.functions","title":"functions  <code>instance-attribute</code>","text":"<pre><code>functions: Iterable[Function]\n</code></pre> <p>Deprecated in favor of <code>tools</code>.</p> <p>A list of functions the model may generate JSON inputs for.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.logit_bias","title":"logit_bias  <code>instance-attribute</code>","text":"<pre><code>logit_bias: Optional[Dict[str, int]]\n</code></pre> <p>Modify the likelihood of specified tokens appearing in the completion.</p> <p>Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.logprobs","title":"logprobs  <code>instance-attribute</code>","text":"<pre><code>logprobs: Optional[bool]\n</code></pre> <p>Whether to return log probabilities of the output tokens or not.</p> <p>If true, returns the log probabilities of each output token returned in the <code>content</code> of <code>message</code>. This option is currently not available on the <code>gpt-4-vision-preview</code> model.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens: Optional[int]\n</code></pre> <p>The maximum number of tokens that can be generated in the chat completion.</p> <p>The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: Required[Iterable[ChatCompletionMessageParam]]\n</code></pre> <p>A list of messages comprising the conversation so far.</p> <p>Example Python code.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[\n    Union[\n        str,\n        Literal[\n            \"gpt-4-0125-preview\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4-vision-preview\",\n            \"gpt-4\",\n            \"gpt-4-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-32k-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo-0301\",\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-16k-0613\",\n        ],\n    ]\n]\n</code></pre> <p>ID of the model to use.</p> <p>See the model endpoint compatibility table for details on which models work with the Chat API.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n: Optional[int]\n</code></pre> <p>How many chat completion choices to generate for each input message.</p> <p>Note that you will be charged based on the number of generated tokens across all of the choices. Keep <code>n</code> as <code>1</code> to minimize costs.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.presence_penalty","title":"presence_penalty  <code>instance-attribute</code>","text":"<pre><code>presence_penalty: Optional[float]\n</code></pre> <p>Number between -2.0 and 2.0.</p> <p>Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <p>See more information about frequency and presence penalties.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.response_format","title":"response_format  <code>instance-attribute</code>","text":"<pre><code>response_format: ResponseFormat\n</code></pre> <p>An object specifying the format that the model must output.</p> <p>Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than <code>gpt-3.5-turbo-1106</code>.</p> <p>Setting to <code>{ \"type\": \"json_object\" }</code> enables JSON mode, which guarantees the message the model generates is valid JSON.</p> <p>Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if <code>finish_reason=\"length\"</code>, which indicates the generation exceeded <code>max_tokens</code> or the conversation exceeded the max context length.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed: Optional[int]\n</code></pre> <p>This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same <code>seed</code> and parameters should return the same result. Determinism is not guaranteed, and you should refer to the <code>system_fingerprint</code> response parameter to monitor changes in the backend.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.stop","title":"stop  <code>instance-attribute</code>","text":"<pre><code>stop: Union[Optional[str], List[str]]\n</code></pre> <p>Up to 4 sequences where the API will stop generating further tokens.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature: Optional[float]\n</code></pre> <p>What sampling temperature to use, between 0 and 2.</p> <p>Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <p>We generally recommend altering this or <code>top_p</code> but not both.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.tool_choice","title":"tool_choice  <code>instance-attribute</code>","text":"<pre><code>tool_choice: ChatCompletionToolChoiceOptionParam\n</code></pre> <p>Controls which (if any) function is called by the model. <code>none</code> means the model will not call a function and instead generates a message. <code>auto</code> means the model can pick between generating a message or calling a function. Specifying a particular function via <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code> forces the model to call that function.</p> <p><code>none</code> is the default when no functions are present. <code>auto</code> is the default if functions are present.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: Iterable[ChatCompletionToolParam]\n</code></pre> <p>A list of tools the model may call.</p> <p>Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.top_logprobs","title":"top_logprobs  <code>instance-attribute</code>","text":"<pre><code>top_logprobs: Optional[int]\n</code></pre> <p>An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. <code>logprobs</code> must be set to <code>true</code> if this parameter is used.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.top_p","title":"top_p  <code>instance-attribute</code>","text":"<pre><code>top_p: Optional[float]\n</code></pre> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.</p> <p>We generally recommend altering this or <code>temperature</code> but not both.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsBase.user","title":"user  <code>instance-attribute</code>","text":"<pre><code>user: str\n</code></pre> <p>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsNonStreaming","title":"CompletionCreateParamsNonStreaming","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsNonStreaming.stream","title":"stream  <code>instance-attribute</code>","text":"<pre><code>stream: Optional[Literal[False]]\n</code></pre> <p>If set, partial message deltas will be sent, like in ChatGPT.</p> <p>Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a <code>data: [DONE]</code> message. Example Python code.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsStreaming","title":"CompletionCreateParamsStreaming","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.CompletionCreateParamsStreaming.stream","title":"stream  <code>instance-attribute</code>","text":"<pre><code>stream: Required[Literal[True]]\n</code></pre> <p>If set, partial message deltas will be sent, like in ChatGPT.</p> <p>Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a <code>data: [DONE]</code> message. Example Python code.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.Function","title":"Function","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.Function.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre> <p>A description of what the function does, used by the model to choose when and how to call the function.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to be called.</p> <p>Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.Function.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: FunctionParameters\n</code></pre> <p>The parameters the functions accepts, described as a JSON Schema object.</p> <p>See the guide for examples, and the JSON Schema reference for documentation about the format.</p> <p>Omitting <code>parameters</code> defines a function with an empty parameter list.</p>"},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.ResponseFormat","title":"ResponseFormat","text":""},{"location":"types/chat/completion_create_params/#src.openai.types.chat.completion_create_params.ResponseFormat.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['text', 'json_object']\n</code></pre> <p>Must be one of <code>text</code> or <code>json_object</code>.</p>"},{"location":"types/fine_tuning/__init__/","title":"openai.types.fine_tuning","text":""},{"location":"types/fine_tuning/__init__/#src.openai.types.fine_tuning","title":"fine_tuning","text":""},{"location":"types/fine_tuning/fine_tuning_job/","title":"Fine tuning job","text":""},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job","title":"fine_tuning_job","text":""},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.Error","title":"Error","text":""},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.Error.code","title":"code  <code>instance-attribute</code>","text":"<pre><code>code: str\n</code></pre> <p>A machine-readable error code.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.Error.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>A human-readable error message.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.Error.param","title":"param  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>param: Optional[str] = None\n</code></pre> <p>The parameter that was invalid, usually <code>training_file</code> or <code>validation_file</code>.</p> <p>This field will be null if the failure was not parameter-specific.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob","title":"FineTuningJob","text":""},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre> <p>The Unix timestamp (in seconds) for when the fine-tuning job was created.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.error","title":"error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error: Optional[Error] = None\n</code></pre> <p>For fine-tuning jobs that have <code>failed</code>, this will contain more information on the cause of the failure.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.fine_tuned_model","title":"fine_tuned_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fine_tuned_model: Optional[str] = None\n</code></pre> <p>The name of the fine-tuned model that is being created.</p> <p>The value will be null if the fine-tuning job is still running.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.finished_at","title":"finished_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>finished_at: Optional[int] = None\n</code></pre> <p>The Unix timestamp (in seconds) for when the fine-tuning job was finished.</p> <p>The value will be null if the fine-tuning job is still running.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.hyperparameters","title":"hyperparameters  <code>instance-attribute</code>","text":"<pre><code>hyperparameters: Hyperparameters\n</code></pre> <p>The hyperparameters used for the fine-tuning job.</p> <p>See the fine-tuning guide for more details.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The object identifier, which can be referenced in the API endpoints.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre> <p>The base model that is being fine-tuned.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['fine_tuning.job']\n</code></pre> <p>The object type, which is always \"fine_tuning.job\".</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.organization_id","title":"organization_id  <code>instance-attribute</code>","text":"<pre><code>organization_id: str\n</code></pre> <p>The organization that owns the fine-tuning job.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.result_files","title":"result_files  <code>instance-attribute</code>","text":"<pre><code>result_files: List[str]\n</code></pre> <p>The compiled results file ID(s) for the fine-tuning job.</p> <p>You can retrieve the results with the Files API.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Literal[\n    \"validating_files\",\n    \"queued\",\n    \"running\",\n    \"succeeded\",\n    \"failed\",\n    \"cancelled\",\n]\n</code></pre> <p>The current status of the fine-tuning job, which can be either <code>validating_files</code>, <code>queued</code>, <code>running</code>, <code>succeeded</code>, <code>failed</code>, or <code>cancelled</code>.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.trained_tokens","title":"trained_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trained_tokens: Optional[int] = None\n</code></pre> <p>The total number of billable tokens processed by this fine-tuning job.</p> <p>The value will be null if the fine-tuning job is still running.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.training_file","title":"training_file  <code>instance-attribute</code>","text":"<pre><code>training_file: str\n</code></pre> <p>The file ID used for training.</p> <p>You can retrieve the training data with the Files API.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.FineTuningJob.validation_file","title":"validation_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>validation_file: Optional[str] = None\n</code></pre> <p>The file ID used for validation.</p> <p>You can retrieve the validation results with the Files API.</p>"},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.Hyperparameters","title":"Hyperparameters","text":""},{"location":"types/fine_tuning/fine_tuning_job/#src.openai.types.fine_tuning.fine_tuning_job.Hyperparameters.n_epochs","title":"n_epochs  <code>instance-attribute</code>","text":"<pre><code>n_epochs: Union[Literal['auto'], int]\n</code></pre> <p>The number of epochs to train the model for.</p> <p>An epoch refers to one full cycle through the training dataset. \"auto\" decides the optimal number of epochs based on the size of the dataset. If setting the number manually, we support any number between 1 and 50 epochs.</p>"},{"location":"types/fine_tuning/fine_tuning_job_event/","title":"Fine tuning job event","text":""},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event","title":"fine_tuning_job_event","text":""},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event.FineTuningJobEvent","title":"FineTuningJobEvent","text":""},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event.FineTuningJobEvent.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: int\n</code></pre>"},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event.FineTuningJobEvent.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event.FineTuningJobEvent.level","title":"level  <code>instance-attribute</code>","text":"<pre><code>level: Literal['info', 'warn', 'error']\n</code></pre>"},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event.FineTuningJobEvent.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre>"},{"location":"types/fine_tuning/fine_tuning_job_event/#src.openai.types.fine_tuning.fine_tuning_job_event.FineTuningJobEvent.object","title":"object  <code>instance-attribute</code>","text":"<pre><code>object: Literal['fine_tuning.job.event']\n</code></pre>"},{"location":"types/fine_tuning/job_create_params/","title":"Job create params","text":""},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params","title":"job_create_params","text":""},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.Hyperparameters","title":"Hyperparameters","text":""},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.Hyperparameters.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size: Union[Literal['auto'], int]\n</code></pre> <p>Number of examples in each batch.</p> <p>A larger batch size means that model parameters are updated less frequently, but with lower variance.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.Hyperparameters.learning_rate_multiplier","title":"learning_rate_multiplier  <code>instance-attribute</code>","text":"<pre><code>learning_rate_multiplier: Union[Literal['auto'], float]\n</code></pre> <p>Scaling factor for the learning rate.</p> <p>A smaller learning rate may be useful to avoid overfitting.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.Hyperparameters.n_epochs","title":"n_epochs  <code>instance-attribute</code>","text":"<pre><code>n_epochs: Union[Literal['auto'], int]\n</code></pre> <p>The number of epochs to train the model for.</p> <p>An epoch refers to one full cycle through the training dataset.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.JobCreateParams","title":"JobCreateParams","text":""},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.JobCreateParams.hyperparameters","title":"hyperparameters  <code>instance-attribute</code>","text":"<pre><code>hyperparameters: Hyperparameters\n</code></pre> <p>The hyperparameters used for the fine-tuning job.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.JobCreateParams.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Required[\n    Union[\n        str,\n        Literal[\n            \"babbage-002\", \"davinci-002\", \"gpt-3.5-turbo\"\n        ],\n    ]\n]\n</code></pre> <p>The name of the model to fine-tune.</p> <p>You can select one of the supported models.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.JobCreateParams.suffix","title":"suffix  <code>instance-attribute</code>","text":"<pre><code>suffix: Optional[str]\n</code></pre> <p>A string of up to 18 characters that will be added to your fine-tuned model name.</p> <p>For example, a <code>suffix</code> of \"custom-model-name\" would produce a model name like <code>ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel</code>.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.JobCreateParams.training_file","title":"training_file  <code>instance-attribute</code>","text":"<pre><code>training_file: Required[str]\n</code></pre> <p>The ID of an uploaded file that contains training data.</p> <p>See upload file for how to upload a file.</p> <p>Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide for more details.</p>"},{"location":"types/fine_tuning/job_create_params/#src.openai.types.fine_tuning.job_create_params.JobCreateParams.validation_file","title":"validation_file  <code>instance-attribute</code>","text":"<pre><code>validation_file: Optional[str]\n</code></pre> <p>The ID of an uploaded file that contains validation data.</p> <p>If you provide this file, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should not be present in both train and validation files.</p> <p>Your dataset must be formatted as a JSONL file. You must upload your file with the purpose <code>fine-tune</code>.</p> <p>See the fine-tuning guide for more details.</p>"},{"location":"types/fine_tuning/job_list_events_params/","title":"Job list events params","text":""},{"location":"types/fine_tuning/job_list_events_params/#src.openai.types.fine_tuning.job_list_events_params","title":"job_list_events_params","text":""},{"location":"types/fine_tuning/job_list_events_params/#src.openai.types.fine_tuning.job_list_events_params.JobListEventsParams","title":"JobListEventsParams","text":""},{"location":"types/fine_tuning/job_list_events_params/#src.openai.types.fine_tuning.job_list_events_params.JobListEventsParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>Identifier for the last event from the previous pagination request.</p>"},{"location":"types/fine_tuning/job_list_events_params/#src.openai.types.fine_tuning.job_list_events_params.JobListEventsParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>Number of events to retrieve.</p>"},{"location":"types/fine_tuning/job_list_params/","title":"Job list params","text":""},{"location":"types/fine_tuning/job_list_params/#src.openai.types.fine_tuning.job_list_params","title":"job_list_params","text":""},{"location":"types/fine_tuning/job_list_params/#src.openai.types.fine_tuning.job_list_params.JobListParams","title":"JobListParams","text":""},{"location":"types/fine_tuning/job_list_params/#src.openai.types.fine_tuning.job_list_params.JobListParams.after","title":"after  <code>instance-attribute</code>","text":"<pre><code>after: str\n</code></pre> <p>Identifier for the last job from the previous pagination request.</p>"},{"location":"types/fine_tuning/job_list_params/#src.openai.types.fine_tuning.job_list_params.JobListParams.limit","title":"limit  <code>instance-attribute</code>","text":"<pre><code>limit: int\n</code></pre> <p>Number of fine-tuning jobs to retrieve.</p>"},{"location":"types/shared/__init__/","title":"openai.types.shared","text":""},{"location":"types/shared/__init__/#src.openai.types.shared","title":"shared","text":""},{"location":"types/shared/function_definition/","title":"Function definition","text":""},{"location":"types/shared/function_definition/#src.openai.types.shared.function_definition","title":"function_definition","text":""},{"location":"types/shared/function_definition/#src.openai.types.shared.function_definition.FunctionDefinition","title":"FunctionDefinition","text":""},{"location":"types/shared/function_definition/#src.openai.types.shared.function_definition.FunctionDefinition.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre> <p>A description of what the function does, used by the model to choose when and how to call the function.</p>"},{"location":"types/shared/function_definition/#src.openai.types.shared.function_definition.FunctionDefinition.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function to be called.</p> <p>Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.</p>"},{"location":"types/shared/function_definition/#src.openai.types.shared.function_definition.FunctionDefinition.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Optional[FunctionParameters] = None\n</code></pre> <p>The parameters the functions accepts, described as a JSON Schema object.</p> <p>See the guide for examples, and the JSON Schema reference for documentation about the format.</p> <p>Omitting <code>parameters</code> defines a function with an empty parameter list.</p>"},{"location":"types/shared/function_parameters/","title":"Function parameters","text":""},{"location":"types/shared/function_parameters/#src.openai.types.shared.function_parameters","title":"function_parameters","text":""},{"location":"types/shared/function_parameters/#src.openai.types.shared.function_parameters.FunctionParameters","title":"FunctionParameters  <code>module-attribute</code>","text":"<pre><code>FunctionParameters = Dict[str, object]\n</code></pre>"},{"location":"types/shared_params/__init__/","title":"openai.types.shared_params","text":""},{"location":"types/shared_params/__init__/#src.openai.types.shared_params","title":"shared_params","text":""},{"location":"types/shared_params/function_definition/","title":"Function definition","text":""},{"location":"types/shared_params/function_definition/#src.openai.types.shared_params.function_definition","title":"function_definition","text":""},{"location":"types/shared_params/function_definition/#src.openai.types.shared_params.function_definition.FunctionDefinition","title":"FunctionDefinition","text":""},{"location":"types/shared_params/function_definition/#src.openai.types.shared_params.function_definition.FunctionDefinition.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre> <p>A description of what the function does, used by the model to choose when and how to call the function.</p>"},{"location":"types/shared_params/function_definition/#src.openai.types.shared_params.function_definition.FunctionDefinition.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to be called.</p> <p>Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.</p>"},{"location":"types/shared_params/function_definition/#src.openai.types.shared_params.function_definition.FunctionDefinition.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: FunctionParameters\n</code></pre> <p>The parameters the functions accepts, described as a JSON Schema object.</p> <p>See the guide for examples, and the JSON Schema reference for documentation about the format.</p> <p>Omitting <code>parameters</code> defines a function with an empty parameter list.</p>"},{"location":"types/shared_params/function_parameters/","title":"Function parameters","text":""},{"location":"types/shared_params/function_parameters/#src.openai.types.shared_params.function_parameters","title":"function_parameters","text":""},{"location":"types/shared_params/function_parameters/#src.openai.types.shared_params.function_parameters.FunctionParameters","title":"FunctionParameters  <code>module-attribute</code>","text":"<pre><code>FunctionParameters = Dict[str, object]\n</code></pre>"}]}